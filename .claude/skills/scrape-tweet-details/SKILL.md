# scrape-tweet-details

---
name: scrape-tweet-details
description: |
  Top 10ãƒ„ã‚¤ãƒ¼ãƒˆã®è©³ç´°ãƒšãƒ¼ã‚¸ï¼ˆx.com/i/status/{tweet_id}ï¼‰ã«é·ç§»ã—ã€
  ãƒªãƒ³ã‚¯ï¼ˆè¨˜äº‹/YouTube/PDFï¼‰ã¨ãƒªãƒ—ãƒ©ã‚¤ä¸Šä½5ä»¶ã‚’æŠ½å‡ºã€‚
  æ‰€è¦æ™‚é–“: 10-15åˆ†ã€å‡ºåŠ›: tweet_details.json
version: 1.0.0
trigger_keywords:
  - "ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°æŠ½å‡º"
  - "ãƒªãƒ³ã‚¯ãƒ»ãƒªãƒ—ãƒ©ã‚¤åé›†"
  - "ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"
stage: Phase 1b - Detail Extraction
dependencies:
  - extract-top-tweets
output_file: "Stock/programs/å‰¯æ¥­/projects/SNS/data/tweet_details_{YYYYMMDD}.json"
execution_time: "10-15åˆ†"
priority: P0
model: claude-haiku-4-5-20251001  # Haiku 4.5 (2026å¹´1æœˆæ™‚ç‚¹ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«)
thinking: false
---

## Overview

`extract-top-tweets` ã‚¹ã‚­ãƒ«ãŒæŠ½å‡ºã—ãŸTop 10ãƒ„ã‚¤ãƒ¼ãƒˆã®è©³ç´°ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åé›†ã—ã¾ã™:

1. **ãƒªãƒ³ã‚¯æŠ½å‡º**: ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡å†…ã®URLï¼ˆè¨˜äº‹/YouTube/PDFï¼‰ã‚’åˆ†é¡
2. **ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡º**: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆä¸Šä½5ä»¶ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’å–å¾—
3. **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—**: æŠ•ç¨¿æ™‚é–“ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆè©³ç´°ã€ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±

**æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**:
- Playwright (ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–)
- Cookieèªè¨¼ï¼ˆ30æ—¥æœ‰åŠ¹æœŸé™ï¼‰
- DOMè§£æ

---

## Instructions

### STEP 1: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ1åˆ†ï¼‰

**å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«**:
- `Stock/programs/å‰¯æ¥­/projects/SNS/data/top_10_tweets_{YYYYMMDD}.json`
- Fallback: æœ€æ–°æ—¥ä»˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œç´¢

**ãƒ‡ãƒ¼ã‚¿æŠ½å‡º**:
```python
# Top 10ã®ãƒ„ã‚¤ãƒ¼ãƒˆIDãƒ»URLãƒªã‚¹ãƒˆã‚’ä½œæˆ
tweet_list = [
    {
        'tweet_id': tweet['tweet_id'],
        'url': f"https://x.com/i/status/{tweet['tweet_id']}",
        'username': tweet['username']
    }
    for tweet in input_data['top_tweets']
]
```

**æ¤œè¨¼**:
- [ ] tweet_id ãŒå­˜åœ¨ã™ã‚‹ã‹
- [ ] URLå½¢å¼ãŒæ­£ã—ã„ã‹

---

### STEP 2: Cookieèªè¨¼æº–å‚™ï¼ˆ1åˆ†ï¼‰

**Cookieèª­ã¿è¾¼ã¿**:
- Cookieä¿å­˜å…ˆ: `Stock/programs/å‰¯æ¥­/projects/SNS/data/x_cookies.json`
- å½¢å¼: `{ "auth_token": "...", "ct0": "..." }`

**Cookieæœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯**:
```python
if cookie_age > 30 days:
    print("âš ï¸ Cookie has expired. Please re-authenticate.")
    # @.claude/skills/_shared/error_handling_patterns.md#5-èªè¨¼ã‚¨ãƒ©ãƒ¼
```

**PlaywrightåˆæœŸåŒ–**:
```python
from playwright.sync_api import sync_playwright

browser = playwright.chromium.launch(headless=True)
context = browser.new_context()
context.add_cookies(cookies)
page = context.new_page()
```

---

### STEP 3: ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãƒšãƒ¼ã‚¸å·¡å›ï¼ˆ8-10åˆ†ï¼‰

**ä¸¦åˆ—å‡¦ç†ã®æ¤œè¨**:
- 10ä»¶ã‚’é †æ¬¡å‡¦ç†ï¼ˆå®‰å®šæ€§å„ªå…ˆï¼‰
- å„ãƒ„ã‚¤ãƒ¼ãƒˆå‡¦ç†æ™‚é–“: ç´„1åˆ†
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«3ç§’å¾…æ©Ÿ

**ãƒšãƒ¼ã‚¸é·ç§»ãƒ­ã‚¸ãƒƒã‚¯**:
```python
for tweet in tweet_list:
    try:
        # 1. ãƒšãƒ¼ã‚¸é·ç§»
        page.goto(tweet['url'], wait_until='networkidle')

        # 2. ãƒšãƒ¼ã‚¸å®Œå…¨èª­ã¿è¾¼ã¿å¾…æ©Ÿï¼ˆå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰
        page.wait_for_selector('article[data-testid="tweet"]', timeout=10000)

        # 3. ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãƒªãƒ—ãƒ©ã‚¤èª­ã¿è¾¼ã¿
        page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        page.wait_for_timeout(2000)  # 2ç§’å¾…æ©Ÿ

        # 4. ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆSTEP 4-6ï¼‰
        extract_links(page, tweet)
        extract_replies(page, tweet)

        # 5. ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
        page.wait_for_timeout(3000)  # 3ç§’å¾…æ©Ÿ

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ² + æ¬¡ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¸
        log_error(tweet['tweet_id'], str(e))
        continue
```

---

### STEP 4: ãƒªãƒ³ã‚¯æŠ½å‡ºã¨åˆ†é¡ï¼ˆ1åˆ†/ãƒ„ã‚¤ãƒ¼ãƒˆï¼‰

**ãƒªãƒ³ã‚¯æŠ½å‡ºã‚»ãƒ¬ã‚¯ã‚¿**:
```python
# ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡å†…ã®ãƒªãƒ³ã‚¯
links = page.query_selector_all('article[data-testid="tweet"] a[href^="http"]')
```

**ãƒªãƒ³ã‚¯åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯**:
```python
def classify_link(url):
    """URLã‚’è¨˜äº‹/YouTube/PDFã«åˆ†é¡"""
    if 'youtube.com' in url or 'youtu.be' in url:
        return 'youtube'
    elif url.endswith('.pdf'):
        return 'pdf'
    elif any(domain in url for domain in ['medium.com', 'note.com', 'zenn.dev', 'qiita.com']):
        return 'article'
    else:
        # HEAD ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§Content-Typeãƒã‚§ãƒƒã‚¯
        response = requests.head(url, allow_redirects=True, timeout=5)
        content_type = response.headers.get('Content-Type', '')

        if 'application/pdf' in content_type:
            return 'pdf'
        elif 'text/html' in content_type:
            return 'article'
        else:
            return 'other'
```

**çŸ­ç¸®URLå±•é–‹**:
```python
# t.co, bit.ly ãªã©ã®çŸ­ç¸®URLã‚’å®ŸURLã«å±•é–‹
if 't.co' in url or 'bit.ly' in url:
    response = requests.head(url, allow_redirects=True, timeout=5)
    url = response.url  # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆURL
```

**å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹é€ **:
```json
"links": [
    {
        "url": "https://example.com/article",
        "type": "article",
        "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå¯èƒ½ãªå ´åˆï¼‰",
        "domain": "example.com"
    },
    {
        "url": "https://youtube.com/watch?v=xxx",
        "type": "youtube",
        "title": "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«",
        "domain": "youtube.com"
    }
]
```

---

### STEP 5: ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºï¼ˆ1åˆ†/ãƒ„ã‚¤ãƒ¼ãƒˆï¼‰

**ãƒªãƒ—ãƒ©ã‚¤å–å¾—ã‚»ãƒ¬ã‚¯ã‚¿**:
```python
# ãƒªãƒ—ãƒ©ã‚¤ãƒ„ã‚¤ãƒ¼ãƒˆã®DOMè¦ç´ 
reply_elements = page.query_selector_all('article[data-testid="tweet"][role="article"]')
# æœ€åˆã®1ä»¶ã¯å…ƒãƒ„ã‚¤ãƒ¼ãƒˆè‡ªèº«ãªã®ã§é™¤å¤–
reply_elements = reply_elements[1:]
```

**ãƒªãƒ—ãƒ©ã‚¤æƒ…å ±æŠ½å‡º**:
```python
for reply_elem in reply_elements[:5]:  # ä¸Šä½5ä»¶
    reply_data = {
        'username': reply_elem.query_selector('[data-testid="User-Name"] a').inner_text(),
        'text': reply_elem.query_selector('[data-testid="tweetText"]').inner_text(),
        'likes': int(reply_elem.query_selector('[data-testid="like"]').inner_text() or 0),
        'created_at': reply_elem.query_selector('time').get_attribute('datetime')
    }
```

**ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé †ã‚½ãƒ¼ãƒˆ**:
- X.comã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤ºé †ï¼ˆTop Repliesã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ã«ä¾å­˜
- è¡¨ç¤ºé †ã®æœ€åˆ5ä»¶ã‚’å–å¾—

**ãƒªãƒ—ãƒ©ã‚¤ãŒ5ä»¶æœªæº€ã®å ´åˆ**:
- è­¦å‘Šè¡¨ç¤º: "âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æ•°ãŒ5ä»¶æœªæº€ã§ã™ï¼ˆ{N}ä»¶ï¼‰"
- å–å¾—å¯èƒ½ãªå…¨ä»¶ã‚’å‡ºåŠ›

---

### STEP 6: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ30ç§’/ãƒ„ã‚¤ãƒ¼ãƒˆï¼‰

**è¿½åŠ æƒ…å ±æŠ½å‡º**:
```python
metadata = {
    'has_media': bool(page.query_selector('[data-testid="tweetPhoto"]')),
    'media_count': len(page.query_selector_all('[data-testid="tweetPhoto"]')),
    'has_video': bool(page.query_selector('video')),
    'view_count': page.query_selector('[data-testid="views"]').inner_text() if page.query_selector('[data-testid="views"]') else None,
    'is_thread': '/' in page.query_selector('[data-testid="reply"]').inner_text() if page.query_selector('[data-testid="reply"]') else False
}
```

---

### STEP 7: ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»å‡ºåŠ›ï¼ˆ2åˆ†ï¼‰

**å‡ºåŠ›JSONæ§‹é€ **:
```json
{
  "metadata": {
    "processed_at": "2026-01-02T11:00:00+09:00",
    "source_file": "top_10_tweets_20260102.json",
    "total_tweets_processed": 10,
    "success_count": 10,
    "error_count": 0
  },
  "tweet_details": [
    {
      "tweet_id": "1234567890123456789",
      "username": "ai_researcher_jp",
      "url": "https://x.com/ai_researcher_jp/status/1234567890123456789",
      "engagement_score": 385,
      "links": [
        {
          "url": "https://arxiv.org/abs/2401.12345",
          "type": "pdf",
          "title": "Research Paper on AI Agents",
          "domain": "arxiv.org"
        },
        {
          "url": "https://youtube.com/watch?v=abc123",
          "type": "youtube",
          "title": "AI Agent Tutorial",
          "domain": "youtube.com"
        }
      ],
      "replies": [
        {
          "username": "tech_enthusiast",
          "text": "ã“ã‚Œã¯ç´ æ™´ã‚‰ã—ã„ç ”ç©¶ã§ã™ã­ï¼ç‰¹ã«...",
          "likes": 45,
          "created_at": "2026-01-02T08:30:00+09:00"
        },
        ...
      ],
      "metadata": {
        "has_media": true,
        "media_count": 1,
        "has_video": false,
        "view_count": "12.5K",
        "is_thread": false
      }
    },
    ...
  ],
  "errors": []
}
```

**Write tool ã§ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜**:
- ãƒ‘ã‚¹: `Stock/programs/å‰¯æ¥­/projects/SNS/data/tweet_details_{YYYYMMDD}.json`
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: JSONï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ2ã‚¹ãƒšãƒ¼ã‚¹ï¼‰

---

### STEP 8: å“è³ªæ¤œè¨¼ï¼ˆ1åˆ†ï¼‰

**æ¤œè¨¼é …ç›®**:

1. **ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§**:
   - [ ] 10ä»¶å…¨ã¦ã®ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãŒå–å¾—æ¸ˆã¿
   - [ ] ãƒªãƒ³ã‚¯æŠ½å‡ºæˆåŠŸç‡ > 80%
   - [ ] ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºæˆåŠŸç‡ > 80%

2. **ãƒªãƒ³ã‚¯åˆ†é¡ç²¾åº¦**:
   - [ ] YouTube URLãŒå…¨ã¦ `youtube` ã«åˆ†é¡
   - [ ] PDF URLãŒå…¨ã¦ `pdf` ã«åˆ†é¡
   - [ ] è¨˜äº‹URLãŒ `article` ã«åˆ†é¡

3. **ãƒªãƒ—ãƒ©ã‚¤å“è³ª**:
   - [ ] ãƒªãƒ—ãƒ©ã‚¤ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºæ–‡å­—åˆ—ã§ãªã„
   - [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒæ­£ã—ãæŠ½å‡ºã•ã‚Œã¦ã„ã‚‹

**æ¤œè¨¼å¤±æ•—æ™‚**:
- è­¦å‘Šãƒ­ã‚°å‡ºåŠ›
- ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ `errors` é…åˆ—ã«è¨˜éŒ²

---

## Output Format

**æˆåŠŸæ™‚ã®è¡¨ç¤ºä¾‹**:
```
âœ… Tweet details extracted successfully

ğŸ“Š Summary:
- Total tweets processed: 10/10
- Success rate: 100%
- Total links extracted: 18
  - Articles: 12 (66.7%)
  - YouTube: 5 (27.8%)
  - PDF: 1 (5.5%)
- Total replies extracted: 50 (avg 5.0/tweet)
- Output file: Stock/programs/å‰¯æ¥­/projects/SNS/data/tweet_details_20260102.json

ğŸ”— Link Breakdown by Tweet:
- Tweet 1 (@ai_researcher_jp): 2 links (1 article, 1 YouTube)
- Tweet 2 (@startup_founder): 1 link (1 article)
- Tweet 3 (@tech_writer_jp): 3 links (2 articles, 1 PDF)
...

ğŸ’¬ Reply Insights:
- Average likes per reply: 23.4
- Most engaged reply: @tech_enthusiast (45 likes)
```

---

## Error Handling

### ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³1: CookieæœŸé™åˆ‡ã‚Œ
- **å‚ç…§**: @.claude/skills/_shared/error_handling_patterns.md#5-èªè¨¼ã‚¨ãƒ©ãƒ¼
- **å¯¾å¿œ**:
  1. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
  2. Cookieå†å–å¾—æ‰‹é †ã‚’æ¡ˆå†…
  3. å‡¦ç†ä¸­æ–­

### ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
- **å¯¾å¿œ**:
  1. 3å›ãƒªãƒˆãƒ©ã‚¤ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•: 5ç§’ã€10ç§’ã€20ç§’ï¼‰
  2. 3å›å¤±æ•—å¾Œã¯ã‚¹ã‚­ãƒƒãƒ— + ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²
  3. æ¬¡ã®ãƒ„ã‚¤ãƒ¼ãƒˆã«é€²ã‚€

### ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ429 Too Many Requestsï¼‰
- **å¯¾å¿œ**:
  1. 60ç§’å¾…æ©Ÿ
  2. å†è©¦è¡Œ
  3. 3å›é€£ç¶šå¤±æ•—ã§å‡¦ç†ä¸­æ–­ + é€²æ—ä¿å­˜

### ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³4: DOMè¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„
- **å¯¾å¿œ**:
  1. ã‚»ãƒ¬ã‚¯ã‚¿ã®ä»£æ›¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
  2. å…¨ã¦å¤±æ•—ã—ãŸå ´åˆã¯ `null` ã‚’è¨˜éŒ²
  3. æ¬¡ã®ãƒ„ã‚¤ãƒ¼ãƒˆã«é€²ã‚€

### ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒªãƒ³ã‚¯URLå±•é–‹å¤±æ•—
- **å¯¾å¿œ**:
  1. çŸ­ç¸®URLã®ã¾ã¾è¨˜éŒ²
  2. `type: "unknown"` ã§ãƒãƒ¼ã‚¯
  3. è­¦å‘Šãƒ­ã‚°å‡ºåŠ›

---

## Best Practices

### Playwrightå®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

1. **å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¾…æ©Ÿ**:
```python
# âŒ æ‚ªã„ä¾‹
page.goto(url)
page.wait_for_timeout(5000)  # å›ºå®šå¾…æ©Ÿã¯ä¸å®‰å®š

# âœ… è‰¯ã„ä¾‹
page.goto(url, wait_until='networkidle')
page.wait_for_selector('article[data-testid="tweet"]')
```

2. **ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿**:
```python
# ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ï¼ˆ3-5ç§’ï¼‰ã§ãƒœãƒƒãƒˆæ¤œå‡ºå›é¿
import random
wait_time = random.uniform(3, 5)
page.wait_for_timeout(int(wait_time * 1000))
```

3. **User-Agentè¨­å®š**:
```python
context = browser.new_context(
    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
)
```

4. **ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è©³ç´°åŒ–**:
```python
try:
    page.goto(url)
except Exception as e:
    error_log = {
        'tweet_id': tweet_id,
        'error_type': type(e).__name__,
        'error_message': str(e),
        'url': url,
        'timestamp': datetime.now().isoformat()
    }
    errors.append(error_log)
```

---

## Quality Checklist

å®Ÿè¡Œå®Œäº†æ™‚ã«ä»¥ä¸‹ã‚’ç¢ºèª:

- [ ] 10ä»¶å…¨ã¦ã®ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°æŠ½å‡ºå®Œäº†
- [ ] ãƒªãƒ³ã‚¯åˆ†é¡ç²¾åº¦90%ä»¥ä¸Šï¼ˆæ‰‹å‹•ã§3ä»¶ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ï¼‰
- [ ] ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºæˆåŠŸç‡90%ä»¥ä¸Š
- [ ] Cookieèªè¨¼æˆåŠŸï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
- [ ] ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«å¼•ã£ã‹ã‹ã£ã¦ã„ãªã„
- [ ] å‡ºåŠ›JSONãŒæ­£ã—ã„å½¢å¼
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãŒé©åˆ‡ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹

---

## Dependencies

**å‰æã‚¹ã‚­ãƒ«**:
- `extract-top-tweets`: Top 10ãƒ„ã‚¤ãƒ¼ãƒˆIDãƒ»URLãƒªã‚¹ãƒˆ

**æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºã‚¹ã‚­ãƒ«**:
- `extract-content`: è¨˜äº‹/YouTube/PDF ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
- `analyze-replies`: ãƒªãƒ—ãƒ©ã‚¤ã‹ã‚‰åéŸ¿ãƒã‚¤ãƒ³ãƒˆåˆ†æ

**æŠ€è¡“ä¾å­˜**:
- Playwright (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: `pip install playwright && playwright install chromium`)
- requests (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: `pip install requests`)

---

## Version History

- **v1.0.0** (2026-01-02): åˆç‰ˆä½œæˆ
  - ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãƒšãƒ¼ã‚¸é·ç§»æ©Ÿèƒ½
  - ãƒªãƒ³ã‚¯æŠ½å‡ºãƒ»åˆ†é¡ï¼ˆè¨˜äº‹/YouTube/PDFï¼‰
  - ãƒªãƒ—ãƒ©ã‚¤ä¸Šä½5ä»¶å–å¾—
  - Cookieèªè¨¼ãƒ»ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
