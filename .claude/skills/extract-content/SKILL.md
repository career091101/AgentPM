---
name: extract-content
description: |
  ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹ãƒ»YouTubeãƒ»PDFãƒªãƒ³ã‚¯ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚
  ClaudeCode LLMãŒç›´æ¥WebFetch/Readãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ãƒ»è§£æã—ã¾ã™ã€‚

  ä½¿ç”¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼š
  - SNSæŠ•ç¨¿ä½œæˆã®äº‹å‰èª¿æŸ»æ™‚
  - å‚ç…§è¨˜äº‹ã®è¦ç´„ãŒå¿…è¦ãªæ™‚
  - ãƒªãƒ³ã‚¯å…ˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ãŸã„æ™‚

  æ‰€è¦æ™‚é–“ï¼š5-10åˆ†ï¼ˆãƒªãƒ³ã‚¯æ•°ã«ä¾å­˜ï¼‰
  å‡ºåŠ›ï¼šextracted_contents_ai_{YYYYMMDD}.json
trigger_keywords:
  - "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º"
  - "è¨˜äº‹æŠ½å‡º"
  - "ãƒªãƒ³ã‚¯å…ˆæŠ½å‡º"
  - "extract content"
stage: Phase 2 - Content Extraction
dependencies: ["scrape-tweet-details"]
output_file: Stock/programs/å‰¯æ¥­/projects/SNS/data/extracted_contents_ai_{YYYYMMDD}.json
execution_time: 5-10åˆ†
framework_reference: Stock/programs/å‰¯æ¥­/projects/SNS/
priority: P1
model: claude-haiku-4-5-20251001  # Haiku 4.5 (2026å¹´1æœˆæ™‚ç‚¹ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«)
---

# Extract Content Skill

ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ã‹ã‚‰è¨˜äº‹ãƒ»YouTubeãƒ»PDFã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚

---

## ã“ã®Skillã§ã§ãã‚‹ã“ã¨

1. **è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º**: WebFetchãƒ„ãƒ¼ãƒ«ã§HTMLè§£æã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ãƒ»ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—
2. **YouTubeå‹•ç”»æƒ…å ±å–å¾—**: ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ã‚’å–å¾—ï¼ˆå­—å¹•æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…ï¼‰
3. **PDFæƒ…å ±å–å¾—**: ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—ï¼ˆå…¨æ–‡æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…ï¼‰
4. **è¤‡æ•°ãƒªãƒ³ã‚¯ä¸€æ‹¬å‡¦ç†**: ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°å†…ã®å…¨ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•å‡¦ç†
5. **AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢ä»˜ä¸**: æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«AIé–¢é€£åº¦ï¼ˆ0-3ç‚¹ï¼‰ã‚’è‡ªå‹•åˆ¤å®šãƒ»ä»˜ä¸
6. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ»403ã‚¨ãƒ©ãƒ¼ç­‰ã‚’é©åˆ‡ã«è¨˜éŒ²

---

## å…¥åŠ›ãƒ»å‡ºåŠ›

| é …ç›® | å†…å®¹ |
|------|------|
| **å…¥åŠ›** | tweet_details_ai_{YYYYMMDD}.jsonï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°+ãƒªãƒ³ã‚¯æƒ…å ±ï¼‰ |
| **å‡ºåŠ›** | extracted_contents_ai_{YYYYMMDD}.jsonï¼ˆæŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ + AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢ï¼‰ |
| **æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³** | filter-extracted-contentï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰ã€analyze-repliesï¼ˆãƒªãƒ—ãƒ©ã‚¤åˆ†æï¼‰ã€research-topicï¼ˆWebèª¿æŸ»ï¼‰ |

---

## Instructions

**å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰**: ClaudeCode LLMè‡ªå¾‹å®Ÿè¡Œ
**æ¨å®šæ‰€è¦æ™‚é–“**: 5-10åˆ†ï¼ˆãƒªãƒ³ã‚¯æ•°ã«ä¾å­˜ï¼‰

### STEP 1: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ30ç§’ï¼‰

**Readãƒ„ãƒ¼ãƒ«ä½¿ç”¨**:
```
/Users/yuichi/AIPM/aipm_v0/Stock/programs/å‰¯æ¥­/projects/SNS/data/tweet_details_ai_{æœ€æ–°æ—¥ä»˜}.json
```

**ç¢ºèªé …ç›®**:
- ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
- `tweet_details` é…åˆ—ã®èª­ã¿è¾¼ã¿
- å„ãƒ„ã‚¤ãƒ¼ãƒˆã® `links` é…åˆ—ã‚’æŠ½å‡º

**ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**:
- æœ€æ–°æ—¥ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€`tweet_details_*.json` ã®æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢

---

### STEP 2: ãƒªãƒ³ã‚¯åˆ†é¡ï¼ˆ1åˆ†ï¼‰

**å…¨ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’åé›†**:
```python
# ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼ˆLLMå†…ã§å®Ÿè¡Œï¼‰
all_links = []
for tweet in tweet_details:
    for link in tweet['links']:
        all_links.append({
            'tweet_id': tweet['tweet_id'],
            'username': tweet['username'],
            'url': link['url'],
            'type': link['type'],  # article, youtube, pdf, other
            'domain': link['domain']
        })

# ã‚¿ã‚¤ãƒ—åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
link_types = {
    'article': [link for link in all_links if link['type'] == 'article'],
    'youtube': [link for link in all_links if link['type'] == 'youtube'],
    'pdf': [link for link in all_links if link['type'] == 'pdf'],
    'other': [link for link in all_links if link['type'] == 'other']
}
```

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å ±å‘Š**:
```
ğŸ“Š ãƒªãƒ³ã‚¯åˆ†é¡çµæœ
- è¨˜äº‹: Xä»¶
- YouTube: Yä»¶
- PDF: Zä»¶
- ãã®ä»–: Wä»¶

åˆè¨ˆ: Nä»¶ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
```

---

### STEP 3: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆ3-8åˆ†ï¼‰

#### 3A. è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆWebFetchãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼‰

**å„è¨˜äº‹URLã«å¯¾ã—ã¦**:
```
WebFetch(
  url=link['url'],
  prompt="ã“ã®è¨˜äº‹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã€æœ¬æ–‡ï¼ˆæœ€åˆã®500ãƒ¯ãƒ¼ãƒ‰ï¼‰ã€ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„: {title: string, content: string, meta_description: string}"
)
```

**æŠ½å‡ºçµæœã®æ§‹é€ åŒ–**:
```json
{
  "url": "https://example.com/article",
  "type": "article",
  "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
  "content": "æœ¬æ–‡ã®æœ€åˆã®500ãƒ¯ãƒ¼ãƒ‰...",
  "meta_description": "è¨˜äº‹ã®èª¬æ˜",
  "word_count": 450,
  "extracted_at": "2026-01-02T12:00:00",
  "status": "success",
  "tweet_id": "2006...",
  "username": "cb_doge",
  "domain": "example.com"
}
```

**ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**:
- **Timeout**: `status: "timeout"`, `error: "Request timeout"`
- **403 Forbidden**: `status: "forbidden"`, `error: "Access denied"`
- **404 Not Found**: `status: "not_found"`, `error: "Page not found"`
- **ãã®ä»–**: `status: "error"`, `error: error_message`

#### 3B. YouTubeå‹•ç”»æƒ…å ±å–å¾—

**ç¾åœ¨ã®å®Ÿè£…**: åŸºæœ¬æƒ…å ±ã®ã¿
```json
{
  "url": "https://youtube.com/watch?v=xxx",
  "type": "youtube",
  "status": "partial",
  "title": "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆURLã‹ã‚‰æ¨æ¸¬ï¼‰",
  "note": "å­—å¹•æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…äºˆå®š",
  "tweet_id": "2006...",
  "username": "hasan28d"
}
```

**ä»Šå¾Œã®å®Ÿè£…**: youtube-transcript-api ä½¿ç”¨

#### 3C. PDFæƒ…å ±å–å¾—

**ç¾åœ¨ã®å®Ÿè£…**: ãƒ¡ã‚¿æƒ…å ±ã®ã¿
```json
{
  "url": "https://example.com/paper.pdf",
  "type": "pdf",
  "status": "partial",
  "note": "PDFå…¨æ–‡æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…äºˆå®š",
  "tweet_id": "2006...",
  "username": "researcher"
}
```

**ä»Šå¾Œã®å®Ÿè£…**: pdfplumber + pytesseract ä½¿ç”¨

---

### STEP 3.5: AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢ä»˜ä¸ï¼ˆ1-2åˆ†ï¼‰

**åˆ¤å®šåŸºæº–ã®å‚ç…§**: `@.claude/skills/_shared/ai_relevance_criteria.md`

#### 3.5A. å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®AIé–¢é€£åº¦åˆ¤å®š

**å…¨æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å¯¾ã—ã¦**:

```python
# ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼ˆLLMå†…ã§å®Ÿè¡Œï¼‰
for content in extracted_contents:
    if content['status'] != 'success':
        # ã‚¨ãƒ©ãƒ¼ãƒ»ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ«ã¯0ç‚¹
        content['ai_relevance_score'] = 0
        content['ai_relevance_reason'] = "æŠ½å‡ºå¤±æ•—"
        continue

    title = content.get('title', '')
    text = content.get('content', '')

    # ã‚¿ã‚¤ãƒˆãƒ«å„ªå…ˆåˆ¤å®š
    score = check_title_keywords(title)

    if score > 0:
        content['ai_relevance_score'] = score
        content['ai_relevance_reason'] = f"ã‚¿ã‚¤ãƒˆãƒ«ã«{score}ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰"
        continue

    # æœ¬æ–‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦åˆ¤å®š
    score = calculate_keyword_density(text)

    content['ai_relevance_score'] = score

    if score == 3:
        content['ai_relevance_reason'] = "æœ¬æ–‡ã«AIæŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ï¼ˆå¯†åº¦2%ä»¥ä¸Šï¼‰"
    elif score == 2:
        content['ai_relevance_reason'] = "æœ¬æ–‡ã«AIä¼æ¥­åå«æœ‰ï¼ˆå¯†åº¦1%ä»¥ä¸Šï¼‰"
    elif score == 1:
        content['ai_relevance_reason'] = "æœ¬æ–‡ã«ML/ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹å«æœ‰ï¼ˆå¯†åº¦0.5%ä»¥ä¸Šï¼‰"
    else:
        content['ai_relevance_reason'] = "AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸åœ¨"
```

#### 3.5B. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°å®Ÿè£…

**ç°¡æ˜“ç‰ˆå®Ÿè£…**ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å­˜åœ¨åˆ¤å®šï¼‰:

```python
# 3ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
keywords_3pt = [
    "LLM", "ChatGPT", "Claude", "GPT", "Gemini", "ç”ŸæˆAI",
    "generative AI", "transformer", "neural network",
    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°", "RAG", "fine-tuning"
]

# 2ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
keywords_2pt = [
    "OpenAI", "Anthropic", "DeepMind", "Google AI",
    "Microsoft AI", "Meta AI", "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«"
]

# 1ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
keywords_1pt = [
    "æ©Ÿæ¢°å­¦ç¿’", "machine learning", "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹",
    "data science", "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"
]

def check_title_keywords(title: str) -> int:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰AIé–¢é€£åº¦ã‚’åˆ¤å®š"""
    title_lower = title.lower()

    if any(kw.lower() in title_lower for kw in keywords_3pt):
        return 3
    if any(kw.lower() in title_lower for kw in keywords_2pt):
        return 2
    if any(kw.lower() in title_lower for kw in keywords_1pt):
        return 1

    return 0
```

**è©³ç´°ãªåˆ¤å®šåŸºæº–**: `@.claude/skills/_shared/ai_relevance_criteria.md` ã‚’å‚ç…§

---

### STEP 4: çµæœé›†è¨ˆï¼ˆ1åˆ†ï¼‰

**çµ±è¨ˆæƒ…å ±è¨ˆç®—**:
```python
# ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰
success_count = len([c for c in extracted if c['status'] == 'success'])
partial_count = len([c for c in extracted if c['status'] == 'partial'])
error_count = len([c for c in extracted if c['status'] in ['timeout', 'forbidden', 'error']])

total_words = sum([c.get('word_count', 0) for c in extracted if c['status'] == 'success'])
avg_words = total_words / success_count if success_count > 0 else 0

success_rate = (success_count / len(all_links)) * 100

# AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢é›†è¨ˆ
score_distribution = {
    '3ç‚¹': len([c for c in extracted if c.get('ai_relevance_score', 0) == 3]),
    '2ç‚¹': len([c for c in extracted if c.get('ai_relevance_score', 0) == 2]),
    '1ç‚¹': len([c for c in extracted if c.get('ai_relevance_score', 0) == 1]),
    '0ç‚¹': len([c for c in extracted if c.get('ai_relevance_score', 0) == 0])
}

ai_relevant_rate = (sum([score_distribution['3ç‚¹'], score_distribution['2ç‚¹'], score_distribution['1ç‚¹']]) / len(extracted)) * 100 if len(extracted) > 0 else 0
```

---

### STEP 5: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆ30ç§’ï¼‰

**å‡ºåŠ›JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**:
```json
{
  "metadata": {
    "processed_at": "2026-01-02T12:38:00",
    "source_file": "tweet_details_ai_20260102.json",
    "total_links": 12,
    "success_count": 11,
    "partial_count": 0,
    "error_count": 1,
    "success_rate": 91.7,
    "link_types": {
      "article": 11,
      "youtube": 0,
      "pdf": 1
    },
    "ai_relevance_distribution": {
      "3ç‚¹": 5,
      "2ç‚¹": 3,
      "1ç‚¹": 1,
      "0ç‚¹": 3
    },
    "ai_relevant_rate": 75.0
  },
  "extracted_contents": [
    {
      "url": "https://...",
      "type": "article",
      "title": "...",
      "content": "...",
      "word_count": 530,
      "status": "success",
      "tweet_id": "...",
      "username": "...",
      "domain": "...",
      "ai_relevance_score": 3,
      "ai_relevance_reason": "ã‚¿ã‚¤ãƒˆãƒ«ã«3ç‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰"
    }
  ]
}
```

**ä¿å­˜å…ˆ**: `Stock/programs/å‰¯æ¥­/projects/SNS/data/extracted_contents_ai_{YYYYMMDD}.json`

---

### STEP 6: ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆ30ç§’ï¼‰

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å ±å‘Š**:
```
âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå®Œäº†

ğŸ“Š Summary:
  - Total links processed: 12
  - Success: 11 (91.7%)
  - Errors: 1 (8.3%)

ğŸ“ Content statistics:
  - Total words extracted: 1,322
  - Average words per article: 120

ğŸ¯ AIé–¢é€£åº¦åˆ†å¸ƒ:
  - 3ç‚¹ï¼ˆé«˜é–¢é€£åº¦ï¼‰: 5ä»¶ (41.7%)
  - 2ç‚¹ï¼ˆä¸­é–¢é€£åº¦ï¼‰: 3ä»¶ (25.0%)
  - 1ç‚¹ï¼ˆä½é–¢é€£åº¦ï¼‰: 1ä»¶ (8.3%)
  - 0ç‚¹ï¼ˆéAIé–¢é€£ï¼‰: 3ä»¶ (25.0%)
  - AIé–¢é€£ç‡: 75.0%

ğŸ† Top 3 AIé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„:
  1. [3ç‚¹] ChatGPT-4ã®RAGå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ (530 words)
  2. [3ç‚¹] Claude 3.5ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° (412 words)
  3. [2ç‚¹] OpenAIæ–°ãƒ¢ãƒ‡ãƒ«ç™ºè¡¨ (298 words)

ğŸ’¾ Output: extracted_contents_ai_20260102.json (35KB)

ğŸ“Œ Next: filter-extracted-contentï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰ã€analyze-repliesï¼ˆãƒªãƒ—ãƒ©ã‚¤åˆ†æï¼‰ã€research-topicï¼ˆWebèª¿æŸ»ï¼‰
```

---

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### WebFetchã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
- **åŸå› **: ã‚µãƒ¼ãƒãƒ¼å¿œç­”é…å»¶ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸å®‰å®š
- **å¯¾å¿œ**: `status: "timeout"` ã§è¨˜éŒ²ã€æ¬¡ã®ãƒªãƒ³ã‚¯ã¸é€²ã‚€
- **ãƒªãƒˆãƒ©ã‚¤**: ãªã—ï¼ˆæ™‚é–“åŠ¹ç‡å„ªå…ˆï¼‰

### 403 Forbidden
- **åŸå› **: User-Agentåˆ¶é™ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™
- **å¯¾å¿œ**: `status: "forbidden"` ã§è¨˜éŒ²ã€æ¬¡ã®ãƒªãƒ³ã‚¯ã¸é€²ã‚€
- **ä¾‹**: help.x.comï¼ˆXå…¬å¼ãƒ˜ãƒ«ãƒ—ã¯ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚ã‚Šï¼‰

### ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå¤±æ•—
- **åŸå› **: HTMLæ§‹é€ ãŒäºˆæ¸¬å¤–
- **å¯¾å¿œ**: `content: ""`, `word_count: 0` ã§è¨˜éŒ²
- **ãƒ­ã‚°**: `status: "success"` ã ãŒ `word_count: 0` ã®å ´åˆã¯è­¦å‘Š

---

## ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

| å“è³ªæŒ‡æ¨™ | ç›®æ¨™ | å®Ÿç¸¾ï¼ˆ2026-01-02ï¼‰ |
|---------|------|------------------|
| **æˆåŠŸç‡** | â‰¥80% | 91.7% (11/12) |
| **ç·æŠ½å‡ºãƒ¯ãƒ¼ãƒ‰æ•°** | â‰¥500 | 1,322 |
| **å¹³å‡ãƒ¯ãƒ¼ãƒ‰æ•°/è¨˜äº‹** | â‰¥50 | 120 |
| **AIé–¢é€£ç‡** | â‰¥60% | 75.0% (9/12) |
| **é«˜é–¢é€£åº¦ï¼ˆ3ç‚¹ï¼‰æ¯”ç‡** | â‰¥30% | 41.7% (5/12) |

---

## ä½¿ç”¨ä¾‹

### åŸºæœ¬çš„ãªä½¿ç”¨

```
User: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
```

ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•çš„ã«ï¼š
1. æœ€æ–°ã® `tweet_details_ai_*.json` ã‚’èª­ã¿è¾¼ã¿
2. å…¨ãƒªãƒ³ã‚¯ã‚’åˆ†é¡
3. WebFetchãƒ„ãƒ¼ãƒ«ã§å„ãƒªãƒ³ã‚¯ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
4. AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸ï¼ˆ0-3ç‚¹ï¼‰
5. çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ï¼ˆAIé–¢é€£åº¦åˆ†å¸ƒå«ã‚€ï¼‰
6. JSONå‡ºåŠ›ç”Ÿæˆ
7. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º

---

## ä¾å­˜ãƒ„ãƒ¼ãƒ«

**å¿…é ˆ**:
- `Read`: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
- `WebFetch`: è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
- `Write`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

**å‚ç…§**:
- `@.claude/skills/_shared/ai_relevance_criteria.md`: AIé–¢é€£åº¦åˆ¤å®šåŸºæº–

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰**:
- `Bash`: youtube-transcript-apiã€pdfplumberå®Ÿè¡Œ

---

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ

æŠ½å‡ºå®Œäº†å¾Œã€ä»¥ä¸‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¾ã™ï¼š

1. **filter-extracted-content**: AIé–¢é€£åº¦ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰
2. **analyze-replies**: ãƒªãƒ—ãƒ©ã‚¤ã‹ã‚‰åéŸ¿ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
3. **research-topic**: WebSearchã§æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯
4. **generate-sns-posts**: AIé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å…ƒã«æŠ•ç¨¿æ–‡ç”Ÿæˆ

---

## æ›´æ–°å±¥æ­´

- 2026-01-02: åˆç‰ˆä½œæˆï¼ˆClaudeCode LLMç›´æ¥å®Ÿè¡Œå‹ï¼‰
  - å®Ÿç¸¾: 11/12ãƒªãƒ³ã‚¯æˆåŠŸï¼ˆ91.7%ï¼‰ã€1,322ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
- 2026-01-12: AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢ä»˜ä¸æ©Ÿèƒ½ã‚’è¿½åŠ ï¼ˆSTEP 3.5ï¼‰
  - AIé–¢é€£åº¦åˆ¤å®šåŸºæº–: `ai_relevance_criteria.md` v1.0æº–æ‹ 
  - å‡ºåŠ›ã«ai_relevance_score, ai_relevance_reasonã‚’è¿½åŠ 
  - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ai_relevance_distribution, ai_relevant_rateã‚’è¿½åŠ 
