---
name: extract-content
description: |
  ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹ãƒ»YouTubeãƒ»PDFãƒªãƒ³ã‚¯ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚
  ClaudeCode LLMãŒç›´æ¥WebFetch/Readãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ãƒ»è§£æã—ã¾ã™ã€‚

  ä½¿ç”¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼š
  - SNSæŠ•ç¨¿ä½œæˆã®äº‹å‰èª¿æŸ»æ™‚
  - å‚ç…§è¨˜äº‹ã®è¦ç´„ãŒå¿…è¦ãªæ™‚
  - ãƒªãƒ³ã‚¯å…ˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ãŸã„æ™‚

  æ‰€è¦æ™‚é–“ï¼š5-10åˆ†ï¼ˆãƒªãƒ³ã‚¯æ•°ã«ä¾å­˜ï¼‰
  å‡ºåŠ›ï¼šextracted_contents_ai_{YYYYMMDD}.json
trigger_keywords:
  - "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º"
  - "è¨˜äº‹æŠ½å‡º"
  - "ãƒªãƒ³ã‚¯å…ˆæŠ½å‡º"
  - "extract content"
stage: Phase 2 - Content Extraction
dependencies: ["scrape-tweet-details"]
output_file: Stock/programs/å‰¯æ¥­/projects/SNS/data/extracted_contents_ai_{YYYYMMDD}.json
execution_time: 5-10åˆ†
framework_reference: Stock/programs/å‰¯æ¥­/projects/SNS/
priority: P1
model: claude-haiku-4-5-20251001  # Haiku 4.5 (2026å¹´1æœˆæ™‚ç‚¹ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«)
---

# Extract Content Skill

ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ã‹ã‚‰è¨˜äº‹ãƒ»YouTubeãƒ»PDFã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚

---

## ã“ã®Skillã§ã§ãã‚‹ã“ã¨

1. **è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º**: WebFetchãƒ„ãƒ¼ãƒ«ã§HTMLè§£æã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ãƒ»ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—
2. **YouTubeå‹•ç”»æƒ…å ±å–å¾—**: ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ã‚’å–å¾—ï¼ˆå­—å¹•æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…ï¼‰
3. **PDFæƒ…å ±å–å¾—**: ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—ï¼ˆå…¨æ–‡æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…ï¼‰
4. **è¤‡æ•°ãƒªãƒ³ã‚¯ä¸€æ‹¬å‡¦ç†**: ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°å†…ã®å…¨ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•å‡¦ç†
5. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ»403ã‚¨ãƒ©ãƒ¼ç­‰ã‚’é©åˆ‡ã«è¨˜éŒ²

---

## å…¥åŠ›ãƒ»å‡ºåŠ›

| é …ç›® | å†…å®¹ |
|------|------|
| **å…¥åŠ›** | tweet_details_ai_{YYYYMMDD}.jsonï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°+ãƒªãƒ³ã‚¯æƒ…å ±ï¼‰ |
| **å‡ºåŠ›** | extracted_contents_ai_{YYYYMMDD}.jsonï¼ˆæŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰ |
| **æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³** | analyze-repliesï¼ˆãƒªãƒ—ãƒ©ã‚¤åˆ†æï¼‰ã€research-topicï¼ˆWebèª¿æŸ»ï¼‰ |

---

## Instructions

**å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰**: ClaudeCode LLMè‡ªå¾‹å®Ÿè¡Œ
**æ¨å®šæ‰€è¦æ™‚é–“**: 5-10åˆ†ï¼ˆãƒªãƒ³ã‚¯æ•°ã«ä¾å­˜ï¼‰

### STEP 1: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆ30ç§’ï¼‰

**Readãƒ„ãƒ¼ãƒ«ä½¿ç”¨**:
```
/Users/yuichi/AIPM/aipm_v0/Stock/programs/å‰¯æ¥­/projects/SNS/data/tweet_details_ai_{æœ€æ–°æ—¥ä»˜}.json
```

**ç¢ºèªé …ç›®**:
- ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
- `tweet_details` é…åˆ—ã®èª­ã¿è¾¼ã¿
- å„ãƒ„ã‚¤ãƒ¼ãƒˆã® `links` é…åˆ—ã‚’æŠ½å‡º

**ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**:
- æœ€æ–°æ—¥ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€`tweet_details_*.json` ã®æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢

---

### STEP 2: ãƒªãƒ³ã‚¯åˆ†é¡ï¼ˆ1åˆ†ï¼‰

**å…¨ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’åé›†**:
```python
# ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼ˆLLMå†…ã§å®Ÿè¡Œï¼‰
all_links = []
for tweet in tweet_details:
    for link in tweet['links']:
        all_links.append({
            'tweet_id': tweet['tweet_id'],
            'username': tweet['username'],
            'url': link['url'],
            'type': link['type'],  # article, youtube, pdf, other
            'domain': link['domain']
        })

# ã‚¿ã‚¤ãƒ—åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
link_types = {
    'article': [link for link in all_links if link['type'] == 'article'],
    'youtube': [link for link in all_links if link['type'] == 'youtube'],
    'pdf': [link for link in all_links if link['type'] == 'pdf'],
    'other': [link for link in all_links if link['type'] == 'other']
}
```

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å ±å‘Š**:
```
ğŸ“Š ãƒªãƒ³ã‚¯åˆ†é¡çµæœ
- è¨˜äº‹: Xä»¶
- YouTube: Yä»¶
- PDF: Zä»¶
- ãã®ä»–: Wä»¶

åˆè¨ˆ: Nä»¶ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
```

---

### STEP 3: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆ3-8åˆ†ï¼‰

#### 3A. è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆWebFetchãƒ„ãƒ¼ãƒ«ä½¿ç”¨ï¼‰

**å„è¨˜äº‹URLã«å¯¾ã—ã¦**:
```
WebFetch(
  url=link['url'],
  prompt="ã“ã®è¨˜äº‹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã€æœ¬æ–‡ï¼ˆæœ€åˆã®500ãƒ¯ãƒ¼ãƒ‰ï¼‰ã€ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„: {title: string, content: string, meta_description: string}"
)
```

**æŠ½å‡ºçµæœã®æ§‹é€ åŒ–**:
```json
{
  "url": "https://example.com/article",
  "type": "article",
  "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
  "content": "æœ¬æ–‡ã®æœ€åˆã®500ãƒ¯ãƒ¼ãƒ‰...",
  "meta_description": "è¨˜äº‹ã®èª¬æ˜",
  "word_count": 450,
  "extracted_at": "2026-01-02T12:00:00",
  "status": "success",
  "tweet_id": "2006...",
  "username": "cb_doge",
  "domain": "example.com"
}
```

**ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**:
- **Timeout**: `status: "timeout"`, `error: "Request timeout"`
- **403 Forbidden**: `status: "forbidden"`, `error: "Access denied"`
- **404 Not Found**: `status: "not_found"`, `error: "Page not found"`
- **ãã®ä»–**: `status: "error"`, `error: error_message`

#### 3B. YouTubeå‹•ç”»æƒ…å ±å–å¾—

**ç¾åœ¨ã®å®Ÿè£…**: åŸºæœ¬æƒ…å ±ã®ã¿
```json
{
  "url": "https://youtube.com/watch?v=xxx",
  "type": "youtube",
  "status": "partial",
  "title": "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆURLã‹ã‚‰æ¨æ¸¬ï¼‰",
  "note": "å­—å¹•æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…äºˆå®š",
  "tweet_id": "2006...",
  "username": "hasan28d"
}
```

**ä»Šå¾Œã®å®Ÿè£…**: youtube-transcript-api ä½¿ç”¨

#### 3C. PDFæƒ…å ±å–å¾—

**ç¾åœ¨ã®å®Ÿè£…**: ãƒ¡ã‚¿æƒ…å ±ã®ã¿
```json
{
  "url": "https://example.com/paper.pdf",
  "type": "pdf",
  "status": "partial",
  "note": "PDFå…¨æ–‡æŠ½å‡ºã¯ä»Šå¾Œå®Ÿè£…äºˆå®š",
  "tweet_id": "2006...",
  "username": "researcher"
}
```

**ä»Šå¾Œã®å®Ÿè£…**: pdfplumber + pytesseract ä½¿ç”¨

---

### STEP 4: çµæœé›†è¨ˆï¼ˆ1åˆ†ï¼‰

**çµ±è¨ˆæƒ…å ±è¨ˆç®—**:
```python
# ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰
success_count = len([c for c in extracted if c['status'] == 'success'])
partial_count = len([c for c in extracted if c['status'] == 'partial'])
error_count = len([c for c in extracted if c['status'] in ['timeout', 'forbidden', 'error']])

total_words = sum([c.get('word_count', 0) for c in extracted if c['status'] == 'success'])
avg_words = total_words / success_count if success_count > 0 else 0

success_rate = (success_count / len(all_links)) * 100
```

---

### STEP 5: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆ30ç§’ï¼‰

**å‡ºåŠ›JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**:
```json
{
  "metadata": {
    "processed_at": "2026-01-02T12:38:00",
    "source_file": "tweet_details_ai_20260102.json",
    "total_links": 12,
    "success_count": 11,
    "partial_count": 0,
    "error_count": 1,
    "success_rate": 91.7,
    "link_types": {
      "article": 11,
      "youtube": 0,
      "pdf": 1
    }
  },
  "extracted_contents": [
    {
      "url": "https://...",
      "type": "article",
      "title": "...",
      "content": "...",
      "word_count": 530,
      "status": "success",
      "tweet_id": "...",
      "username": "...",
      "domain": "..."
    }
  ]
}
```

**ä¿å­˜å…ˆ**: `Stock/programs/å‰¯æ¥­/projects/SNS/data/extracted_contents_ai_{YYYYMMDD}.json`

---

### STEP 6: ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆ30ç§’ï¼‰

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å ±å‘Š**:
```
âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå®Œäº†

ğŸ“Š Summary:
  - Total links processed: 12
  - Success: 11 (91.7%)
  - Errors: 1 (8.3%)

ğŸ“ Content statistics:
  - Total words extracted: 1,322
  - Average words per article: 120

ğŸ† Top 3 longest articles:
  1. Multibaggeræ ªã®å®Ÿè¨¼ç ”ç©¶... (530 words)
  2. AIç¿»è¨³ãƒ„ãƒ¼ãƒ«2026... (212 words)
  3. RAGå¤±æ•—ã®çµ„ç¹”çš„å•é¡Œ... (198 words)

ğŸ’¾ Output: extracted_contents_ai_20260102.json (35KB)

ğŸ“Œ Next: analyze-repliesï¼ˆãƒªãƒ—ãƒ©ã‚¤åˆ†æï¼‰ã€research-topicï¼ˆWebèª¿æŸ»ï¼‰
```

---

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### WebFetchã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
- **åŸå› **: ã‚µãƒ¼ãƒãƒ¼å¿œç­”é…å»¶ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸å®‰å®š
- **å¯¾å¿œ**: `status: "timeout"` ã§è¨˜éŒ²ã€æ¬¡ã®ãƒªãƒ³ã‚¯ã¸é€²ã‚€
- **ãƒªãƒˆãƒ©ã‚¤**: ãªã—ï¼ˆæ™‚é–“åŠ¹ç‡å„ªå…ˆï¼‰

### 403 Forbidden
- **åŸå› **: User-Agentåˆ¶é™ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™
- **å¯¾å¿œ**: `status: "forbidden"` ã§è¨˜éŒ²ã€æ¬¡ã®ãƒªãƒ³ã‚¯ã¸é€²ã‚€
- **ä¾‹**: help.x.comï¼ˆXå…¬å¼ãƒ˜ãƒ«ãƒ—ã¯ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚ã‚Šï¼‰

### ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå¤±æ•—
- **åŸå› **: HTMLæ§‹é€ ãŒäºˆæ¸¬å¤–
- **å¯¾å¿œ**: `content: ""`, `word_count: 0` ã§è¨˜éŒ²
- **ãƒ­ã‚°**: `status: "success"` ã ãŒ `word_count: 0` ã®å ´åˆã¯è­¦å‘Š

---

## ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼

| å“è³ªæŒ‡æ¨™ | ç›®æ¨™ | å®Ÿç¸¾ï¼ˆ2026-01-02ï¼‰ |
|---------|------|------------------|
| **æˆåŠŸç‡** | â‰¥80% | 91.7% (11/12) |
| **ç·æŠ½å‡ºãƒ¯ãƒ¼ãƒ‰æ•°** | â‰¥500 | 1,322 |
| **å¹³å‡ãƒ¯ãƒ¼ãƒ‰æ•°/è¨˜äº‹** | â‰¥50 | 120 |

---

## ä½¿ç”¨ä¾‹

### åŸºæœ¬çš„ãªä½¿ç”¨

```
User: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
```

ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•çš„ã«ï¼š
1. æœ€æ–°ã® `tweet_details_ai_*.json` ã‚’èª­ã¿è¾¼ã¿
2. å…¨ãƒªãƒ³ã‚¯ã‚’åˆ†é¡
3. WebFetchãƒ„ãƒ¼ãƒ«ã§å„ãƒªãƒ³ã‚¯ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
4. çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
5. JSONå‡ºåŠ›ç”Ÿæˆ
6. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º

---

## ä¾å­˜ãƒ„ãƒ¼ãƒ«

**å¿…é ˆ**:
- `Read`: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
- `WebFetch`: è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
- `Write`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰**:
- `Bash`: youtube-transcript-apiã€pdfplumberå®Ÿè¡Œ

---

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ

æŠ½å‡ºå®Œäº†å¾Œã€ä»¥ä¸‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¾ã™ï¼š

1. **analyze-replies**: ãƒªãƒ—ãƒ©ã‚¤ã‹ã‚‰åéŸ¿ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
2. **research-topic**: WebSearchã§æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯
3. **generate-sns-posts**: æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å…ƒã«æŠ•ç¨¿æ–‡ç”Ÿæˆ

---

## æ›´æ–°å±¥æ­´

- 2026-01-02: åˆç‰ˆä½œæˆï¼ˆClaudeCode LLMç›´æ¥å®Ÿè¡Œå‹ï¼‰
- å®Ÿç¸¾: 11/12ãƒªãƒ³ã‚¯æˆåŠŸï¼ˆ91.7%ï¼‰ã€1,322ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
