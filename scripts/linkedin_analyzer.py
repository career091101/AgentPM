#!/usr/bin/env python3
"""
LinkedInæŠ•ç¨¿åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    python linkedin_analyzer.py --input takano_linkedin_posts.json
"""

import json
import re
from collections import Counter
from pathlib import Path
import statistics
from janome.tokenizer import Tokenizer


class LinkedInAnalyzer:
    """LinkedInæŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""

    # æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ7ç¨®é¡ï¼‰
    PATTERNS = {
        'question_first': r'^[ï¼Ÿ?]|^(ä½•|ã©ã†|ãªãœ|ã©ã®|ã„ã¤|èª°)',
        'conclusion_last': r'(ã§ã™|ã¾ã™|ã§ã—ãŸ|ã¾ã—ãŸ|ã |ã§ã‚ã‚‹)$',
        'bullet_points': r'[ãƒ»â€¢â—¦\-\*]\s',
        'numbered_list': r'^\d+[\.\)ã€]\s',
        'story_opening': r'^(å…ˆæ—¥|æœ€è¿‘|ä»Šæ—¥|æ˜¨æ—¥|å…ˆé€±|å»å¹´)',
        'call_to_action': r'(ãã ã•ã„|ã—ã¾ã—ã‚‡ã†|ã„ã‹ãŒã§ã—ã‚‡ã†ã‹|ã©ã†æ€|è€ƒãˆã¦ã¿ã¦)',
        'emoji_usage': r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]'
    }

    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹ä¸€èˆ¬çš„ãªå˜èªï¼‰
    STOPWORDS = {
        'ã™ã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ãªã„', 'ã›ã‚‹',
        'ã•ã›ã‚‹', 'ãã‚Œã‚‹', 'ã‚„ã‚‹', 'ãã‚‹', 'ã„ã', 'ã‚‚ã‚‰ã†', 'ã“ã¨', 'ã‚‚ã®',
        'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ãŒ', 'ã§', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ',
        'ã•', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã‚‚', 'ã™ã‚‹', 'ã‹ã‚‰', 'ãª', 'ã“ã¨', 'ã¨ã—ã¦',
        'ã„', 'ã‚„', 'ã‚Œã‚‹', 'ãªã©', 'ãªã£', 'ãªã„', 'ã“ã®', 'ãŸã‚', 'ãã®',
        'ã‚ã£', 'ã‚ˆã†', 'ã¾ãŸ', 'ã‚‚ã®', 'ã¨ã„ã†', 'ã‚ã‚Š', 'ã¾ã§', 'ã‚‰ã‚Œ',
        'ãªã‚‹', 'ã¸', 'ã‹', 'ã ', 'ã“ã‚Œ', 'ã«ã‚ˆã£ã¦', 'ã«ã‚ˆã‚Š', 'ãŠã‚Š',
        'ã‚ˆã‚Š', 'ã«ã‚ˆã‚‹', 'ãš', 'ãªã‚Š', 'ã‚‰ã‚Œã‚‹', 'ã«ãŠã„ã¦', 'ã°', 'ãªã‹ã£',
        'ãªã', 'ã—ã‹ã—', 'ã«ã¤ã„ã¦', 'ã›', 'ã ã£', 'ãã®å¾Œ', 'ã§ãã‚‹',
        'ãã‚Œ', 'ã†', 'ã®ã§', 'ãªãŠ', 'ã®ã¿', 'ã§ã', 'ã', 'ã¤', 'ã«ãŠã‘ã‚‹',
        'ãŠã‚ˆã³', 'ã„ã†', 'ã•ã‚‰ã«', 'ã§ã‚‚', 'ã‚‰', 'ãŸã‚Š', 'ãã®ä»–', 'ã«é–¢ã™ã‚‹',
        'ãŸã¡', 'ã¾ã™', 'ã‚“', 'ãªã‚‰', 'ã«å¯¾ã—ã¦', 'åŠã³', 'ã“ã‚Œã‚‰', 'ã¨ã‚‚', 'ã¨ã“ã‚',
        'ã“ã“'
    }

    def __init__(self, json_path: str):
        self.json_path = json_path
        self.data = self._load_data()
        self.posts = self.data['posts']
        self.tokenizer = Tokenizer()

    def _load_data(self) -> dict:
        """JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
        with open(self.json_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def analyze_statistics(self) -> dict:
        """æ–‡å­—æ•°çµ±è¨ˆåˆ†æ"""
        char_counts = [p['char_count'] for p in self.posts]

        # åˆ†å¸ƒä½œæˆï¼ˆ0-500, 501-1000, 1001-1500, 1501+ï¼‰
        distribution = {
            '0-500': sum(1 for c in char_counts if c <= 500),
            '501-1000': sum(1 for c in char_counts if 501 <= c <= 1000),
            '1001-1500': sum(1 for c in char_counts if 1001 <= c <= 1500),
            '1501+': sum(1 for c in char_counts if c > 1500)
        }

        return {
            'char_count': {
                'mean': round(statistics.mean(char_counts), 1),
                'median': round(statistics.median(char_counts), 1),
                'min': min(char_counts),
                'max': max(char_counts),
                'std': round(statistics.stdev(char_counts), 1) if len(char_counts) > 1 else 0
            },
            'distribution': distribution,
            'total_posts': len(self.posts)
        }

    def classify_patterns(self) -> dict:
        """æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ï¼ˆ7ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
        results = {pattern: [] for pattern in self.PATTERNS.keys()}

        for i, post in enumerate(self.posts, 1):
            text = post['text']

            for pattern_name, pattern_regex in self.PATTERNS.items():
                if re.search(pattern_regex, text, re.MULTILINE):
                    results[pattern_name].append({
                        'post_index': i,
                        'post_id': post['post_id'],
                        'char_count': post['char_count'],
                        'preview': text[:100] + '...' if len(text) > 100 else text
                    })

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        summary = {
            pattern: {
                'count': len(matches),
                'percentage': round(len(matches) / len(self.posts) * 100, 1),
                'avg_char_count': round(statistics.mean([m['char_count'] for m in matches]), 1) if matches else 0
            }
            for pattern, matches in results.items()
        }

        return {
            'matches': results,
            'summary': summary
        }

    def extract_keywords(self, top_n: int = 50) -> list:
        """é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆJanomeå½¢æ…‹ç´ è§£æï¼‰"""
        all_words = []

        for post in self.posts:
            text = post['text']
            tokens = self.tokenizer.tokenize(text)

            for token in tokens:
                # å“è©æƒ…å ±ã‚’å–å¾—
                parts = str(token).split('\t')
                if len(parts) < 2:
                    continue

                word = parts[0]
                features = parts[1].split(',')
                pos = features[0]  # å“è©

                # åè©ã€å‹•è©ã€å½¢å®¹è©ã®ã¿æŠ½å‡º
                if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                    base_form = features[6] if len(features) > 6 and features[6] != '*' else word

                    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
                    if base_form not in self.STOPWORDS and len(base_form) > 1:
                        all_words.append(base_form)

        # é »å‡ºä¸Šä½Nå€‹
        counter = Counter(all_words)
        top_keywords = [
            {'word': word, 'count': count, 'pos': 'keyword'}
            for word, count in counter.most_common(top_n)
        ]

        return top_keywords

    def analyze_engagement(self) -> dict:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç›¸é–¢åˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿åˆ¶é™ç‰ˆï¼‰"""
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        sorted_posts = sorted(self.posts, key=lambda p: p['char_count'], reverse=True)
        top_20_percent = sorted_posts[:len(sorted_posts) // 5]

        # ãƒˆãƒƒãƒ—20%ã®ç‰¹å¾´åˆ†æ
        top_patterns = {pattern: 0 for pattern in self.PATTERNS.keys()}

        for post in top_20_percent:
            text = post['text']
            for pattern_name, pattern_regex in self.PATTERNS.items():
                if re.search(pattern_regex, text, re.MULTILINE):
                    top_patterns[pattern_name] += 1

        return {
            'note': 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æœªå–å¾—ã®ãŸã‚ã€æ–‡å­—æ•°ãƒˆãƒƒãƒ—20%ã®æŠ•ç¨¿ã‚’åˆ†æ',
            'top_performing_count': len(top_20_percent),
            'avg_char_count_top20': round(statistics.mean([p['char_count'] for p in top_20_percent]), 1),
            'pattern_frequency_top20': top_patterns,
            'top_posts': [
                {
                    'post_id': p['post_id'],
                    'char_count': p['char_count'],
                    'preview': p['text'][:150] + '...' if len(p['text']) > 150 else p['text']
                }
                for p in top_20_percent[:10]
            ]
        }

    def generate_report(self, output_path: str):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆJSONå½¢å¼ï¼‰"""
        report = {
            'statistics': self.analyze_statistics(),
            'patterns': self.classify_patterns(),
            'keywords': self.extract_keywords(50),
            'engagement': self.analyze_engagement()
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ… Analysis report saved: {output_path}")
        return report

    def print_summary(self):
        """ã‚µãƒãƒªãƒ¼ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›"""
        stats = self.analyze_statistics()
        patterns = self.classify_patterns()
        keywords = self.extract_keywords(20)

        print("=" * 60)
        print("LinkedInæŠ•ç¨¿åˆ†æã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç·æŠ•ç¨¿æ•°: {stats['total_posts']}ä»¶")
        print(f"  å¹³å‡æ–‡å­—æ•°: {stats['char_count']['mean']}å­—")
        print(f"  ä¸­å¤®å€¤: {stats['char_count']['median']}å­—")
        print(f"  æœ€å°-æœ€å¤§: {stats['char_count']['min']}-{stats['char_count']['max']}å­—")
        print(f"  æ¨™æº–åå·®: {stats['char_count']['std']}")

        print(f"\nğŸ“ˆ æ–‡å­—æ•°åˆ†å¸ƒ:")
        for range_label, count in stats['distribution'].items():
            print(f"  {range_label}å­—: {count}ä»¶")

        print(f"\nğŸ¨ æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for pattern, data in patterns['summary'].items():
            print(f"  {pattern}: {data['count']}ä»¶ ({data['percentage']}%)")

        print(f"\nğŸ”‘ é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆTop 20ï¼‰:")
        for i, kw in enumerate(keywords, 1):
            print(f"  {i}. {kw['word']} ({kw['count']}å›)")

        print("=" * 60)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="LinkedInæŠ•ç¨¿åˆ†æ")
    parser.add_argument('--input', required=True, help='å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', default=None, help='å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: analysis_report.jsonï¼‰')

    args = parser.parse_args()

    # å‡ºåŠ›ãƒ‘ã‚¹æ±ºå®š
    if args.output is None:
        input_path = Path(args.input)
        output_path = input_path.parent / 'analysis_report.json'
    else:
        output_path = args.output

    # åˆ†æå®Ÿè¡Œ
    analyzer = LinkedInAnalyzer(args.input)
    analyzer.print_summary()
    analyzer.generate_report(str(output_path))


if __name__ == "__main__":
    main()
