#!/usr/bin/env python3
"""
SNS Data Analysis Script
åˆ†æè¦ä»¶ã«åŸºã¥ã„ãŸFacebookæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
"""

import csv
import json
from collections import defaultdict
from datetime import datetime
import os

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DATA_DIR = "/Users/yuichi/AIPM/aipm_v0/Stock/programs/å‰¯æ¥­/projects/SNS/documents/2_discovery/data"
FB_CSV = os.path.join(DATA_DIR, "facebook_Sep-22-2025_Dec-21-2025_ã‚³ãƒ³ãƒ†ãƒ³ãƒ„_å…¬é–‹æ—¥æ™‚_æ¦‚è¦_1438120577831327.csv")

def load_facebook_data():
    """Facebookãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    posts = []
    with open(FB_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # è‡ªåˆ†ã®æŠ•ç¨¿ã®ã¿ï¼ˆä»–è€…ã®ã‚·ã‚§ã‚¢ã‚„è¨€åŠã‚’é™¤å¤–ï¼‰
            if row.get('ãƒšãƒ¼ã‚¸å') == 'ä½è—¤ å„ªä¸€' and row.get('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°'):
                try:
                    impressions = int(row.get('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', '0').replace(',', ''))
                    interactions = int(row.get('ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³', '0').replace(',', ''))
                    reactions = int(row.get('ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³', '0').replace(',', ''))
                    saves = int(row.get('ä¿å­˜æ•°', '0').replace(',', ''))
                    shares = int(row.get('ã‚·ã‚§ã‚¢', '0').replace(',', ''))
                    
                    posts.append({
                        'id': row.get('æŠ•ç¨¿ID'),
                        'title': row.get('ã‚¿ã‚¤ãƒˆãƒ«', '')[:200],  # æœ€åˆã®200æ–‡å­—
                        'date': row.get('å…¬é–‹æ™‚é–“'),
                        'type': row.get('æŠ•ç¨¿ã‚¿ã‚¤ãƒ—'),
                        'impressions': impressions,
                        'interactions': interactions,
                        'reactions': reactions,
                        'saves': saves,
                        'shares': shares,
                        'engagement_rate': (interactions / impressions * 100) if impressions > 0 else 0,
                        'link': row.get('ãƒªãƒ³ã‚¯')
                    })
                except (ValueError, TypeError):
                    continue
    return posts

def basic_statistics(posts):
    """åŸºç¤çµ±è¨ˆé‡ã®ç®—å‡º"""
    impressions = [p['impressions'] for p in posts]
    engagement_rates = [p['engagement_rate'] for p in posts]
    
    stats = {
        'total_posts': len(posts),
        'impressions': {
            'total': sum(impressions),
            'mean': sum(impressions) / len(impressions) if impressions else 0,
            'median': sorted(impressions)[len(impressions)//2] if impressions else 0,
            'max': max(impressions) if impressions else 0,
            'min': min(impressions) if impressions else 0,
        },
        'engagement_rate': {
            'mean': sum(engagement_rates) / len(engagement_rates) if engagement_rates else 0,
            'median': sorted(engagement_rates)[len(engagement_rates)//2] if engagement_rates else 0,
        }
    }
    return stats

def get_top_bottom_posts(posts, n=10):
    """ãƒˆãƒƒãƒ—10%ã¨ãƒœãƒˆãƒ 10%ã®æŠ•ç¨¿ã‚’æŠ½å‡º"""
    sorted_by_imp = sorted(posts, key=lambda x: x['impressions'], reverse=True)
    
    top_n = max(1, len(posts) // 10)  # 10%
    bottom_n = max(1, len(posts) // 10)
    
    return {
        'top_posts': sorted_by_imp[:top_n],
        'bottom_posts': sorted_by_imp[-bottom_n:],
        'top_n': top_n,
        'bottom_n': bottom_n
    }

def analyze_post_types(posts):
    """æŠ•ç¨¿ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†æ"""
    type_stats = defaultdict(lambda: {'count': 0, 'total_imp': 0, 'total_eng': 0})
    
    for p in posts:
        ptype = p['type'] or 'ãã®ä»–'
        type_stats[ptype]['count'] += 1
        type_stats[ptype]['total_imp'] += p['impressions']
        type_stats[ptype]['total_eng'] += p['interactions']
    
    # å¹³å‡ã‚’è¨ˆç®—
    for ptype in type_stats:
        count = type_stats[ptype]['count']
        type_stats[ptype]['avg_imp'] = type_stats[ptype]['total_imp'] / count if count > 0 else 0
        type_stats[ptype]['avg_eng'] = type_stats[ptype]['total_eng'] / count if count > 0 else 0
    
    return dict(type_stats)

def extract_topics(posts):
    """ãƒˆãƒ”ãƒƒã‚¯/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    keywords = {
        'OpenAI': ['OpenAI', 'ChatGPT', 'GPT', 'ã‚µãƒ ãƒ»ã‚¢ãƒ«ãƒˆãƒãƒ³', 'ã‚¢ãƒ«ãƒˆãƒãƒ³'],
        'Google/Gemini': ['Google', 'Gemini', 'ã‚°ãƒ¼ã‚°ãƒ«'],
        'AIå…¨èˆ¬': ['AI', 'äººå·¥çŸ¥èƒ½', 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ', 'AGI'],
        'ãƒ­ãƒœãƒƒãƒˆ': ['ãƒ­ãƒœãƒƒãƒˆ', 'ãƒ†ã‚¹ãƒ©', 'ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰'],
        'åŠå°ä½“': ['NVIDIA', 'ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢', 'GPU', 'åŠå°ä½“'],
        'æŠ•è³‡/çµŒæ¸ˆ': ['æŠ•è³‡', 'ãƒãƒ–ãƒ«', 'æ ª', 'GDP', 'çµŒæ¸ˆ'],
    }
    
    topic_stats = defaultdict(lambda: {'count': 0, 'total_imp': 0, 'posts': []})
    
    for p in posts:
        title = p['title']
        for topic, kws in keywords.items():
            if any(kw in title for kw in kws):
                topic_stats[topic]['count'] += 1
                topic_stats[topic]['total_imp'] += p['impressions']
                topic_stats[topic]['posts'].append(p)
    
    # å¹³å‡ã‚’è¨ˆç®—
    for topic in topic_stats:
        count = topic_stats[topic]['count']
        topic_stats[topic]['avg_imp'] = topic_stats[topic]['total_imp'] / count if count > 0 else 0
    
    return dict(topic_stats)

def main():
    print("=" * 60)
    print("SNS Data Analysis - Facebook")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    posts = load_facebook_data()
    print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(posts)}ä»¶ã®æŠ•ç¨¿")
    
    # åŸºç¤çµ±è¨ˆ
    print("\nğŸ“Š åŸºç¤çµ±è¨ˆé‡")
    print("-" * 40)
    stats = basic_statistics(posts)
    print(f"   ç·æŠ•ç¨¿æ•°: {stats['total_posts']}ä»¶")
    print(f"   ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {stats['impressions']['total']:,}")
    print(f"   å¹³å‡ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {stats['impressions']['mean']:,.0f}")
    print(f"   ä¸­å¤®å€¤ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {stats['impressions']['median']:,}")
    print(f"   æœ€å¤§ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {stats['impressions']['max']:,}")
    print(f"   å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {stats['engagement_rate']['mean']:.2f}%")
    
    # ãƒˆãƒƒãƒ—/ãƒœãƒˆãƒ æŠ•ç¨¿
    print("\nğŸ† ãƒˆãƒƒãƒ—10%æŠ•ç¨¿")
    print("-" * 40)
    top_bottom = get_top_bottom_posts(posts)
    for i, p in enumerate(top_bottom['top_posts'][:5], 1):
        print(f"   {i}. [{p['impressions']:,} imp] {p['title'][:60]}...")
    
    print("\nâš ï¸ ãƒœãƒˆãƒ 10%æŠ•ç¨¿")
    print("-" * 40)
    for i, p in enumerate(top_bottom['bottom_posts'][:5], 1):
        print(f"   {i}. [{p['impressions']:,} imp] {p['title'][:60]}...")
    
    # æŠ•ç¨¿ã‚¿ã‚¤ãƒ—åˆ¥
    print("\nğŸ“ æŠ•ç¨¿ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    print("-" * 40)
    type_stats = analyze_post_types(posts)
    for ptype, data in sorted(type_stats.items(), key=lambda x: x[1]['avg_imp'], reverse=True):
        print(f"   {ptype}: {data['count']}ä»¶, å¹³å‡{data['avg_imp']:,.0f} imp")
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥
    print("\nğŸ·ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    print("-" * 40)
    topic_stats = extract_topics(posts)
    for topic, data in sorted(topic_stats.items(), key=lambda x: x[1]['avg_imp'], reverse=True):
        print(f"   {topic}: {data['count']}ä»¶, å¹³å‡{data['avg_imp']:,.0f} imp")
    
    # çµæœã‚’JSONã§å‡ºåŠ›
    output = {
        'basic_stats': stats,
        'top_posts': top_bottom['top_posts'][:10],
        'bottom_posts': top_bottom['bottom_posts'][:10],
        'type_stats': type_stats,
        'topic_stats': {k: {kk: vv for kk, vv in v.items() if kk != 'posts'} for k, v in topic_stats.items()}
    }
    
    output_path = os.path.join(DATA_DIR, "facebook_analysis_results.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜: {output_path}")
    
    print("\n" + "=" * 60)
    print("åˆ†æå®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
