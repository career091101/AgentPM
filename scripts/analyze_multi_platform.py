#!/usr/bin/env python3
"""
SNS Multi-Platform Analysis Script
Facebook, LinkedIn, X ã®çµ±åˆåˆ†æ
"""

import csv
import json
from collections import defaultdict
from datetime import datetime
import os

# pandasã¨openpyxlãŒã‚ã‚Œã°ä½¿ç”¨
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DATA_DIR = "/Users/yuichi/AIPM/aipm_v0/Stock/programs/å‰¯æ¥­/projects/SNS/documents/2_discovery/data"
FB_CSV = os.path.join(DATA_DIR, "facebook_Sep-22-2025_Dec-21-2025_ã‚³ãƒ³ãƒ†ãƒ³ãƒ„_å…¬é–‹æ—¥æ™‚_æ¦‚è¦_1438120577831327.csv")
X_CSV = os.path.join(DATA_DIR, "account_overview_analytics.csv")
LINKEDIN_XLSX = os.path.join(DATA_DIR, "Linkedin_Content_2025-09-23_2025-12-21_å„ªä¸€ä½è—¤.xlsx")

def load_facebook_data():
    """Facebookãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    posts = []
    with open(FB_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get('ãƒšãƒ¼ã‚¸å') == 'ä½è—¤ å„ªä¸€' and row.get('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°'):
                try:
                    impressions = int(row.get('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', '0').replace(',', ''))
                    interactions = int(row.get('ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³', '0').replace(',', ''))
                    reactions = int(row.get('ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³', '0').replace(',', ''))
                    saves = int(row.get('ä¿å­˜æ•°', '0').replace(',', ''))
                    shares = int(row.get('ã‚·ã‚§ã‚¢', '0').replace(',', ''))
                    
                    posts.append({
                        'id': row.get('æŠ•ç¨¿ID'),
                        'title': row.get('ã‚¿ã‚¤ãƒˆãƒ«', '')[:300],
                        'date': row.get('å…¬é–‹æ™‚é–“'),
                        'type': row.get('æŠ•ç¨¿ã‚¿ã‚¤ãƒ—'),
                        'impressions': impressions,
                        'interactions': interactions,
                        'reactions': reactions,
                        'saves': saves,
                        'shares': shares,
                        'engagement_rate': (interactions / impressions * 100) if impressions > 0 else 0,
                        'link': row.get('ãƒªãƒ³ã‚¯'),
                        'platform': 'Facebook'
                    })
                except (ValueError, TypeError):
                    continue
    return posts

def load_x_data():
    """X(Twitter)ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ - æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿"""
    daily_stats = []
    with open(X_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            try:
                impressions = int(row.get('ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°', '0').replace(',', ''))
                likes = int(row.get('ã„ã„ã­', '0').replace(',', ''))
                engagement = int(row.get('ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', '0').replace(',', ''))
                bookmarks = int(row.get('ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯', '0').replace(',', ''))
                shares = int(row.get('å…±æœ‰ã•ã‚ŒãŸå›æ•°\\', '0').replace(',', ''))
                new_follows = int(row.get('æ–°ã—ã„ãƒ•ã‚©ãƒ­ãƒ¼', '0').replace(',', ''))
                posts_created = int(row.get('ãƒã‚¹ãƒˆã‚’ä½œæˆ', '0').replace(',', ''))
                
                daily_stats.append({
                    'date': row.get('Date'),
                    'impressions': impressions,
                    'likes': likes,
                    'engagement': engagement,
                    'bookmarks': bookmarks,
                    'shares': shares,
                    'new_follows': new_follows,
                    'posts_created': posts_created,
                    'platform': 'X'
                })
            except (ValueError, TypeError):
                continue
    return daily_stats

def load_linkedin_data():
    """LinkedInãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆXLSXãƒ•ã‚¡ã‚¤ãƒ«ï¼‰"""
    if not HAS_PANDAS:
        print("   âš ï¸ pandasãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€LinkedInãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—")
        return []
    
    try:
        df = pd.read_excel(LINKEDIN_XLSX)
        posts = []
        for _, row in df.iterrows():
            try:
                impressions = int(row.get('Impressions', 0)) if pd.notna(row.get('Impressions')) else 0
                reactions = int(row.get('Reactions', 0)) if pd.notna(row.get('Reactions')) else 0
                comments = int(row.get('Comments', 0)) if pd.notna(row.get('Comments')) else 0
                reposts = int(row.get('Reposts', 0)) if pd.notna(row.get('Reposts')) else 0
                
                posts.append({
                    'date': str(row.get('Date', '')),
                    'title': str(row.get('Post copy', ''))[:300] if pd.notna(row.get('Post copy')) else '',
                    'impressions': impressions,
                    'reactions': reactions,
                    'comments': comments,
                    'reposts': reposts,
                    'interactions': reactions + comments + reposts,
                    'engagement_rate': ((reactions + comments + reposts) / impressions * 100) if impressions > 0 else 0,
                    'platform': 'LinkedIn'
                })
            except (ValueError, TypeError) as e:
                continue
        return posts
    except Exception as e:
        print(f"   âš ï¸ LinkedInãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def extract_topics(title):
    """ãƒˆãƒ”ãƒƒã‚¯/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
    keywords = {
        'OpenAI': ['OpenAI', 'ChatGPT', 'GPT', 'ã‚µãƒ ãƒ»ã‚¢ãƒ«ãƒˆãƒãƒ³', 'ã‚¢ãƒ«ãƒˆãƒãƒ³', 'Sam Altman'],
        'Google/Gemini': ['Google', 'Gemini', 'ã‚°ãƒ¼ã‚°ãƒ«', 'Antigravity'],
        'Anthropic': ['Anthropic', 'Claude', 'ã‚¢ãƒ³ã‚½ãƒ­ãƒ”ãƒƒã‚¯', 'ã‚¢ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯'],
        'AIå…¨èˆ¬': ['AI', 'äººå·¥çŸ¥èƒ½', 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ', 'AGI', 'LLM'],
        'ãƒ­ãƒœãƒƒãƒˆ': ['ãƒ­ãƒœãƒƒãƒˆ', 'ãƒ†ã‚¹ãƒ©', 'ãƒ’ãƒ¥ãƒ¼ãƒãƒã‚¤ãƒ‰', 'humanoid', 'robot'],
        'åŠå°ä½“': ['NVIDIA', 'ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢', 'GPU', 'åŠå°ä½“', 'ãƒãƒƒãƒ—'],
        'æŠ•è³‡/çµŒæ¸ˆ': ['æŠ•è³‡', 'ãƒãƒ–ãƒ«', 'æ ª', 'GDP', 'çµŒæ¸ˆ', 'èµ¤å­—', 'åç›Š'],
    }
    
    matched_topics = []
    for topic, kws in keywords.items():
        if any(kw.lower() in title.lower() for kw in kws):
            matched_topics.append(topic)
    
    return matched_topics if matched_topics else ['ãã®ä»–']

def analyze_facebook(posts):
    """Facebookè©³ç´°åˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ“˜ FACEBOOK åˆ†æ")
    print("=" * 60)
    
    # åŸºç¤çµ±è¨ˆ
    impressions = [p['impressions'] for p in posts]
    total_imp = sum(impressions)
    avg_imp = total_imp / len(posts) if posts else 0
    
    print(f"\nğŸ“Š åŸºç¤çµ±è¨ˆ")
    print(f"   æŠ•ç¨¿æ•°: {len(posts)}ä»¶")
    print(f"   ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {total_imp:,}")
    print(f"   å¹³å‡ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {avg_imp:,.0f}")
    print(f"   æœ€å¤§: {max(impressions):,} / æœ€å°: {min(impressions):,}")
    
    # ãƒˆãƒƒãƒ—æŠ•ç¨¿
    sorted_posts = sorted(posts, key=lambda x: x['impressions'], reverse=True)
    print(f"\nğŸ† ãƒˆãƒƒãƒ—5æŠ•ç¨¿")
    for i, p in enumerate(sorted_posts[:5], 1):
        topics = extract_topics(p['title'])
        print(f"   {i}. [{p['impressions']:,} imp] [{', '.join(topics)}]")
        print(f"      {p['title'][:80]}...")
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥åˆ†æ
    topic_stats = defaultdict(lambda: {'count': 0, 'total_imp': 0})
    for p in posts:
        for topic in extract_topics(p['title']):
            topic_stats[topic]['count'] += 1
            topic_stats[topic]['total_imp'] += p['impressions']
    
    print(f"\nğŸ·ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    for topic, data in sorted(topic_stats.items(), key=lambda x: x[1]['total_imp']/max(x[1]['count'],1), reverse=True):
        avg = data['total_imp'] / data['count'] if data['count'] > 0 else 0
        print(f"   {topic}: {data['count']}ä»¶, å¹³å‡{avg:,.0f} imp")
    
    return {
        'total_posts': len(posts),
        'total_impressions': total_imp,
        'avg_impressions': avg_imp,
        'top_posts': sorted_posts[:10],
        'topic_stats': dict(topic_stats)
    }

def analyze_x(daily_stats):
    """X(Twitter)åˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ¦ X (Twitter) åˆ†æ")
    print("=" * 60)
    
    total_imp = sum(d['impressions'] for d in daily_stats)
    total_engagement = sum(d['engagement'] for d in daily_stats)
    total_posts = sum(d['posts_created'] for d in daily_stats)
    total_follows = sum(d['new_follows'] for d in daily_stats)
    
    print(f"\nğŸ“Š æœŸé–“ã‚µãƒãƒªãƒ¼ï¼ˆ{len(daily_stats)}æ—¥é–“ï¼‰")
    print(f"   ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {total_imp:,}")
    print(f"   ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {total_engagement:,}")
    print(f"   ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {(total_engagement/total_imp*100):.2f}%" if total_imp > 0 else "N/A")
    print(f"   æŠ•ç¨¿æ•°: {total_posts}ä»¶")
    print(f"   æ–°è¦ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼: {total_follows}äºº")
    print(f"   æ—¥å¹³å‡ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {total_imp/len(daily_stats):,.0f}")
    
    # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ—¥
    sorted_days = sorted(daily_stats, key=lambda x: x['impressions'], reverse=True)
    print(f"\nğŸ† ãƒˆãƒƒãƒ—5æ—¥")
    for i, d in enumerate(sorted_days[:5], 1):
        print(f"   {i}. {d['date']}: {d['impressions']:,} imp, {d['engagement']} eng")
    
    return {
        'total_days': len(daily_stats),
        'total_impressions': total_imp,
        'total_engagement': total_engagement,
        'total_posts': total_posts,
        'avg_daily_impressions': total_imp / len(daily_stats) if daily_stats else 0,
        'top_days': sorted_days[:10]
    }

def analyze_linkedin(posts):
    """LinkedInåˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ’¼ LINKEDIN åˆ†æ")
    print("=" * 60)
    
    if not posts:
        print("   ãƒ‡ãƒ¼ã‚¿ãªã—")
        return {}
    
    impressions = [p['impressions'] for p in posts]
    total_imp = sum(impressions)
    avg_imp = total_imp / len(posts) if posts else 0
    
    print(f"\nğŸ“Š åŸºç¤çµ±è¨ˆ")
    print(f"   æŠ•ç¨¿æ•°: {len(posts)}ä»¶")
    print(f"   ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {total_imp:,}")
    print(f"   å¹³å‡ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {avg_imp:,.0f}")
    
    # ãƒˆãƒƒãƒ—æŠ•ç¨¿
    sorted_posts = sorted(posts, key=lambda x: x['impressions'], reverse=True)
    print(f"\nğŸ† ãƒˆãƒƒãƒ—5æŠ•ç¨¿")
    for i, p in enumerate(sorted_posts[:5], 1):
        topics = extract_topics(p['title'])
        print(f"   {i}. [{p['impressions']:,} imp] [{', '.join(topics)}]")
        print(f"      {p['title'][:80]}...")
    
    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥åˆ†æ
    topic_stats = defaultdict(lambda: {'count': 0, 'total_imp': 0})
    for p in posts:
        for topic in extract_topics(p['title']):
            topic_stats[topic]['count'] += 1
            topic_stats[topic]['total_imp'] += p['impressions']
    
    print(f"\nğŸ·ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    for topic, data in sorted(topic_stats.items(), key=lambda x: x[1]['total_imp']/max(x[1]['count'],1), reverse=True):
        avg = data['total_imp'] / data['count'] if data['count'] > 0 else 0
        print(f"   {topic}: {data['count']}ä»¶, å¹³å‡{avg:,.0f} imp")
    
    return {
        'total_posts': len(posts),
        'total_impressions': total_imp,
        'avg_impressions': avg_imp,
        'top_posts': sorted_posts[:10],
        'topic_stats': dict(topic_stats)
    }

def cross_platform_comparison(fb_stats, x_stats, li_stats):
    """ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¯”è¼ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¯”è¼ƒ")
    print("=" * 60)
    
    print(f"\n{'ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ':<15} {'æŠ•ç¨¿/æ—¥æ•°':<12} {'ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³':<20} {'å¹³å‡':<15}")
    print("-" * 70)
    
    if fb_stats:
        print(f"{'Facebook':<15} {fb_stats['total_posts']:<12} {fb_stats['total_impressions']:>15,} {fb_stats['avg_impressions']:>12,.0f}")
    
    if x_stats:
        print(f"{'X (Twitter)':<15} {x_stats['total_days']:<12} {x_stats['total_impressions']:>15,} {x_stats['avg_daily_impressions']:>12,.0f}/æ—¥")
    
    if li_stats and li_stats.get('total_posts'):
        print(f"{'LinkedIn':<15} {li_stats['total_posts']:<12} {li_stats['total_impressions']:>15,} {li_stats['avg_impressions']:>12,.0f}")
    
    # åˆè¨ˆ
    total_imp = (fb_stats.get('total_impressions', 0) + 
                 x_stats.get('total_impressions', 0) + 
                 li_stats.get('total_impressions', 0))
    print("-" * 70)
    print(f"{'åˆè¨ˆ':<15} {'':<12} {total_imp:>15,}")
    print(f"\nğŸ“ˆ æœˆé–“æ›ç®—: {total_imp / 3:,.0f} imp/æœˆ")
    print(f"ğŸ“ ç›®æ¨™(100ä¸‡)ã¨ã®å·®: {((total_imp / 3) / 1000000 * 100):.1f}%é”æˆ")

def main():
    print("=" * 60)
    print("SNS Multi-Platform Analysis")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    fb_posts = load_facebook_data()
    print(f"   Facebook: {len(fb_posts)}ä»¶")
    
    x_daily = load_x_data()
    print(f"   X: {len(x_daily)}æ—¥åˆ†")
    
    li_posts = load_linkedin_data()
    print(f"   LinkedIn: {len(li_posts)}ä»¶")
    
    # å„ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ†æ
    fb_stats = analyze_facebook(fb_posts)
    x_stats = analyze_x(x_daily)
    li_stats = analyze_linkedin(li_posts)
    
    # ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¯”è¼ƒ
    cross_platform_comparison(fb_stats, x_stats, li_stats)
    
    # çµæœã‚’JSONã§ä¿å­˜
    output = {
        'facebook': {
            'total_posts': fb_stats['total_posts'],
            'total_impressions': fb_stats['total_impressions'],
            'avg_impressions': fb_stats['avg_impressions'],
        },
        'x': {
            'total_days': x_stats['total_days'],
            'total_impressions': x_stats['total_impressions'],
            'avg_daily_impressions': x_stats['avg_daily_impressions'],
        },
        'linkedin': {
            'total_posts': li_stats.get('total_posts', 0),
            'total_impressions': li_stats.get('total_impressions', 0),
            'avg_impressions': li_stats.get('avg_impressions', 0),
        }
    }
    
    output_path = os.path.join(DATA_DIR, "multi_platform_analysis.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜: {output_path}")
    
    print("\n" + "=" * 60)
    print("åˆ†æå®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
