#!/usr/bin/env python3
"""
Xãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ç‰¹å¾´æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ

823ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æŠ•ç¨¿ã‚’å…¨ã¦èª­ã¿è¾¼ã‚“ã§ã€ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŠ½å‡ºï¼š
1. ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ–‡ä½“ã€æ§‹é€ ã€æ–‡å­—æ•°åˆ†å¸ƒï¼‰
2. ãƒˆãƒ”ãƒƒã‚¯ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé »å‡ºèªã€å°‚é–€ç”¨èªï¼‰
3. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‰¹æ€§ï¼ˆã„ã„ã­ãƒ»RTãƒ»è¿”ä¿¡ã®ç›¸é–¢ï¼‰
4. æŠ•ç¨¿è€…ã®ç‰¹å¾´ï¼ˆå°‚é–€æ€§ã€ç™ºä¿¡ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
5. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ï¼ˆæŠ€è¡“è§£èª¬ã€äº‹ä¾‹ç´¹ä»‹ã€ãƒã‚¦ãƒ„ãƒ¼ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ç­‰ï¼‰
"""

import json
import re
from collections import Counter, defaultdict
from pathlib import Path
from datetime import datetime
import unicodedata

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path("/Users/yuichi/AIPM/aipm_v0")
INPUT_FILE = BASE_DIR / "Flow/202512/2025-12-31/x_bookmarks_data_fulltext.json"
OUTPUT_DIR = BASE_DIR / "Flow/202512/2025-12-31"

# æ—¥æœ¬èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ãªåŠ©è©ãƒ»åŠ©å‹•è©ç­‰ï¼‰
STOP_WORDS = {
    'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŸ', 'ãŒ', 'ã§', 'ã¦', 'ã¨', 'ã—', 'ã‚Œ', 'ã•', 'ã‚ã‚‹', 'ã„ã‚‹',
    'ã‚‚', 'ã™ã‚‹', 'ã‹ã‚‰', 'ãª', 'ã“ã¨', 'ã¨ã—ã¦', 'ã„', 'ã‚„', 'ã‚Œã‚‹', 'ãªã©', 'ãªã£',
    'ãªã„', 'ã“ã®', 'ãŸã‚', 'ãã®', 'ã‚ã£', 'ã‚ˆã†', 'ã¾ãŸ', 'ã‚‚ã®', 'ã¨ã„ã†', 'ã‚ã‚Š',
    'ã¾ã§', 'ã‚‰ã‚Œ', 'ãªã‚‹', 'ã¸', 'ã‹', 'ã ', 'ã“ã‚Œ', 'ã«ã‚ˆã£ã¦', 'ã«ã‚ˆã‚Š', 'ãŠã‚Š',
    'ã‚ˆã‚Š', 'ã«ã‚ˆã‚‹', 'ãš', 'ãªã‚Š', 'ã‚‰ã‚Œã‚‹', 'ã«ãŠã„ã¦', 'ã°', 'ãªã‹ã£', 'ãªã',
    'ã—ã‹ã—', 'ã«ã¤ã„ã¦', 'ã›', 'ã ã£', 'ãã®å¾Œ', 'ã§ãã‚‹', 'ãã‚Œ', 'ã†', 'ã®ã§',
    'ãªãŠ', 'ã®ã¿', 'ã§ã', 'ã', 'ã¤', 'ã«ãŠã‘ã‚‹', 'ãŠã‚ˆã³', 'ã„ã†', 'ã•ã‚‰ã«',
    'ã§ã‚‚', 'ã‚‰', 'ãŸã‚Š', 'ãã®ä»–', 'ã«é–¢ã™ã‚‹', 'ãŸã¡', 'ã¾ã™', 'ã‚“', 'ãªã‚‰',
    'ã«å¯¾ã—ã¦', 'ç‰¹ã«', 'ã›ã‚‹', 'ãŠã‚ˆã³', 'ã‚ã‚‹ã„ã¯', 'ã¾ã—', 'ã‚‚ã®ã®', 'ã¨ã„ã£ãŸ',
    'ã®ã¯', 'ãã‚‹', 'çš„', 'ä¸­', 'rt', 'x', 'com'
}

def load_bookmarks():
    """ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['bookmarks'], data['metadata']

def extract_keywords(text, top_n=100):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    # URLã‚’é™¤å»
    text = re.sub(r'https?://[^\s]+', '', text)
    text = re.sub(r'x\.com/[^\s]+', '', text)

    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’é™¤å»
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)

    # è¨˜å·ã‚’é™¤å»ï¼ˆå¥èª­ç‚¹ã¯æ®‹ã™ï¼‰
    text = re.sub(r'[ã€Œã€ã€ã€ï¼ˆï¼‰()ã€ã€‘\[\]ï½œãƒ»â€¦~ã€œ]', ' ', text)

    # å˜èªã«åˆ†å‰²ï¼ˆç©ºç™½ã€å¥èª­ç‚¹ã§åˆ†å‰²ï¼‰
    words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ a-zA-Z0-9]+', text)

    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å» & 2æ–‡å­—ä»¥ä¸Š
    words = [w.lower() for w in words if len(w) >= 2 and w.lower() not in STOP_WORDS]

    return words

def categorize_content_type(text):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
    text_lower = text.lower()

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    if any(keyword in text_lower for keyword in ['claude', 'chatgpt', 'gpt', 'llm', 'ai', 'ç”Ÿæˆai', 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ']):
        return 'AIãƒ»ç”ŸæˆAI'
    elif any(keyword in text_lower for keyword in ['startup', 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'pmf', 'saas', 'ãƒ“ã‚¸ãƒã‚¹', 'èµ·æ¥­']):
        return 'ãƒ“ã‚¸ãƒã‚¹ãƒ»èµ·æ¥­'
    elif any(keyword in text_lower for keyword in ['typescript', 'react', 'python', 'docker', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‰', 'é–‹ç™º']):
        return 'é–‹ç™ºãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°'
    elif any(keyword in text_lower for keyword in ['ux', 'ui', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'figma', 'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ']):
        return 'ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ»UX'
    elif any(keyword in text_lower for keyword in ['åŠ¹ç‡åŒ–', 'ç”Ÿç”£æ€§', 'ãƒãƒ¼ãƒˆ', 'ã‚¿ã‚¹ã‚¯ç®¡ç†']):
        return 'ç”Ÿç”£æ€§ãƒ»è‡ªå·±å•“ç™º'
    else:
        return 'ãã®ä»–'

def detect_content_structure(text):
    """æŠ•ç¨¿ã®æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
    features = {
        'has_list': bool(re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©123456789]\s*[ï¼.]', text) or
                        re.search(r'\n[-ãƒ»]\s', text)),
        'has_emoji': bool(re.search(r'[\U0001F300-\U0001F9FF]', text)),
        'has_url': bool(re.search(r'https?://', text)),
        'has_quote': 'å¼•ç”¨' in text or 'RT' in text.upper(),
        'has_code': bool(re.search(r'```|`[^`]+`', text)),
        'has_numbers': bool(re.search(r'\d+%|\d+å€|\d+å††|\d+ä»¶', text)),
        'is_thread': 'ğŸ§µ' in text or 'ã‚¹ãƒ¬ãƒƒãƒ‰' in text,
        'is_tutorial': any(keyword in text for keyword in ['æ–¹æ³•', 'æ‰‹é †', 'ã‚„ã‚Šæ–¹', 'å®Œå…¨ã‚¬ã‚¤ãƒ‰', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«']),
        'is_announcement': any(keyword in text for keyword in ['ç™ºè¡¨', 'ãƒªãƒªãƒ¼ã‚¹', 'å…¬é–‹', 'ãƒ­ãƒ¼ãƒ³ãƒ'])
    }
    return features

def analyze_all_bookmarks():
    """å…¨ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’åˆ†æ"""
    bookmarks, metadata = load_bookmarks()

    print(f"=== Xãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ç‰¹å¾´æŠ½å‡ºé–‹å§‹ ===")
    print(f"ç·ä»¶æ•°: {len(bookmarks)} ä»¶")
    print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ—¥æ™‚: {metadata['scrape_date']}")
    print()

    # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†
    all_keywords = []
    category_counter = Counter()
    structure_stats = defaultdict(int)
    engagement_by_category = defaultdict(list)
    author_posts = defaultdict(int)
    text_lengths = []

    # ç‰¹å¾´çš„ãªæŠ•ç¨¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¾‹ã‚’ä¿å­˜
    examples_by_pattern = defaultdict(list)

    for i, bm in enumerate(bookmarks):
        text = bm.get('text', '')
        likes = bm['engagement'].get('likes', 0)
        retweets = bm['engagement'].get('retweets', 0)
        replies = bm['engagement'].get('replies', 0)
        author = bm.get('author_username', 'unknown')

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = extract_keywords(text)
        all_keywords.extend(keywords)

        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
        category = categorize_content_type(text)
        category_counter[category] += 1
        engagement_by_category[category].append(likes)

        # æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        structure = detect_content_structure(text)
        for key, value in structure.items():
            if value:
                structure_stats[key] += 1
                # ä¾‹ã‚’ä¿å­˜ï¼ˆå„ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€å¤§5ä»¶ï¼‰
                if len(examples_by_pattern[key]) < 5:
                    examples_by_pattern[key].append({
                        'text': text[:200],
                        'author': author,
                        'likes': likes,
                        'url': bm.get('url', '')
                    })

        # æŠ•ç¨¿è€…ã‚«ã‚¦ãƒ³ãƒˆ
        author_posts[author] += 1

        # ãƒ†ã‚­ã‚¹ãƒˆé•·
        text_lengths.append(len(text))

        if (i + 1) % 100 == 0:
            print(f"å‡¦ç†ä¸­... {i + 1}/{len(bookmarks)} ä»¶")

    # çµæœé›†è¨ˆ
    keyword_counter = Counter(all_keywords)
    top_keywords = keyword_counter.most_common(50)
    top_authors = sorted(author_posts.items(), key=lambda x: x[1], reverse=True)[:20]

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ
    avg_engagement_by_category = {
        cat: sum(likes) / len(likes) if likes else 0
        for cat, likes in engagement_by_category.items()
    }

    # ãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ
    avg_text_length = sum(text_lengths) / len(text_lengths)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = {
        'metadata': {
            'analyzed_at': datetime.now().isoformat(),
            'total_bookmarks': len(bookmarks),
            'unique_authors': len(author_posts),
            'analysis_version': '1.0.0'
        },
        'keyword_analysis': {
            'top_50_keywords': [{'word': w, 'count': c} for w, c in top_keywords],
            'total_unique_keywords': len(keyword_counter)
        },
        'category_distribution': {
            cat: {
                'count': count,
                'percentage': round(count / len(bookmarks) * 100, 1),
                'avg_likes': round(avg_engagement_by_category.get(cat, 0), 1)
            }
            for cat, count in category_counter.most_common()
        },
        'content_structure': {
            pattern: {
                'count': count,
                'percentage': round(count / len(bookmarks) * 100, 1),
                'examples': examples_by_pattern[pattern]
            }
            for pattern, count in sorted(structure_stats.items(), key=lambda x: x[1], reverse=True)
        },
        'author_analysis': {
            'top_20_authors': [
                {'username': author, 'post_count': count, 'percentage': round(count / len(bookmarks) * 100, 1)}
                for author, count in top_authors
            ]
        },
        'text_statistics': {
            'average_length': round(avg_text_length, 1),
            'min_length': min(text_lengths),
            'max_length': max(text_lengths)
        }
    }

    # JSONä¿å­˜
    output_file = OUTPUT_DIR / "bookmark_features_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\n=== åˆ†æå®Œäº† ===")
    print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\nã€ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã€‘")
    for cat, data in report['category_distribution'].items():
        print(f"  {cat}: {data['count']}ä»¶ ({data['percentage']}%) - å¹³å‡ã„ã„ã­ {data['avg_likes']}")

    print(f"\nã€TOP 20 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘")
    for kw in report['keyword_analysis']['top_50_keywords'][:20]:
        print(f"  {kw['word']}: {kw['count']}å›")

    print(f"\nã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘")
    for pattern, data in list(report['content_structure'].items())[:10]:
        print(f"  {pattern}: {data['count']}ä»¶ ({data['percentage']}%)")

    print(f"\nã€TOP 10 æŠ•ç¨¿è€…ã€‘")
    for author in report['author_analysis']['top_20_authors'][:10]:
        print(f"  @{author['username']}: {author['post_count']}ä»¶ ({author['percentage']}%)")

    return report

if __name__ == "__main__":
    report = analyze_all_bookmarks()
