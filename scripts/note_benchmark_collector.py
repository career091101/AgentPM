#!/usr/bin/env python3
"""
Noteè¨˜äº‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

3ã‚¸ãƒ£ãƒ³ãƒ«ï¼ˆãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ»AIã€ãƒ“ã‚¸ãƒã‚¹ãƒ»èµ·æ¥­ã€ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãƒ»å‰¯æ¥­ï¼‰ã‹ã‚‰
é«˜å“è³ªãªè¨˜äº‹ã‚’åé›†ã—ã€AIå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

Usage:
    python note_benchmark_collector.py --genre tech_ai --limit 40
    python note_benchmark_collector.py --genre business --limit 40
    python note_benchmark_collector.py --genre creator --limit 40
    python note_benchmark_collector.py --all --limit 120
"""
import argparse
import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BENCHMARK_DIR = Path(__file__).parent.parent / "Stock/programs/å‰¯æ¥­/projects/SNS/knowledge/Note/benchmark"

# ã‚¸ãƒ£ãƒ³ãƒ«å®šç¾©ï¼ˆã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼IDã¯note.comã§ç¢ºèªæ¸ˆã¿ - 2026å¹´1æœˆæ›´æ–°ï¼‰
GENRES = {
    "tech_ai": {
        "name": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ»AI",
        "hashtags": ["AI", "ChatGPT", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", "æ©Ÿæ¢°å­¦ç¿’", "LLM", "ç”ŸæˆAI", "Python", "é–‹ç™º"],
        "creators": [
            "ochyai",       # è½åˆé™½ä¸€
            "norinity1103", # ã®ã‚Šã«ã¦ãƒ
            "takahiroanno", # å®‰é‡è²´åš
            "fladdict",     # æ·±æ´¥è²´ä¹‹ï¼ˆTHE GUILDï¼‰
            "shi3z",        # shi3zï¼ˆæ¸…æ°´äº®ï¼‰
            "mizchi",       # mizchi
            "aiconsulting", # inoue_AI
            "belnon",       # ã‚ã¹ã‚€ã¤ãï¼ˆAIæ´»ç”¨ãƒã‚¬ã‚¸ãƒ³ï¼‰
            "ai_freelancer", # è‡ªç”±äºº@ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹
            "mori_mori_ta",  # æ£®ã€…ç”°
        ],
        "keywords": ["AIæ´»ç”¨", "ChatGPT", "Claude", "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "è‡ªå‹•åŒ–"],
    },
    "business": {
        "name": "ãƒ“ã‚¸ãƒã‚¹ãƒ»èµ·æ¥­",
        "hashtags": ["ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—", "èµ·æ¥­", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "SaaS", "çµŒå–¶", "ãƒ“ã‚¸ãƒã‚¹", "äº‹æ¥­é–‹ç™º"],
        "creators": [
            "kensuu",       # ã‘ã‚“ã™ã†ï¼ˆã‚¢ãƒ«ï¼‰
            "tamesue",      # ç‚ºæœ«å¤§
            "kishidanami",  # å²¸ç”°å¥ˆç¾
            "kajiken0630",  # æ¢¶è°·å¥äºº
            "ikedanoriyuki", # æ± ç”°ç´€è¡Œ
            "osamu_fujitani", # è—¤è°·æ²»
            "juninagao",     # æ°¸å°¾æº–ä¸€
            "dada696",       # ä¸–è‰¯é™½ä¸€
            "suadd",         # suadd
            "shota_y",       # æ¨ªå±±ç¿”å¤ª
        ],
        "keywords": ["èµ·æ¥­", "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "æˆé•·æˆ¦ç•¥"],
    },
    "creator": {
        "name": "ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ãƒ»å‰¯æ¥­",
        "hashtags": ["å‰¯æ¥­", "SNSé‹ç”¨", "ãƒãƒã‚¿ã‚¤ã‚º", "å€‹äººãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°", "ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹", "ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼"],
        "creators": [
            "golddust",      # ã‚‚ãˆãï¼ˆå†™çœŸãƒ»æ—…è¡Œï¼‰
            "fujiwarahana",  # è—¤åŸè¯ï¼ˆæ–‡ç« è¡“ï¼‰
            "harunoyuki0906", # ã¯ã‚‹ã®ã‚†ã
            "yuki_note_writer", # ã‚†ã
            "okeydon",       # ãŠã‘ã„ã©ã‚“
            "mayumi_tanaka", # ç”°ä¸­çœŸå¼“
            "hiroyuki_note", # ã²ã‚ã‚†ãï¼ˆå‰¯æ¥­ç³»ï¼‰
            "sachiyo",       # ã•ã¡ã‚ˆ
            "yurufuwa_life", # ã‚†ã‚‹ãµã‚ãƒ©ã‚¤ãƒ•
            "freelance_hero", # ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ãƒ’ãƒ¼ãƒ­ãƒ¼
        ],
        "keywords": ["å‰¯æ¥­", "åç›ŠåŒ–", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼", "noteåç›Š"],
    },
}

# åé›†æœŸé–“ï¼ˆéå»1å¹´ï¼‰
COLLECT_START_DATE = datetime.now() - timedelta(days=365)
COLLECT_END_DATE = datetime.now()


def now_iso():
    return datetime.now().isoformat(timespec="seconds")


def load_cookies(session, cookies_path):
    """Cookieãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¨­å®š"""
    if not cookies_path.exists():
        return False
    with open(cookies_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    cookies = data.get("cookies", data)
    for c in cookies:
        session.cookies.set(
            c.get("name"),
            c.get("value"),
            domain=c.get("domain"),
            path=c.get("path", "/"),
        )
    return True


def get_session(cookies_path=None):
    """èªè¨¼æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    })
    if cookies_path and cookies_path.exists():
        load_cookies(session, cookies_path)
    return session


def fetch_json(session, url, sleep_s=1.0, retries=3):
    """JSON APIã‚’å–å¾—"""
    for attempt in range(retries):
        try:
            resp = session.get(url, timeout=30)
            resp.raise_for_status()
            time.sleep(sleep_s)
            return resp.json()
        except Exception as e:
            if attempt == retries - 1:
                raise
            time.sleep(2 ** attempt)
    return None


def fetch_html(session, url, sleep_s=1.0):
    """HTMLãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    resp = session.get(url, timeout=30)
    resp.raise_for_status()
    time.sleep(sleep_s)
    return resp.text


def fetch_creator_articles(session, creator_id, limit=20):
    """ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã®è¨˜äº‹ä¸€è¦§ã‚’å–å¾—"""
    articles = []
    page = 1

    while len(articles) < limit:
        # ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼è¨˜äº‹ä¸€è¦§API
        url = f"https://note.com/api/v2/creators/{creator_id}/contents?kind=note&page={page}"
        try:
            data = fetch_json(session, url, sleep_s=0.5)
            if not data:
                break

            notes = data.get("data", {}).get("contents", [])
            if not notes:
                break

            for note in notes:
                # éå»1å¹´ä»¥å†…ã®è¨˜äº‹ã®ã¿
                published = note.get("publishAt", "")
                if published:
                    try:
                        pub_date = datetime.fromisoformat(published.replace("Z", "+00:00"))
                        if pub_date.replace(tzinfo=None) < COLLECT_START_DATE:
                            continue
                    except:
                        pass

                articles.append({
                    "id": note.get("key", ""),
                    "title": note.get("name", ""),
                    "url": f"https://note.com/{creator_id}/n/{note.get('key', '')}",
                    "author": note.get("user", {}).get("name", creator_id),
                    "author_id": creator_id,
                    "published_at": published,
                    "like_count": note.get("likeCount", 0),
                    "comment_count": note.get("commentCount", 0),
                    "is_paid": note.get("price", 0) > 0,
                    "price": note.get("price", 0),
                    "creator_source": creator_id,
                })

                if len(articles) >= limit:
                    break

            page += 1
            if page > 10:  # æœ€å¤§10ãƒšãƒ¼ã‚¸
                break

        except Exception as e:
            print(f"  Warning: Failed to fetch from {creator_id}: {e}")
            break

    return articles


def collect_genre_articles(session, genre_key, limit=40):
    """ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã«è¨˜äº‹ã‚’åé›†ï¼ˆã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼APIãƒ™ãƒ¼ã‚¹ï¼‰"""
    genre = GENRES.get(genre_key)
    if not genre:
        raise ValueError(f"Unknown genre: {genre_key}")

    print(f"\nğŸ“‚ {genre['name']}ã‚¸ãƒ£ãƒ³ãƒ«ã®è¨˜äº‹åé›†é–‹å§‹...")

    all_articles = []
    seen_ids = set()

    # ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã”ã¨ã«åé›†
    articles_per_creator = max(5, limit // len(genre["creators"]))

    for creator_id in genre["creators"]:
        print(f"  ğŸ‘¤ {creator_id} ã®è¨˜äº‹ã‚’å–å¾—ä¸­...")
        articles = fetch_creator_articles(session, creator_id, limit=articles_per_creator)

        for article in articles:
            if article["id"] not in seen_ids:
                seen_ids.add(article["id"])
                article["genre"] = genre_key
                all_articles.append(article)

        print(f"     â†’ {len(articles)}ä»¶å–å¾—ï¼ˆç´¯è¨ˆ: {len(all_articles)}ä»¶ï¼‰")

        if len(all_articles) >= limit:
            break

    # ã‚¹ã‚­æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
    all_articles.sort(key=lambda x: x.get("like_count", 0), reverse=True)
    selected = all_articles[:limit]

    print(f"  âœ“ {len(selected)}ä»¶ã‚’é¸æŠï¼ˆã‚¹ã‚­æ•°ä¸Šä½ï¼‰")
    return selected


def extract_article_content(session, url):
    """è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡º"""
    try:
        html = fetch_html(session, url, sleep_s=1.0)
        soup = BeautifulSoup(html, "html.parser")

        # ã‚¿ã‚¤ãƒˆãƒ«
        title_el = soup.find("h1")
        title = title_el.get_text(strip=True) if title_el else ""

        # æœ¬æ–‡
        article = soup.find("article")
        if article:
            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for el in article.find_all(["script", "style", "nav", "footer"]):
                el.decompose()

            body_html = str(article)
            body_text = article.get_text(separator="\n", strip=True)
            body_md = md(body_html, heading_style="ATX")
        else:
            body_html = ""
            body_text = ""
            body_md = ""

        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°
        hashtags = []
        for a in soup.find_all("a", href=True):
            if "/hashtag/" in a["href"]:
                tag = a.get_text(strip=True)
                if tag and tag not in hashtags:
                    hashtags.append(tag)

        # æ–‡å­—æ•°
        word_count = len(body_text)

        # è¦‹å‡ºã—æ•°
        heading_count = len(soup.find_all(["h2", "h3"]))

        return {
            "title": title,
            "body_text": body_text,
            "body_markdown": body_md,
            "hashtags": hashtags,
            "word_count": word_count,
            "heading_count": heading_count,
        }

    except Exception as e:
        print(f"    Warning: Failed to extract content: {e}")
        return None


def calculate_quality_score(article, content):
    """å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0
    details = {}

    # æ–‡å­—æ•° (2000-5000å­—ãŒæœ€é©) - 20ç‚¹
    wc = content.get("word_count", 0)
    if 2000 <= wc <= 5000:
        score += 20
        details["word_count"] = {"score": 20, "value": wc, "status": "optimal"}
    elif 1000 <= wc < 2000 or 5000 < wc <= 8000:
        score += 15
        details["word_count"] = {"score": 15, "value": wc, "status": "acceptable"}
    elif wc > 500:
        score += 10
        details["word_count"] = {"score": 10, "value": wc, "status": "short"}
    else:
        details["word_count"] = {"score": 0, "value": wc, "status": "too_short"}

    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•° (3-5å€‹ãŒæœ€é©) - 15ç‚¹
    tag_count = len(content.get("hashtags", []))
    if 3 <= tag_count <= 5:
        score += 15
        details["hashtags"] = {"score": 15, "count": tag_count, "status": "optimal"}
    elif 1 <= tag_count <= 7:
        score += 10
        details["hashtags"] = {"score": 10, "count": tag_count, "status": "acceptable"}
    else:
        details["hashtags"] = {"score": 0, "count": tag_count, "status": "suboptimal"}

    # ã‚¹ã‚­æ•°ï¼ˆç›¸å¯¾è©•ä¾¡ï¼‰- 25ç‚¹
    likes = article.get("like_count", 0)
    if likes >= 100:
        score += 25
        details["likes"] = {"score": 25, "count": likes, "status": "high"}
    elif likes >= 50:
        score += 20
        details["likes"] = {"score": 20, "count": likes, "status": "good"}
    elif likes >= 20:
        score += 15
        details["likes"] = {"score": 15, "count": likes, "status": "moderate"}
    elif likes >= 5:
        score += 10
        details["likes"] = {"score": 10, "count": likes, "status": "low"}
    else:
        details["likes"] = {"score": 0, "count": likes, "status": "very_low"}

    # ã‚³ãƒ¡ãƒ³ãƒˆæ•° - 10ç‚¹
    comments = article.get("comment_count", 0)
    if comments >= 1:
        score += 10
        details["comments"] = {"score": 10, "count": comments, "status": "has_comments"}
    else:
        details["comments"] = {"score": 0, "count": comments, "status": "no_comments"}

    # è¦‹å‡ºã—æ§‹é€  (3å€‹ä»¥ä¸ŠãŒæœ€é©) - 20ç‚¹
    headings = content.get("heading_count", 0)
    if headings >= 3:
        score += 20
        details["structure"] = {"score": 20, "headings": headings, "status": "well_structured"}
    elif headings >= 1:
        score += 10
        details["structure"] = {"score": 10, "headings": headings, "status": "has_structure"}
    else:
        details["structure"] = {"score": 0, "headings": headings, "status": "flat"}

    # æŠ•ç¨¿æ™‚åˆ»ï¼ˆ19æ™‚å‰å¾ŒãŒæœ€é©ï¼‰- 10ç‚¹
    published = article.get("published_at", "")
    if published:
        try:
            pub_dt = datetime.fromisoformat(published.replace("Z", "+00:00"))
            hour = pub_dt.hour
            if 18 <= hour <= 20:
                score += 10
                details["timing"] = {"score": 10, "hour": hour, "status": "golden_hour"}
            elif 17 <= hour <= 21:
                score += 5
                details["timing"] = {"score": 5, "hour": hour, "status": "good_time"}
            else:
                details["timing"] = {"score": 0, "hour": hour, "status": "off_peak"}
        except Exception:
            details["timing"] = {"score": 0, "status": "unknown"}

    return score, details


def save_article(article, content, quality_score, quality_details, output_dir):
    """è¨˜äº‹ã‚’ä¿å­˜"""
    genre_dir = output_dir / "raw" / article["genre"]
    genre_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    article_id = article["id"]
    published = article.get("published_at", "")[:10] or "unknown"
    safe_title = re.sub(r"[^\w\s-]", "", article.get("title", "untitled"))[:50]
    filename = f"{published}_{article_id}_{safe_title}"

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata = {
        **article,
        "content": {
            "word_count": content.get("word_count", 0),
            "heading_count": content.get("heading_count", 0),
            "hashtags": content.get("hashtags", []),
        },
        "quality": {
            "score": quality_score,
            "tier": "A" if quality_score >= 80 else "B" if quality_score >= 60 else "C" if quality_score >= 40 else "D",
            "details": quality_details,
        },
        "collected_at": now_iso(),
    }

    # JSONä¿å­˜
    json_path = genre_dir / f"{filename}.json"
    json_path.write_text(json.dumps(metadata, ensure_ascii=False, indent=2), encoding="utf-8")

    # Markdownä¿å­˜ï¼ˆç„¡æ–™è¨˜äº‹ã®ã¿æœ¬æ–‡ï¼‰
    if not article.get("is_paid", False):
        md_path = genre_dir / f"{filename}.md"
        md_content = f"# {article.get('title', 'Untitled')}\n\n"
        md_content += f"> Author: {article.get('author', 'Unknown')}\n"
        md_content += f"> Published: {published}\n"
        md_content += f"> Likes: {article.get('like_count', 0)}\n"
        md_content += f"> Quality Score: {quality_score}/100\n\n"
        md_content += "---\n\n"
        md_content += content.get("body_markdown", "")
        md_path.write_text(md_content, encoding="utf-8")

    return filename


def main():
    parser = argparse.ArgumentParser(description="Noteè¨˜äº‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åé›†ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--genre", type=str, choices=["tech_ai", "business", "creator"],
                        help="åé›†ã™ã‚‹ã‚¸ãƒ£ãƒ³ãƒ«")
    parser.add_argument("--all", action="store_true", help="å…¨ã‚¸ãƒ£ãƒ³ãƒ«ã‚’åé›†")
    parser.add_argument("--limit", type=int, default=40, help="ã‚¸ãƒ£ãƒ³ãƒ«ã‚ãŸã‚Šã®è¨˜äº‹æ•°")
    parser.add_argument("--cookies", type=str,
                        default="Stock/programs/å‰¯æ¥­/projects/æœˆåˆŠã‚¢ãƒ—ãƒªãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°/data/cookies/note_cookies.json",
                        help="Cookiesãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--output", type=str, default=str(BENCHMARK_DIR), help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--skip-content", action="store_true", help="æœ¬æ–‡å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    args = parser.parse_args()

    output_dir = Path(args.output).expanduser().resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    # Cookieè¨­å®š
    cookies_path = Path(args.cookies).expanduser().resolve()
    if not cookies_path.is_absolute():
        cookies_path = Path(__file__).parent.parent / args.cookies

    session = get_session(cookies_path if cookies_path.exists() else None)

    print("=" * 60)
    print("ğŸ“š Noteè¨˜äº‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åé›†")
    print("=" * 60)
    print(f"æœŸé–“: {COLLECT_START_DATE.strftime('%Y-%m-%d')} ï½ {COLLECT_END_DATE.strftime('%Y-%m-%d')}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    print()

    # åé›†å¯¾è±¡ã‚¸ãƒ£ãƒ³ãƒ«
    if args.all:
        genres_to_collect = list(GENRES.keys())
    elif args.genre:
        genres_to_collect = [args.genre]
    else:
        print("Error: --genre ã¾ãŸã¯ --all ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        sys.exit(1)

    all_results = []

    for genre_key in genres_to_collect:
        articles = collect_genre_articles(session, genre_key, limit=args.limit)

        print(f"\n  ğŸ“¥ æœ¬æ–‡å–å¾—ä¸­...")
        for idx, article in enumerate(articles, 1):
            print(f"    [{idx}/{len(articles)}] {article['title'][:40]}...")

            if args.skip_content:
                content = {"word_count": 0, "heading_count": 0, "hashtags": []}
            else:
                content = extract_article_content(session, article["url"])
                if not content:
                    content = {"word_count": 0, "heading_count": 0, "hashtags": []}

            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            score, details = calculate_quality_score(article, content)

            # ä¿å­˜
            filename = save_article(article, content, score, details, output_dir)

            all_results.append({
                "genre": genre_key,
                "article_id": article["id"],
                "title": article["title"],
                "quality_score": score,
                "filename": filename,
            })

            print(f"      â†’ Score: {score}/100 ({details.get('likes', {}).get('status', 'unknown')})")

    # ã‚µãƒãƒªãƒ¼ä¿å­˜
    summary = {
        "collected_at": now_iso(),
        "total_articles": len(all_results),
        "by_genre": {},
        "articles": all_results,
    }

    for genre_key in genres_to_collect:
        genre_articles = [a for a in all_results if a["genre"] == genre_key]
        avg_score = sum(a["quality_score"] for a in genre_articles) / len(genre_articles) if genre_articles else 0
        summary["by_genre"][genre_key] = {
            "count": len(genre_articles),
            "avg_quality_score": round(avg_score, 1),
        }

    summary_path = output_dir / "metadata" / "collection_summary.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    summary_path.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")

    print("\n" + "=" * 60)
    print("âœ… åé›†å®Œäº†!")
    print("=" * 60)
    print(f"ç·è¨˜äº‹æ•°: {len(all_results)}ä»¶")
    for genre_key, stats in summary["by_genre"].items():
        print(f"  {GENRES[genre_key]['name']}: {stats['count']}ä»¶ (å¹³å‡ã‚¹ã‚³ã‚¢: {stats['avg_quality_score']})")
    print(f"\nå‡ºåŠ›å…ˆ: {output_dir}")


if __name__ == "__main__":
    main()
