#!/usr/bin/env python3
"""
LinkedInæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆæ¤œå‡ºå›é¿å®Ÿè£…ï¼‰

Usage:
    source venv_linkedin/bin/activate
    python linkedin_scraper.py --profile-url https://www.linkedin.com/in/hidetoshitakano/ --target-count 50
"""

import asyncio
import os
import json
import logging
import re
import random
from datetime import datetime
from urllib.parse import urlparse
from playwright.async_api import async_playwright, Page
from bs4 import BeautifulSoup

# Configuration
BASE_DIR = "/Users/yuichi/AIPM/aipm_v0"
SCRIPTS_DIR = os.path.join(BASE_DIR, "scripts")
OUTPUT_DIR = os.path.join(BASE_DIR, "Flow/202512/2025-12-31")
USER_DATA_DIR = os.path.join(SCRIPTS_DIR, "linkedin_user_data")

# LinkedIn Selectors (2025å¹´æƒ³å®š)
POST_SELECTORS = {
    'container': 'div[data-id*="urn:li:activity"]',
    'text': 'div.feed-shared-update-v2__description, div.update-components-text',
    'timestamp': 'time.update-components-actor__sub-description, span.update-components-actor__sub-description time',
    'reactions': 'span.social-details-social-counts__reactions-count',
    'comments': 'button.social-details-social-counts__comments span',
    'shares': 'button.social-details-social-counts__shares-count',
    'author': 'span.update-components-actor__name'
}

# Detection Avoidance Settings
MIN_SCROLL_DELAY = 2.0  # seconds
MAX_SCROLL_DELAY = 4.5  # seconds
WARNING_SLEEP = 60  # seconds when rate limit detected

# Logging Configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(SCRIPTS_DIR, "linkedin_scraper.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class LoginTimeoutError(Exception):
    """ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"""
    pass


class LinkedInScraper:
    """LinkedInæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆæ¤œå‡ºå›é¿å®Ÿè£…ï¼‰"""

    def __init__(self, profile_url: str, target_count: int = 50):
        self.profile_url = profile_url
        self.target_count = target_count
        self.posts_data = []
        self.user_data_dir = USER_DATA_DIR
        os.makedirs(self.user_data_dir, exist_ok=True)
        os.makedirs(OUTPUT_DIR, exist_ok=True)

    async def initialize_browser(self):
        """Persistent Contextèµ·å‹•ï¼ˆèªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³å†åˆ©ç”¨ï¼‰"""
        p = await async_playwright().start()
        browser = await p.chromium.launch_persistent_context(
            user_data_dir=self.user_data_dir,
            headless=False,  # LinkedInæ¤œå‡ºå›é¿
            slow_mo=100,     # äººé–“ã‚‰ã—ã„æ“ä½œ
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='ja-JP',
            timezone_id='Asia/Tokyo'
        )
        return p, browser

    async def wait_for_login(self, page: Page, max_wait: int = 300) -> bool:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿï¼ˆæœ€å¤§5åˆ†ï¼‰"""
        logger.info("=" * 60)
        logger.info("ãƒ–ãƒ©ã‚¦ã‚¶ãŒèµ·å‹•ã—ã¾ã—ãŸã€‚LinkedInã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")
        logger.info("ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¨è‡ªå‹•çš„ã«æŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™...")
        logger.info("=" * 60)

        for i in range(max_wait // 5):
            try:
                # Wait for page to be in stable state
                await page.wait_for_load_state("domcontentloaded", timeout=3000)
                content = await page.content()
                # ãƒ­ã‚°ã‚¤ãƒ³æ¤œå‡ºï¼šFeed, ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ç­‰ã®è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                if any(keyword in content for keyword in ["Feed", "ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£", "activity", "feed-shared"]):
                    logger.info("âœ… ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªï¼æŠ½å‡ºã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                    return True
            except Exception as e:
                # Navigation in progress is expected during login
                logger.debug(f"Login check (navigation in progress): {type(e).__name__}")
                pass

            if i % 6 == 0:  # 30ç§’ã”ã¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿä¸­... ({(i * 5) // 60}åˆ†{(i * 5) % 60}ç§’çµŒé)")

            await asyncio.sleep(5)

        raise LoginTimeoutError(f"Timeout: ãƒ­ã‚°ã‚¤ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆ{max_wait}ç§’çµŒéï¼‰")

    async def human_scroll(self, page: Page):
        """äººé–“ã‚‰ã—ã„ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‹•ä½œ"""
        scroll_count = random.randint(5, 10)
        for _ in range(scroll_count):
            scroll_amount = random.uniform(0.6, 0.8)
            await page.evaluate(f"window.scrollBy(0, window.innerHeight * {scroll_amount})")
            delay = random.uniform(MIN_SCROLL_DELAY, MAX_SCROLL_DELAY)
            logger.debug(f"Scrolling... (delay: {delay:.2f}s)")
            await asyncio.sleep(delay)

    async def detect_warning(self, page: Page) -> bool:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™è­¦å‘Šæ¤œå‡º"""
        content = await page.content()
        warning_keywords = [
            "You're visiting too many pages",
            "ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ",
            "å¤šæ•°ã®ãƒšãƒ¼ã‚¸ã‚’è¨ªå•",
            "rate-limit"
        ]

        for keyword in warning_keywords:
            if keyword.lower() in content.lower():
                logger.warning(f"âš ï¸  Rate limit detected: '{keyword}'! Sleeping {WARNING_SLEEP} seconds...")
                await asyncio.sleep(WARNING_SLEEP)
                return True
        return False

    async def scrape_posts(self, page: Page):
        """æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ30-50ä»¶ï¼‰"""
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®activityãƒšãƒ¼ã‚¸ã«ç§»å‹•
        activity_url = f"{self.profile_url}/recent-activity/all/"
        logger.info(f"Navigating to: {activity_url}")
        await page.goto(activity_url, wait_until="domcontentloaded")
        await asyncio.sleep(3)

        posts_loaded = 0
        scroll_attempts = 0
        max_scrolls = 20
        previous_count = 0
        stagnant_count = 0

        while posts_loaded < self.target_count and scroll_attempts < max_scrolls:
            # äººé–“ã‚‰ã—ã„ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            await self.human_scroll(page)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è­¦å‘Šã‚’æ¤œå‡º
            if await self.detect_warning(page):
                break

            # æŠ•ç¨¿è¦ç´ ã‚’å–å¾—
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')

            # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
            post_elements = (
                soup.select(POST_SELECTORS['container']) or
                soup.select('div[data-urn*="activity"]') or
                soup.select('div.feed-shared-update-v2')
            )

            posts_loaded = len(post_elements)
            logger.info(f"Loaded {posts_loaded} posts (target: {self.target_count}, scroll: {scroll_attempts})")

            # é€²æ—ãŒåœæ»ã—ã¦ã„ã‚‹å ´åˆã¯çµ‚äº†
            if posts_loaded == previous_count:
                stagnant_count += 1
                if stagnant_count >= 3:
                    logger.warning(f"æŠ•ç¨¿æ•°ãŒ3å›é€£ç¶šã§å¤‰åŒ–ãªã—ã€‚ç¾åœ¨{posts_loaded}ä»¶ã§çµ‚äº†ã—ã¾ã™ã€‚")
                    break
            else:
                stagnant_count = 0
                previous_count = posts_loaded

            scroll_attempts += 1

        logger.info(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ã€‚{posts_loaded}ä»¶ã®æŠ•ç¨¿è¦ç´ ã‚’æŠ½å‡ºã—ã¾ã™ã€‚")

        # æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        return await self._extract_all_posts(post_elements[:self.target_count])

    async def _extract_all_posts(self, post_elements):
        """å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        posts = []

        for i, post_element in enumerate(post_elements, 1):
            try:
                post_data = self._extract_post_data(post_element, i)
                if post_data:
                    posts.append(post_data)
                    logger.debug(f"Extracted post {i}/{len(post_elements)}: {post_data['char_count']} chars")
            except Exception as e:
                logger.error(f"Failed to extract post {i}: {e}")

        logger.info(f"âœ… {len(posts)}ä»¶ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        return posts

    def _extract_post_data(self, post_element, index: int) -> dict:
        """å€‹åˆ¥æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        # post_id
        post_id = post_element.get('data-id') or post_element.get('data-urn') or f"unknown_{index}"

        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        text_elem = (
            post_element.select_one(POST_SELECTORS['text']) or
            post_element.select_one('div[dir="ltr"]') or
            post_element.find('div', class_=re.compile(r'.*update.*text.*'))
        )
        text = text_elem.get_text(strip=True) if text_elem else ""

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        timestamp_elem = (
            post_element.select_one(POST_SELECTORS['timestamp']) or
            post_element.find('time')
        )
        timestamp = timestamp_elem.get('datetime') if timestamp_elem and timestamp_elem.has_attr('datetime') else None
        if not timestamp and timestamp_elem:
            timestamp = timestamp_elem.get_text(strip=True)

        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ½å‡º
        reactions = self._extract_count(post_element, POST_SELECTORS['reactions'])
        comments = self._extract_count(post_element, POST_SELECTORS['comments'])
        shares = self._extract_count(post_element, POST_SELECTORS['shares'])

        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ½å‡º
        hashtags = self._extract_hashtags(text)

        # URLï¼ˆæ¨æ¸¬ï¼‰
        url = f"https://www.linkedin.com/feed/update/{post_id}" if post_id.startswith('urn:') else None

        return {
            "post_id": post_id,
            "author": "Hidetoshi Takano",
            "published_at": timestamp,
            "text": text,
            "char_count": len(text),
            "reactions": reactions,
            "comments": comments,
            "shares": shares,
            "hashtags": hashtags,
            "url": url
        }

    def _extract_count(self, element, selector: str) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ•°å€¤æŠ½å‡º"""
        try:
            count_elem = element.select_one(selector)
            if not count_elem:
                return 0

            text = count_elem.get_text(strip=True)
            # "45", "1,234", "1K" ãªã©ã®å½¢å¼ã«å¯¾å¿œ
            text = text.replace(',', '').replace('K', '000').replace('k', '000')
            match = re.search(r'\d+', text)
            return int(match.group()) if match else 0
        except Exception as e:
            logger.debug(f"Count extraction failed for selector '{selector}': {e}")
            return 0

    def _extract_hashtags(self, text: str) -> list:
        """ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ½å‡º"""
        return re.findall(r'#[\w\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+', text)

    def save_to_json(self, posts: list, output_path: str):
        """JSONä¿å­˜ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰"""
        data = {
            "posts": posts,
            "metadata": {
                "profile_url": self.profile_url,
                "scraped_at": datetime.now().isoformat(),
                "total_posts": len(posts),
                "scraper_version": "1.0.0"
            }
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ JSON saved: {output_path}")

    async def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("=" * 60)
        logger.info("LinkedInæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•")
        logger.info(f"Profile: {self.profile_url}")
        logger.info(f"Target: {self.target_count} posts")
        logger.info("=" * 60)

        p, browser = await self.initialize_browser()
        page = await browser.new_page()

        try:
            # LinkedIné–‹ã
            await page.goto("https://www.linkedin.com")
            await asyncio.sleep(2)

            # ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿ
            await self.wait_for_login(page)

            # æŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            posts = await self.scrape_posts(page)
            self.posts_data = posts

            # JSONä¿å­˜
            output_path = os.path.join(OUTPUT_DIR, "takano_linkedin_posts.json")
            self.save_to_json(posts, output_path)

            logger.info("=" * 60)
            logger.info(f"âœ… Complete! {len(posts)} posts scraped.")
            logger.info(f"Output: {output_path}")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ Error: {e}", exc_info=True)
            raise
        finally:
            await browser.close()
            await p.stop()


async def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse

    parser = argparse.ArgumentParser(description="LinkedInæŠ•ç¨¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼")
    parser.add_argument('--profile-url', required=True, help='LinkedInãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URL')
    parser.add_argument('--target-count', type=int, default=50, help='å–å¾—ã™ã‚‹æŠ•ç¨¿æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰')

    args = parser.parse_args()

    scraper = LinkedInScraper(
        profile_url=args.profile_url,
        target_count=args.target_count
    )

    await scraper.run()


if __name__ == "__main__":
    asyncio.run(main())
