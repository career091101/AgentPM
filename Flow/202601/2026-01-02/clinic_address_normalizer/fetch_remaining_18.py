#!/usr/bin/env python3
"""
æ®‹ã‚Š18ã‚¯ã‚¨ãƒªï¼ˆæ„›åª›ã€œé¹¿å…å³¶ï¼‰ã®å­ä¾›å‘ã‘æ­¯ç§‘åŒ»é™¢ãƒ‡ãƒ¼ã‚¿å–å¾—
"""

import os
import csv
import time
import requests
from datetime import datetime

API_KEY = os.environ.get("GOOGLE_MAPS_API_KEY")

if not API_KEY:
    print("âŒ Error: GOOGLE_MAPS_API_KEY environment variable not set")
    exit(1)

# æ®‹ã‚Š9çœŒï¼ˆæ„›åª›ã€œé¹¿å…å³¶ï¼‰
PREFECTURES = [
    "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ",
    "ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ", "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ"
]

SPECIALTIES = ["å°å…æ­¯ç§‘", "çŸ¯æ­£æ­¯ç§‘"]


def search_clinics(prefecture: str, specialty: str) -> list:
    """æ­¯ç§‘åŒ»é™¢ã‚’æ¤œç´¢"""
    query = f"{specialty} {prefecture}"

    search_url = "https://places.googleapis.com/v1/places:searchText"
    search_headers = {
        "X-Goog-Api-Key": API_KEY,
        "Content-Type": "application/json",
        "X-Goog-FieldMask": "places.id,places.displayName,places.formattedAddress,places.rating,places.userRatingCount,places.googleMapsUri,places.websiteUri"
    }
    search_body = {
        "textQuery": query,
        "languageCode": "ja",
        "maxResultCount": 20
    }

    try:
        response = requests.post(search_url, headers=search_headers, json=search_body, timeout=20)

        if response.status_code != 200:
            print(f" âŒ {response.status_code}")
            return []

        data = response.json()
        places = data.get("places", [])

        if not places:
            return []

        results = []
        for place in places:
            place_id = place.get("id")
            display_name = place.get("displayName", {}).get("text", "")
            formatted_address = place.get("formattedAddress", "")
            rating = place.get("rating", 0)
            user_rating_count = place.get("userRatingCount", 0)
            google_maps_uri = place.get("googleMapsUri", "")
            website_uri = place.get("websiteUri", "")

            # Place Details ã§éƒµä¾¿ç•ªå·å–å¾—
            postal_code = ""
            details_url = f"https://places.googleapis.com/v1/places/{place_id}"
            details_headers = {
                "X-Goog-Api-Key": API_KEY,
                "X-Goog-FieldMask": "addressComponents"
            }
            details_params = {"languageCode": "ja"}

            try:
                details_response = requests.get(details_url, headers=details_headers, params=details_params, timeout=20)
                if details_response.status_code == 200:
                    details_data = details_response.json()
                    address_components = details_data.get("addressComponents", [])
                    for component in address_components:
                        if "postal_code" in component.get("types", []):
                            postal_code = component.get("longText", "")
                            break
            except:
                pass

            results.append({
                "place_id": place_id,
                "åŒ»é™¢å": display_name,
                "ä½æ‰€": formatted_address,
                "éƒµä¾¿ç•ªå·": postal_code,
                "è©•ä¾¡": rating,
                "å£ã‚³ãƒŸä»¶æ•°": user_rating_count,
                "Google Maps URL": google_maps_uri,
                "å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ": website_uri,
                "éƒ½é“åºœçœŒ": prefecture,
                "è¨ºç™‚ç§‘ç›®": specialty
            })

        return results

    except Exception as e:
        print(f" âŒ {e}")
        return []


def filter_results(results: list) -> list:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ãƒ¬ãƒ“ãƒ¥ãƒ¼50ä»¶ä»¥ä¸Šã€è©•ä¾¡3.0ä»¥ä¸Š"""
    filtered = []

    for row in results:
        address = row["ä½æ‰€"]
        rating = row["è©•ä¾¡"]
        review_count = row["å£ã‚³ãƒŸä»¶æ•°"]

        if "åŒ—æµ·é“" in address or "æ²–ç¸„" in address:
            continue
        if review_count < 50:
            continue
        if rating and rating < 3.0:
            continue

        filtered.append(row)

    return filtered


def deduplicate(results: list) -> list:
    """place_idã§é‡è¤‡å‰Šé™¤"""
    seen = set()
    unique = []

    for row in results:
        pid = row["place_id"]
        if pid not in seen:
            seen.add(pid)
            unique.append(row)

    return unique


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
print("=" * 60)
print("æ®‹ã‚Š18ã‚¯ã‚¨ãƒªãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ„›åª›ã€œé¹¿å…å³¶ï¼‰")
print("=" * 60)
print(f"å¯¾è±¡: {len(PREFECTURES)}çœŒ Ã— {len(SPECIALTIES)}ãƒ‘ã‚¿ãƒ¼ãƒ³ = {len(PREFECTURES) * len(SPECIALTIES)}ã‚¯ã‚¨ãƒª\n")

all_results = []
total_queries = len(PREFECTURES) * len(SPECIALTIES)
current_query = 0

for prefecture in PREFECTURES:
    for specialty in SPECIALTIES:
        current_query += 1
        print(f"[{current_query}/{total_queries}] {prefecture} - {specialty}...", end="", flush=True)

        results = search_clinics(prefecture, specialty)
        all_results.extend(results)

        print(f" âœ… {len(results)}ä»¶")
        time.sleep(0.5)  # Rate Limitå¯¾ç­–ï¼ˆå°‘ã—é•·ã‚ã«ï¼‰

print(f"\n{'=' * 60}")
print(f"æ¤œç´¢å®Œäº†: {len(all_results)}ä»¶ï¼ˆé‡è¤‡å«ã‚€ï¼‰")

# é‡è¤‡å‰Šé™¤
unique_results = deduplicate(all_results)
print(f"é‡è¤‡å‰Šé™¤å¾Œ: {len(unique_results)}ä»¶")

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
filtered_results = filter_results(unique_results)
print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered_results)}ä»¶")

# CSVå‡ºåŠ›
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = f"pediatric_dental_remaining_{timestamp}.csv"

if filtered_results:
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ["åŒ»é™¢å", "ä½æ‰€", "éƒµä¾¿ç•ªå·", "è©•ä¾¡", "å£ã‚³ãƒŸä»¶æ•°", "Google Maps URL", "å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ", "éƒ½é“åºœçœŒ", "è¨ºç™‚ç§‘ç›®"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in filtered_results:
            writer.writerow({k: row[k] for k in fieldnames})

    print(f"\nâœ… å‡ºåŠ›å®Œäº†: {output_file}")

    # çµ±è¨ˆ
    if filtered_results:
        avg_rating = sum(r["è©•ä¾¡"] for r in filtered_results if r["è©•ä¾¡"]) / len([r for r in filtered_results if r["è©•ä¾¡"]])
        avg_reviews = sum(r["å£ã‚³ãƒŸä»¶æ•°"] for r in filtered_results) / len(filtered_results)
        has_website = sum(1 for r in filtered_results if r["å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ"])

        print(f"\nğŸ“Š å“è³ªæŒ‡æ¨™:")
        print(f"  å¹³å‡è©•ä¾¡: â­{avg_rating:.2f}")
        print(f"  å¹³å‡å£ã‚³ãƒŸä»¶æ•°: {avg_reviews:.0f}ä»¶")
        print(f"  å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚ã‚Š: {has_website}/{len(filtered_results)} ({has_website/len(filtered_results)*100:.1f}%)")

        # éƒ½é“åºœçœŒåˆ¥å†…è¨³
        from collections import Counter
        prefecture_counts = Counter(r["éƒ½é“åºœçœŒ"] for r in filtered_results)
        print(f"\néƒ½é“åºœçœŒåˆ¥å†…è¨³:")
        for pref, count in sorted(prefecture_counts.items()):
            print(f"  {pref}: {count}ä»¶")

print(f"\nå®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
