#!/usr/bin/env python3
"""
å…¨45éƒ½åºœçœŒãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆæ—¢å­˜170ä»¶ + æ–°è¦4ãƒãƒƒãƒ369ä»¶ï¼‰
"""

import csv
from datetime import datetime
from collections import Counter
import glob

print("=" * 60)
print("å…¨45éƒ½åºœçœŒãƒ‡ãƒ¼ã‚¿çµ±åˆ")
print("=" * 60)

# çµ±åˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
files_to_merge = [
    "nationwide_pediatric_dental_final_20260102_222039.csv",  # æ—¢å­˜170ä»¶ï¼ˆæ±äº¬ãƒ»å¤§é˜ªãƒ»æ„›çŸ¥ + 9çœŒï¼‰
    "batch1_pediatric_dental_20260102_230015.csv",  # 99ä»¶ï¼ˆé’æ£®ã€œåŸ¼ç‰ï¼‰
    "batch2_pediatric_dental_20260102_232001.csv",  # 109ä»¶ï¼ˆåƒè‘‰ã€œé™å²¡ï¼‰
    "batch3_pediatric_dental_20260102_232339.csv",  # 128ä»¶ï¼ˆä¸‰é‡ã€œåºƒå³¶ï¼‰
    "batch4_pediatric_dental_20260102_232500.csv",  # 33ä»¶ï¼ˆå±±å£ãƒ»å¾³å³¶ãƒ»é¦™å·ï¼‰
]

# çµ±åˆ
all_data = []
seen_place_ids = set()
file_stats = {}

for filename in files_to_merge:
    print(f"\nèª­ã¿è¾¼ã¿: {filename}")
    initial_count = len(all_data)

    try:
        with open(filename, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # place_id ã¾ãŸã¯ åŒ»é™¢å+ä½æ‰€ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
                if 'place_id' in row and row['place_id']:
                    unique_key = row['place_id']
                else:
                    unique_key = f"{row['åŒ»é™¢å']}_{row['ä½æ‰€']}"

                if unique_key not in seen_place_ids:
                    seen_place_ids.add(unique_key)
                    all_data.append(row)

        added = len(all_data) - initial_count
        file_stats[filename] = added
        print(f"  è¿½åŠ : {added}ä»¶")
    except FileNotFoundError:
        print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")

# çµ±è¨ˆ
print(f"\n{'=' * 60}")
print("ğŸ“Š çµ±åˆçµæœã‚µãƒãƒªãƒ¼")
print(f"{'=' * 60}")

total = len(all_data)
print(f"\nç·ä»¶æ•°: {total}ä»¶ï¼ˆé‡è¤‡ãªã—ï¼‰")

# ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å†…è¨³
print(f"\nãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å†…è¨³:")
for filename, count in file_stats.items():
    print(f"  {filename}: {count}ä»¶")

# å“è³ªæŒ‡æ¨™
avg_rating = sum(float(r["è©•ä¾¡"]) for r in all_data if r["è©•ä¾¡"]) / len([r for r in all_data if r["è©•ä¾¡"]])
avg_reviews = sum(int(r["å£ã‚³ãƒŸä»¶æ•°"]) for r in all_data) / len(all_data)
has_website = sum(1 for r in all_data if r.get("å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ"))

print(f"\nå“è³ªæŒ‡æ¨™:")
print(f"  å¹³å‡è©•ä¾¡: â­{avg_rating:.2f}")
print(f"  å¹³å‡å£ã‚³ãƒŸä»¶æ•°: {avg_reviews:.0f}ä»¶")
print(f"  å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚ã‚Š: {has_website}/{total} ({has_website/total*100:.1f}%)")

# éƒ½é“åºœçœŒåˆ¥å†…è¨³
prefecture_counts = Counter(r.get("éƒ½é“åºœçœŒ", "ä¸æ˜") for r in all_data)
print(f"\néƒ½é“åºœçœŒåˆ¥å†…è¨³ï¼ˆ45éƒ½åºœçœŒï¼‰:")
for pref, count in sorted(prefecture_counts.items(), key=lambda x: x[1], reverse=True):
    print(f"  {pref}: {count}ä»¶")

# è¨ºç™‚ç§‘ç›®åˆ¥å†…è¨³
specialty_counts = Counter(r.get("è¨ºç™‚ç§‘ç›®", "ä¸æ˜") for r in all_data)
print(f"\nè¨ºç™‚ç§‘ç›®åˆ¥å†…è¨³:")
for spec, count in sorted(specialty_counts.items()):
    print(f"  {spec}: {count}ä»¶")

# æœ€çµ‚CSVå‡ºåŠ›
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = f"nationwide_45prefectures_pediatric_dental_final_{timestamp}.csv"

fieldnames = ["åŒ»é™¢å", "ä½æ‰€", "éƒµä¾¿ç•ªå·", "è©•ä¾¡", "å£ã‚³ãƒŸä»¶æ•°", "Google Maps URL", "å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ", "éƒ½é“åºœçœŒ", "è¨ºç™‚ç§‘ç›®"]

with open(output_file, 'w', encoding='utf-8', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    for row in all_data:
        # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿å‡ºåŠ›
        output_row = {k: row.get(k, '') for k in fieldnames}
        writer.writerow(output_row)

print(f"\nâœ… æœ€çµ‚CSVå‡ºåŠ›å®Œäº†: {output_file}")
print(f"{'=' * 60}")
