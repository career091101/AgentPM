#!/usr/bin/env python3
"""
ãƒãƒƒãƒå˜ä½ã®Webã‚µã‚¤ãƒˆåˆ†æï¼‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
  python3 analyze_batch.py --batch 2
"""

import os
import sys
import json
import csv
import argparse
import glob
from datetime import datetime
from dotenv import load_dotenv
from anthropic import Anthropic

# .envèª­ã¿è¾¼ã¿
load_dotenv()

# ANTHROPIC_API_KEYã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ç›´æ¥å–å¾—
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
if not ANTHROPIC_API_KEY:
    print("âŒ ã‚¨ãƒ©ãƒ¼: ANTHROPIC_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("   ~/.zshrc ã¾ãŸã¯ ~/.bashrc ã«ä»¥ä¸‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„:")
    print("   export ANTHROPIC_API_KEY='your-api-key-here'")
    sys.exit(1)

def analyze_website_with_claude(url, name):
    """
    Claude APIã§Webã‚µã‚¤ãƒˆã‚’åˆ†æ

    åˆ†æé …ç›®:
    - å°å…æ­¯ç§‘ã®æœ‰ç„¡
    - ã‚­ãƒƒã‚ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆçµµæœ¬ã€ãŠã‚‚ã¡ã‚ƒã€ã‚­ãƒƒã‚ºã‚¹ãƒšãƒ¼ã‚¹ç­‰ã®è¨˜è¼‰ï¼‰
    - å¾…åˆå®¤ã®å†™çœŸ
    - SNSé€£æºï¼ˆInstagram, Facebook, LINE, Twitter/Xï¼‰
    - å–¶æ¥­æ™‚é–“
    - ãƒ–ãƒ­ã‚°æœ€çµ‚æ›´æ–°æ—¥
    - é™¢é•·å
    """
    client = Anthropic(api_key=ANTHROPIC_API_KEY)

    prompt = f"""
ã‚ãªãŸã¯æ­¯ç§‘åŒ»é™¢ã®Webã‚µã‚¤ãƒˆåˆ†æã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®Webã‚µã‚¤ãƒˆã‚’åˆ†æã—ã€JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

åŒ»é™¢å: {name}
Webã‚µã‚¤ãƒˆURL: {url}

ã€åˆ†æé …ç›®ã€‘
1. has_pediatric: å°å…æ­¯ç§‘ã®è¨ºç™‚ç§‘ç›®ãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
2. kids_content: ã‚­ãƒƒã‚ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¨˜è¼‰ï¼ˆçµµæœ¬ã€ãŠã‚‚ã¡ã‚ƒã€ã‚­ãƒƒã‚ºã‚¹ãƒšãƒ¼ã‚¹ã€å­ã©ã‚‚å°‚ç”¨å¾…åˆå®¤ç­‰ï¼‰ãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
3. waiting_room_photo: å¾…åˆå®¤ã®å†™çœŸãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
4. sns_instagram: Instagramã¸ã®ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
5. sns_facebook: Facebookã¸ã®ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
6. sns_line: LINEã¸ã®ãƒªãƒ³ã‚¯ã¾ãŸã¯QRã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
7. sns_twitter: Twitter/Xã¸ã®ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ï¼ˆtrue/falseï¼‰
8. operating_hours: å–¶æ¥­æ™‚é–“ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ä¾‹: "æœˆ-é‡‘ 9:00-18:00"ï¼‰
9. blog_updated: ãƒ–ãƒ­ã‚°æœ€çµ‚æ›´æ–°æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€ãƒ–ãƒ­ã‚°ãŒãªã„å ´åˆã¯nullï¼‰
10. director_name: é™¢é•·åï¼ˆãƒ•ãƒ«ãƒãƒ¼ãƒ ã€è¨˜è¼‰ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
{{
  "has_pediatric": true,
  "kids_content": true,
  "waiting_room_photo": false,
  "sns_instagram": true,
  "sns_facebook": false,
  "sns_line": true,
  "sns_twitter": false,
  "operating_hours": "æœˆ-é‡‘ 9:00-18:00ã€åœŸ 9:00-13:00",
  "blog_updated": "2025-12-25",
  "director_name": "ç”°ä¸­å¤ªéƒ"
}}

æ³¨æ„:
- Webã‚µã‚¤ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã›ãšã«URLã‹ã‚‰åˆ¤æ–­ã—ã¦ãã ã•ã„
- æƒ…å ±ãŒä¸æ˜ãªå ´åˆã¯false/null/ç©ºæ–‡å­—ã‚’è¿”ã—ã¦ãã ã•ã„
- å¿…ãšJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„
"""

    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )

        response_text = message.content[0].text

        # JSONæŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼‰
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()

        analysis = json.loads(response_text)
        return analysis

    except Exception as e:
        print(f"âš ï¸  {name} ã®åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "has_pediatric": False,
            "kids_content": False,
            "waiting_room_photo": False,
            "sns_instagram": False,
            "sns_facebook": False,
            "sns_line": False,
            "sns_twitter": False,
            "operating_hours": "",
            "blog_updated": None,
            "director_name": ""
        }

def calculate_score(raw_data, website_analysis):
    """
    130ç‚¹æº€ç‚¹ã§ã‚¹ã‚³ã‚¢è¨ˆç®—

    é…ç‚¹:
    - åŸºç¤è©•ä¾¡: 10ç‚¹ï¼ˆGoogleè©•ä¾¡â˜…4.0ä»¥ä¸Šï¼‰
    - æ¥é™¢æ‚£è€…æ•°: 15ç‚¹ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°100ä»¶ä»¥ä¸Šã§æº€ç‚¹ï¼‰
    - å­ã©ã‚‚å¯¾å¿œåŠ›: 30ç‚¹ï¼ˆå°å…æ­¯ç§‘15 + ã‚­ãƒƒã‚ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„15ï¼‰
    - Webç©æ¥µæ€§: 25ç‚¹ï¼ˆSNS å„5ç‚¹ Ã— 4ç¨®é¡ + Webã‚µã‚¤ãƒˆã‚ã‚Š5ç‚¹ï¼‰
    - åŒ»é™¢è¦æ¨¡: 20ç‚¹ï¼ˆå†™çœŸæšæ•°10 + å–¶æ¥­æ™‚é–“10ï¼‰
    - ãƒ–ãƒ­ã‚°æ´»å‹•: 30ç‚¹ï¼ˆ30æ—¥ä»¥å†…æ›´æ–°ã§æº€ç‚¹ï¼‰
    """
    score = 0
    breakdown = {
        "base_evaluation": 0,
        "patient_volume": 0,
        "children_friendliness": 0,
        "web_activity": 0,
        "clinic_scale": 0,
        "blog_activity": 0
    }

    # 1. åŸºç¤è©•ä¾¡ï¼ˆ10ç‚¹ï¼‰
    rating = raw_data.get('rating', 0)
    if rating >= 4.0:
        breakdown["base_evaluation"] = 10
        score += 10

    # 2. æ¥é™¢æ‚£è€…æ•°ï¼ˆ15ç‚¹ï¼‰
    reviews = raw_data.get('user_ratings_total', 0)
    if reviews >= 100:
        breakdown["patient_volume"] = 15
        score += 15
    elif reviews >= 50:
        breakdown["patient_volume"] = 10
        score += 10
    elif reviews >= 20:
        breakdown["patient_volume"] = 5
        score += 5

    # 3. å­ã©ã‚‚å¯¾å¿œåŠ›ï¼ˆ30ç‚¹ï¼‰
    if website_analysis.get('has_pediatric'):
        breakdown["children_friendliness"] += 15
        score += 15
    if website_analysis.get('kids_content'):
        breakdown["children_friendliness"] += 15
        score += 15

    # 4. Webç©æ¥µæ€§ï¼ˆ25ç‚¹ï¼‰
    if website_analysis.get('sns_instagram'):
        breakdown["web_activity"] += 5
        score += 5
    if website_analysis.get('sns_facebook'):
        breakdown["web_activity"] += 5
        score += 5
    if website_analysis.get('sns_line'):
        breakdown["web_activity"] += 5
        score += 5
    if website_analysis.get('sns_twitter'):
        breakdown["web_activity"] += 5
        score += 5
    if raw_data.get('website'):
        breakdown["web_activity"] += 5
        score += 5

    # 5. åŒ»é™¢è¦æ¨¡ï¼ˆ20ç‚¹ï¼‰
    photos = raw_data.get('photos', [])
    photo_count = len(photos) if isinstance(photos, list) else 0
    if photo_count >= 10:
        breakdown["clinic_scale"] += 10
        score += 10
    elif photo_count >= 5:
        breakdown["clinic_scale"] += 5
        score += 5

    operating_hours = website_analysis.get('operating_hours', '')
    if operating_hours:
        if 'åœŸ' in operating_hours or 'æ—¥' in operating_hours:
            breakdown["clinic_scale"] += 5
            score += 5
        if '18:00' in operating_hours or '19:00' in operating_hours:
            breakdown["clinic_scale"] += 5
            score += 5

    # 6. ãƒ–ãƒ­ã‚°æ´»å‹•ï¼ˆ30ç‚¹ï¼‰
    blog_updated = website_analysis.get('blog_updated')
    if blog_updated:
        try:
            last_update = datetime.strptime(blog_updated, '%Y-%m-%d')
            days_ago = (datetime.now() - last_update).days

            if days_ago <= 30:
                breakdown["blog_activity"] = 30
                score += 30
            elif days_ago <= 90:
                breakdown["blog_activity"] = 20
                score += 20
            elif days_ago <= 180:
                breakdown["blog_activity"] = 10
                score += 10
        except:
            pass

    return score, breakdown

def main():
    parser = argparse.ArgumentParser(description='ãƒãƒƒãƒå˜ä½ã®Webã‚µã‚¤ãƒˆåˆ†æï¼‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°')
    parser.add_argument('--batch', type=int, required=True, help='ãƒãƒƒãƒç•ªå·ï¼ˆ1-360ï¼‰')
    parser.add_argument('--limit', type=int, default=None, help='å‡¦ç†ä»¶æ•°åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰')
    args = parser.parse_args()

    batch_num = args.batch

    print("=" * 60)
    print(f"ãƒãƒƒãƒ {batch_num}/360 ã®åˆ†æé–‹å§‹")
    print("=" * 60)

    # RAWãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    pattern = f"batch_{batch_num:03d}_raw_data_*.json"
    files = glob.glob(pattern)

    if not files:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {pattern} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    raw_file = files[0]
    print(f"\nğŸ“‚ RAWãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {raw_file}")

    with open(raw_file, 'r', encoding='utf-8') as f:
        raw_json = json.load(f)

    # JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¿œã˜ã¦ clinics ãƒªã‚¹ãƒˆã‚’å–å¾—
    if isinstance(raw_json, dict) and 'clinics' in raw_json:
        raw_data_list = raw_json['clinics']
    elif isinstance(raw_json, list):
        raw_data_list = raw_json
    else:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ä¸æ˜ãªJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
        sys.exit(1)

    # limitæŒ‡å®šãŒã‚ã‚‹å ´åˆã¯å…ˆé ­Nä»¶ã®ã¿å‡¦ç†
    if args.limit:
        raw_data_list = raw_data_list[:args.limit]
        print(f"âœ… {len(raw_data_list)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ--limit {args.limit} æŒ‡å®šï¼‰")
    else:
        print(f"âœ… {len(raw_data_list)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # Webã‚µã‚¤ãƒˆåˆ†æ
    print(f"\nğŸ” Webã‚µã‚¤ãƒˆåˆ†æé–‹å§‹...")

    results = []
    total = len(raw_data_list)

    for i, raw_data in enumerate(raw_data_list, 1):
        name = raw_data.get('name', 'Unknown')
        url = raw_data.get('website', '')

        print(f"   [{i}/{total}] {name}")

        if not url:
            print(f"      âš ï¸  Webã‚µã‚¤ãƒˆURLãªã— â†’ ã‚¹ã‚­ãƒƒãƒ—")
            website_analysis = {
                "has_pediatric": False,
                "kids_content": False,
                "waiting_room_photo": False,
                "sns_instagram": False,
                "sns_facebook": False,
                "sns_line": False,
                "sns_twitter": False,
                "operating_hours": "",
                "blog_updated": None,
                "director_name": ""
            }
        else:
            website_analysis = analyze_website_with_claude(url, name)

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        score, breakdown = calculate_score(raw_data, website_analysis)

        # CSVè¡Œä½œæˆ
        sns_list = []
        if website_analysis.get('sns_instagram'):
            sns_list.append('Instagram')
        if website_analysis.get('sns_facebook'):
            sns_list.append('Facebook')
        if website_analysis.get('sns_line'):
            sns_list.append('LINE')
        if website_analysis.get('sns_twitter'):
            sns_list.append('X')

        row = {
            'ã‚¹ã‚³ã‚¢': score,
            'åŒ»é™¢å': name,
            'åŒ»é™¢é•·å': website_analysis.get('director_name', ''),
            'éƒµä¾¿ç•ªå·': '',  # Google Maps APIã«ã¯ãªã„
            'ä½æ‰€': raw_data.get('formatted_address', ''),
            'åŸºç¤è©•ä¾¡': breakdown['base_evaluation'],
            'æ¥é™¢æ‚£è€…æ•°': breakdown['patient_volume'],
            'å­ã©ã‚‚å¯¾å¿œåŠ›': breakdown['children_friendliness'],
            'Webç©æ¥µæ€§': breakdown['web_activity'],
            'åŒ»é™¢è¦æ¨¡': breakdown['clinic_scale'],
            'ãƒ–ãƒ­ã‚°æ´»å‹•': breakdown['blog_activity'],
            'å–¶æ¥­æ™‚é–“': website_analysis.get('operating_hours', ''),
            'ãƒ–ãƒ­ã‚°æ›´æ–°æ—¥': website_analysis.get('blog_updated', ''),
            'é›»è©±ç•ªå·': raw_data.get('formatted_phone_number', ''),
            'Webã‚µã‚¤ãƒˆURL': url,
            'è©•ä¾¡': raw_data.get('rating', 0),
            'ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°': raw_data.get('user_ratings_total', 0),
            'è¨ºç™‚ç§‘ç›®ã‚¿ã‚°': ','.join(raw_data.get('types', [])),
            'å†™çœŸæšæ•°': len(raw_data.get('photos', [])),
            'SNSé€£æº': ','.join(sns_list) if sns_list else '',
            'å­ã©ã‚‚å¯¾å¿œåŠ›ã‚¹ã‚³ã‚¢': breakdown['children_friendliness'],
            'Google Maps URL': f"https://maps.google.com/?cid={raw_data.get('place_id', '')}"
        }

        results.append(row)

    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    results_sorted = sorted(results, key=lambda x: x['ã‚¹ã‚³ã‚¢'], reverse=True)

    # CSVå‡ºåŠ›
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_output = f"batch_{batch_num:03d}_leads_{timestamp}.csv"

    fieldnames = [
        'ã‚¹ã‚³ã‚¢', 'åŒ»é™¢å', 'åŒ»é™¢é•·å', 'éƒµä¾¿ç•ªå·', 'ä½æ‰€',
        'åŸºç¤è©•ä¾¡', 'æ¥é™¢æ‚£è€…æ•°', 'å­ã©ã‚‚å¯¾å¿œåŠ›', 'Webç©æ¥µæ€§', 'åŒ»é™¢è¦æ¨¡', 'ãƒ–ãƒ­ã‚°æ´»å‹•',
        'å–¶æ¥­æ™‚é–“', 'ãƒ–ãƒ­ã‚°æ›´æ–°æ—¥', 'é›»è©±ç•ªå·', 'Webã‚µã‚¤ãƒˆURL',
        'è©•ä¾¡', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', 'è¨ºç™‚ç§‘ç›®ã‚¿ã‚°', 'å†™çœŸæšæ•°', 'SNSé€£æº',
        'å­ã©ã‚‚å¯¾å¿œåŠ›ã‚¹ã‚³ã‚¢', 'Google Maps URL'
    ]

    with open(csv_output, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results_sorted)

    print(f"\nâœ… CSVå‡ºåŠ›å®Œäº†: {csv_output}")

    # çµ±è¨ˆæƒ…å ±
    print(f"\n--- çµ±è¨ˆæƒ…å ± ---")
    print(f"ç·ä»¶æ•°: {len(results)}ä»¶")

    if results:
        avg_score = sum(r['ã‚¹ã‚³ã‚¢'] for r in results) / len(results)
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.1f}ç‚¹")

        high_score_count = sum(1 for r in results if r['ã‚¹ã‚³ã‚¢'] >= 80)
        print(f"é«˜ã‚¹ã‚³ã‚¢ï¼ˆ80ç‚¹ä»¥ä¸Šï¼‰: {high_score_count}ä»¶")

        print(f"\nTop 3åŒ»é™¢:")
        for i, row in enumerate(results_sorted[:3], 1):
            print(f"  {i}. {row['åŒ»é™¢å']}: {row['ã‚¹ã‚³ã‚¢']}ç‚¹")

    print(f"\nâœ… ãƒãƒƒãƒ {batch_num} å‡¦ç†å®Œäº†")

if __name__ == '__main__':
    main()
