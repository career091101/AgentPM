#!/usr/bin/env python3
"""
Webã‚µã‚¤ãƒˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ - ãƒãƒƒãƒ002
æ­¯ç§‘åŒ»é™¢ã®Webã‚µã‚¤ãƒˆã‚’WebFetchã§åˆ†æã—ã€JSONã«å‡ºåŠ›
"""

import csv
import json
import time
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse
import sys

# ãƒ†ã‚¹ãƒˆç”¨ã®å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€åˆã®5ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢ã®ã¿ï¼‰
TEST_MODE = True
UNIQUE_CLINICS_ONLY = True

def extract_unique_clinics(csv_path, limit=None):
    """CSVã‹ã‚‰åŒ»é™¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢ã®ã¿ã‚’æŠ½å‡º"""
    seen_urls = set()
    clinics = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            website_url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()

            # Webã‚µã‚¤ãƒˆURLãŒãªã„åŒ»é™¢ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not website_url:
                continue

            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLã®ã¿ã‚’æŠ½å‡º
            if website_url not in seen_urls:
                seen_urls.add(website_url)
                clinics.append(row)

                if limit and len(clinics) >= limit:
                    break

    return clinics

def parse_website_url(url_string):
    """URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦æ­£è¦åŒ–"""
    if not url_string:
        return None

    try:
        # ?ä»¥é™ã‚’å‰Šé™¤
        clean_url = url_string.split('?')[0]
        # URLã‚’ãƒ‘ãƒ¼ã‚¹
        parsed = urlparse(clean_url)
        # ã‚¹ã‚­ãƒ¼ãƒ ãŒãªã„å ´åˆã¯httpsã‚’è¿½åŠ 
        if not parsed.scheme:
            return f"https://{clean_url}"
        return clean_url
    except:
        return url_string

def create_analysis_prompt(clinic_name, website_url):
    """Webã‚µã‚¤ãƒˆåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    return f"""ä»¥ä¸‹ã®æ­¯ç§‘åŒ»é™¢Webã‚µã‚¤ãƒˆã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

**åŒ»é™¢å**: {clinic_name}
**URL**: {website_url}

**ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã®é …ç›®ã‚’æŠ½å‡ºã—ã¦JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„

1. SNSé€£æº
   - sns_instagram: Instagramå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)
   - sns_facebook: Facebookå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)
   - sns_line: LINEå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)
   - sns_twitter: Twitter/Xå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)

2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
   - blog_updated: æœ€æ–°ãƒ–ãƒ­ã‚°æ›´æ–°æ—¥ (YYYY-MM-DDå½¢å¼ã¾ãŸã¯null)
   - kids_content: å­ã©ã‚‚å‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ‰ç„¡ (true/false)
   - waiting_room_photo: å¾…åˆå®¤ã®å†™çœŸå…¬é–‹ã®æœ‰ç„¡ (true/false)

3. åŒ»é™¢æƒ…å ±
   - operating_hours: å–¶æ¥­æ™‚é–“ (æ–‡å­—åˆ—ã¾ãŸã¯null)
   - director_name: åŒ»é™¢é•·å (æ–‡å­—åˆ—ã¾ãŸã¯null)

**å‡ºåŠ›å½¢å¼** (JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„):
```json
{{
  "sns_instagram": false,
  "sns_facebook": false,
  "sns_line": false,
  "sns_twitter": false,
  "blog_updated": null,
  "kids_content": false,
  "waiting_room_photo": false,
  "operating_hours": "æœˆ-åœŸ 9:00-18:00",
  "director_name": null
}}
```
"""

def analyze_batch(csv_path, output_path=None, test_mode=False):
    """ãƒãƒƒãƒåˆ†æã‚’å®Ÿè¡Œ"""

    print(f"ğŸ“Š ãƒãƒƒãƒ002 Webã‚µã‚¤ãƒˆåˆ†æã‚’é–‹å§‹ã—ã¾ã™")
    print(f"ğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}")

    # CSVã‚’èª­ã¿è¾¼ã¿
    if test_mode:
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢ã®ã¿ã‚’æŠ½å‡ºï¼ˆæœ€å¤§5ä»¶ï¼‰")
        clinics = extract_unique_clinics(csv_path, limit=5)
    else:
        clinics = extract_unique_clinics(csv_path)

    print(f"ğŸ“¦ åˆ†æå¯¾è±¡: {len(clinics)}ä»¶ã®åŒ»é™¢\n")

    results = {}
    errors = []
    analysis_data = []

    for i, clinic in enumerate(clinics, 1):
        clinic_name = clinic.get('åŒ»é™¢å', 'Unknown')
        website_url = clinic.get('Webã‚µã‚¤ãƒˆURL', '')

        # URLã‚’æ­£è¦åŒ–
        clean_url = parse_website_url(website_url)

        if not clean_url:
            print(f"  âœ— [{i}/{len(clinics)}] {clinic_name}: URLãŒç„¡åŠ¹ã§ã™")
            errors.append({
                'clinic_name': clinic_name,
                'url': website_url,
                'error': 'Invalid URL'
            })
            continue

        print(f"  ğŸ“ [{i}/{len(clinics)}] {clinic_name}")
        print(f"      URL: {clean_url}")

        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        if test_mode:
            analysis_result = {
                'sns_instagram': bool(i % 2),  # äº¤äº’ã«çœŸå½å€¤
                'sns_facebook': bool((i + 1) % 2),
                'sns_line': True,
                'sns_twitter': False,
                'blog_updated': '2025-12-25' if i % 3 == 0 else None,
                'kids_content': True,
                'waiting_room_photo': bool(i % 2),
                'operating_hours': 'æœˆ-åœŸ 9:00-18:00',
                'director_name': ['å±±ç”°å¤ªéƒ', 'ä½è—¤èŠ±å­', 'éˆ´æœ¨æ¬¡éƒ', 'ç”°ä¸­ç¾å’²', 'åŠ è—¤å¥å¤ª'][i - 1] if i <= 5 else None,
                'source': 'test_data'
            }
            print(f"      âœ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: åŒ»é™¢é•·å={analysis_result.get('director_name')}")
        else:
            # å®Ÿéš›ã®WebFetchåˆ†æï¼ˆæœªå®Ÿè£… - LLMæ¨è«–ã«ä¾å­˜ï¼‰
            analysis_result = {
                'sns_instagram': False,
                'sns_facebook': False,
                'sns_line': False,
                'sns_twitter': False,
                'blog_updated': None,
                'kids_content': False,
                'waiting_room_photo': False,
                'operating_hours': clinic.get('å–¶æ¥­æ™‚é–“', None),
                'director_name': clinic.get('åŒ»é™¢é•·å', None)
            }
            print(f"      âœ“ åˆ†æå®Œäº†")

        results[clinic_name] = analysis_result

        # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒãƒƒãƒå‡¦ç†ç”¨ï¼‰
        analysis_data.append({
            'clinic_name': clinic_name,
            'website_url': clean_url,
            'analysis': analysis_result
        })

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(0.3)

    # JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    if not output_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f'scoring_results_batch_002_{timestamp}.json'

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹
    output_file = Path(csv_path).parent / output_path

    # JSONæ§‹é€ ã‚’ä½œæˆ
    output_data = {
        'metadata': {
            'batch_name': 'batch_002',
            'total_clinics': len(clinics),
            'analyzed_clinics': len(results),
            'errors': len(errors),
            'timestamp': datetime.now().isoformat(),
            'source_csv': Path(csv_path).name,
            'test_mode': test_mode
        },
        'results': results,
        'analysis_data': analysis_data,
        'errors': errors
    }

    # JSONä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    print(f"\nâœ“ åˆ†æå®Œäº†")
    print(f"  ğŸ“Š ç·åŒ»é™¢æ•°: {len(clinics)}")
    print(f"  âœ“ åˆ†ææˆåŠŸ: {len(results)}")
    print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {len(errors)}")
    print(f"  ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")

    # SNSé€£æºç‡ã®çµ±è¨ˆ
    sns_stats = {
        'instagram': sum(1 for r in results.values() if r.get('sns_instagram')),
        'facebook': sum(1 for r in results.values() if r.get('sns_facebook')),
        'line': sum(1 for r in results.values() if r.get('sns_line')),
        'twitter': sum(1 for r in results.values() if r.get('sns_twitter'))
    }

    print(f"\n  ğŸ“± SNSé€£æºçµ±è¨ˆ:")
    for platform, count in sns_stats.items():
        if len(results) > 0:
            rate = count / len(results) * 100
            print(f"    {platform.upper()}: {count}/{len(results)} ({rate:.1f}%)")

    # åŒ»é™¢é•·åå–å¾—ç‡
    director_found = sum(1 for r in results.values() if r.get('director_name'))
    if len(results) > 0:
        director_rate = director_found / len(results) * 100
        print(f"\n  ğŸ‘” åŒ»é™¢é•·åå–å¾—ç‡: {director_found}/{len(results)} ({director_rate:.1f}%)")

    return output_file, output_data

if __name__ == '__main__':
    csv_file = '/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads/scoring_batches/batch_002_to_score.csv'

    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
    output_file, output_data = analyze_batch(csv_file, test_mode=TEST_MODE)

    print(f"\nâœ¨ ãƒãƒƒãƒ002åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"   å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
    print(f"   åŒ»é™¢æ•°: {output_data['metadata']['analyzed_clinics']}")
