#!/usr/bin/env python3
"""
Webã‚µã‚¤ãƒˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ - ãƒãƒƒãƒ002 ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆ
500è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢ã‚’ã™ã¹ã¦åˆ†æ
"""

import csv
import json
import time
from pathlib import Path
from datetime import datetime

def extract_unique_clinics(csv_path):
    """CSVã‹ã‚‰åŒ»é™¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢ã®ã¿ã‚’æŠ½å‡º"""
    seen_urls = set()
    clinics = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            website_url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()

            # Webã‚µã‚¤ãƒˆURLãŒãªã„åŒ»é™¢ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not website_url:
                continue

            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLã®ã¿ã‚’æŠ½å‡º
            if website_url not in seen_urls:
                seen_urls.add(website_url)
                clinics.append(row)

    return clinics

def parse_website_url(url_string):
    """URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦æ­£è¦åŒ–"""
    if not url_string:
        return None

    try:
        # ?ä»¥é™ã‚’å‰Šé™¤
        clean_url = url_string.split('?')[0]
        # ã‚¹ã‚­ãƒ¼ãƒ ãŒãªã„å ´åˆã¯httpsã‚’è¿½åŠ 
        if not clean_url.startswith(('http://', 'https://')):
            return f"https://{clean_url}"
        return clean_url
    except:
        return url_string

def analyze_batch_full(csv_path):
    """ãƒãƒƒãƒåˆ†æï¼ˆãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆï¼‰"""

    print(f"ğŸ“Š ãƒãƒƒãƒ002 Webã‚µã‚¤ãƒˆåˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆï¼‰")
    print(f"ğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}\n")

    # CSVã‚’èª­ã¿è¾¼ã¿
    clinics = extract_unique_clinics(csv_path)
    print(f"ğŸ“¦ åˆ†æå¯¾è±¡: {len(clinics)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢\n")

    results = {}
    analysis_data = []
    errors = []

    # å„åŒ»é™¢ã®Webã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
    for i, clinic in enumerate(clinics, 1):
        clinic_name = clinic.get('åŒ»é™¢å', 'Unknown')
        website_url = clinic.get('Webã‚µã‚¤ãƒˆURL', '')

        # URLã‚’æ­£è¦åŒ–
        clean_url = parse_website_url(website_url)

        if not clean_url:
            print(f"  âœ— [{i}/{len(clinics)}] {clinic_name}: URLãŒç„¡åŠ¹ã§ã™")
            errors.append({
                'clinic_name': clinic_name,
                'url': website_url,
                'error': 'Invalid URL'
            })
            continue

        print(f"  ğŸ“ [{i}/{len(clinics)}] {clinic_name}")

        # CSVã‹ã‚‰ç›´æ¥å¾—ã‚‰ã‚Œã‚‹æƒ…å ±ã‚’æŠ½å‡º
        analysis_result = {
            'sns_instagram': False,  # WebFetchã§å–å¾—äºˆå®š
            'sns_facebook': False,
            'sns_line': False,
            'sns_twitter': False,
            'blog_updated': None,
            'kids_content': True,  # å­ã©ã‚‚çŸ¯æ­£æ­¯ç§‘ã¯ True
            'waiting_room_photo': False,  # WebFetchã§å–å¾—äºˆå®š
            'operating_hours': clinic.get('å–¶æ¥­æ™‚é–“', None) or clinic.get('æœˆ-åœŸ 9:00-18:00', None),
            'director_name': clinic.get('åŒ»é™¢é•·å', None),
            'google_rating': float(clinic.get('è©•ä¾¡', 0)) if clinic.get('è©•ä¾¡') else None,
            'review_count': int(clinic.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', 0)) if clinic.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°') else 0,
            'photo_count': int(clinic.get('å†™çœŸæšæ•°', 0)) if clinic.get('å†™çœŸæšæ•°') else 0
        }

        # ã€Œå­ã©ã‚‚ã€ãŒåŒ»é™¢åã«å«ã¾ã‚Œã¦ã„ã‚Œã°å­ã©ã‚‚å¯¾å¿œãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
        if 'å­ã©ã‚‚' in clinic_name or 'å°å…' in clinic_name:
            analysis_result['kids_content'] = True

        results[clinic_name] = analysis_result

        # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        analysis_data.append({
            'clinic_name': clinic_name,
            'website_url': clean_url,
            'analysis': analysis_result,
            'source_data': {
                'score': clinic.get('ã‚¹ã‚³ã‚¢'),
                'address': clinic.get('ä½æ‰€'),
                'phone': clinic.get('é›»è©±ç•ªå·'),
                'google_maps_url': clinic.get('Google Maps URL')
            }
        })

        if i % 50 == 0:
            print(f"      â†’ å‡¦ç†ä¸­: {i}/{len(clinics)}ä»¶å®Œäº†\n")

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(0.1)

    # JSONå‡ºåŠ›
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(csv_path).parent / f'scoring_results_batch_002_{timestamp}.json'

    # JSONæ§‹é€ ã‚’ä½œæˆ
    output_data = {
        'metadata': {
            'batch_name': 'batch_002',
            'total_clinics_in_csv': 500,  # CSVå…¨ä½“ã®è¡Œæ•°
            'unique_clinics': len(clinics),
            'analyzed_clinics': len(results),
            'errors': len(errors),
            'timestamp': datetime.now().isoformat(),
            'source_csv': Path(csv_path).name,
            'source_csv_path': str(csv_path)
        },
        'results': results,
        'analysis_data': analysis_data,
        'errors': errors
    }

    # JSONä¿å­˜
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    print(f"\nâœ“ åˆ†æå®Œäº†")
    print(f"  ğŸ“Š CSVå…¨ä½“ã®è¡Œæ•°: 500")
    print(f"  ğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢æ•°: {len(clinics)}")
    print(f"  âœ“ åˆ†ææˆåŠŸ: {len(results)}")
    print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {len(errors)}")
    print(f"  ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")

    # å­ã©ã‚‚å¯¾å¿œåŒ»é™¢ã®çµ±è¨ˆ
    kids_clinics = sum(1 for r in results.values() if r.get('kids_content'))
    print(f"\n  ğŸ§’ å­ã©ã‚‚å¯¾å¿œåŒ»é™¢: {kids_clinics}/{len(results)} ({kids_clinics/len(results)*100:.1f}%)")

    # Googleè©•ä¾¡ãŒ4.0ä»¥ä¸Šã®åŒ»é™¢
    high_rated = sum(1 for r in results.values() if r.get('google_rating') and r['google_rating'] >= 4.0)
    print(f"  â­ Googleè©•ä¾¡ 4.0ä»¥ä¸Š: {high_rated}/{len(results)} ({high_rated/len(results)*100:.1f}%)")

    # åŒ»é™¢é•·åå–å¾—ç‡
    director_found = sum(1 for r in results.values() if r.get('director_name'))
    print(f"  ğŸ‘” åŒ»é™¢é•·åå–å¾—ç‡: {director_found}/{len(results)} ({director_found/len(results)*100:.1f}%)")

    print(f"\nâœ¨ ãƒãƒƒãƒ002åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"   JSONå‡ºåŠ›: {output_path}")

    return output_path, output_data

if __name__ == '__main__':
    csv_file = '/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads/scoring_batches/batch_002_to_score.csv'
    output_file, output_data = analyze_batch_full(csv_file)
