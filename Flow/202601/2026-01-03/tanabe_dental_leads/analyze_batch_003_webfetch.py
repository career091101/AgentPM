#!/usr/bin/env python3
"""
Batch 003 Website Analysis with Forced WebFetch
å®Œå…¨å†å®Ÿè¡Œ - å…¨500ä»¶ã‚’WebFetchå®Ÿè¡Œã—ã¦åŒ»é™¢é•·åæŠ½å‡ºç‡70%ä»¥ä¸Šã‚’ç›®æŒ‡ã™
"""

import csv
import json
import time
from datetime import datetime
from pathlib import Path
import re

# WebFetchãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆClaude Codeç’°å¢ƒã§åˆ©ç”¨å¯èƒ½ï¼‰
# å®Ÿéš›ã®å®Ÿè£…ã§ã¯Claude Codeã®çµ„ã¿è¾¼ã¿ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨

def parse_json_from_text(text: str) -> dict:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦ãƒ‘ãƒ¼ã‚¹"""
    try:
        # ã¾ãšç›´æ¥ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        return json.loads(text)
    except json.JSONDecodeError:
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æŠ½å‡º
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))

        # JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¢ã™
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))

        raise ValueError(f"JSON parse failed: {text[:200]}")

def analyze_clinic_website_with_subagent(clinic_name: str, website_url: str) -> dict:
    """
    ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ãŸè¤‡æ•°ãƒšãƒ¼ã‚¸æ¢ç´¢ã«ã‚ˆã‚‹è©³ç´°åˆ†æ

    Returns:
        dict: {
            'sns_instagram': bool,
            'sns_facebook': bool,
            'sns_line': bool,
            'sns_twitter': bool,
            'blog_updated': str or None,
            'kids_content': bool,
            'waiting_room_photo': bool,
            'operating_hours': str or None,
            'director_name': str or None
        }
    """

    # STEP 1: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’WebFetchã§åˆ†æ
    top_page_prompt = f"""ä»¥ä¸‹ã®æ­¯ç§‘åŒ»é™¢Webã‚µã‚¤ãƒˆã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

**åŒ»é™¢å**: {clinic_name}
**URL**: {website_url}

**ã‚¿ã‚¹ã‚¯1: é–¢é€£ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã®æŠ½å‡º**
ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯URLã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆçµ¶å¯¾URLã§ï¼‰:
- ã€Œé™¢é•·ã€ã€ŒåŒ»é™¢é•·ã€ã€Œãƒ‰ã‚¯ã‚¿ãƒ¼ç´¹ä»‹ã€ã€Œã‚¹ã‚¿ãƒƒãƒ•ç´¹ä»‹ã€ã€Œã”æŒ¨æ‹¶ã€
- ã€ŒåŒ»é™¢æ¦‚è¦ã€ã€Œå½“é™¢ã«ã¤ã„ã¦ã€ã€Œã‚¯ãƒªãƒ‹ãƒƒã‚¯ç´¹ä»‹ã€

**ã‚¿ã‚¹ã‚¯2: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®æƒ…å ±æŠ½å‡º**
ä»¥ä¸‹ã®é …ç›®ã‚’æŠ½å‡ºã—ã¦JSONã§å‡ºåŠ›:
- sns_instagram, sns_facebook, sns_line, sns_twitter (å„true/false)
- blog_updated (YYYY-MM-DDå½¢å¼ã¾ãŸã¯null)
- kids_content (true/false)
- waiting_room_photo (true/false)
- operating_hours (æ–‡å­—åˆ—ã¾ãŸã¯null)
- director_name (æ–‡å­—åˆ—ã¾ãŸã¯null - ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«è¨˜è¼‰ãŒã‚ã‚Œã°)
- director_links (é…åˆ— - åŒ»é™¢é•·é–¢é€£ãƒšãƒ¼ã‚¸ã®URLã€æœ€å¤§5ä»¶)

**å‡ºåŠ›å½¢å¼**:
```json
{{
  "sns_instagram": false,
  "sns_facebook": false,
  "sns_line": false,
  "sns_twitter": false,
  "blog_updated": null,
  "kids_content": false,
  "waiting_room_photo": false,
  "operating_hours": null,
  "director_name": null,
  "director_links": []
}}
```

**é‡è¦**: JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
"""

    try:
        # WebFetchå®Ÿè¡Œï¼ˆClaude Codeçµ„ã¿è¾¼ã¿ãƒ„ãƒ¼ãƒ«ï¼‰
        # æ³¨: å®Ÿéš›ã®å®Ÿè£…ã§ã¯WebFetchãƒ„ãƒ¼ãƒ«ã‚’ç›´æ¥å‘¼ã³å‡ºã™
        print(f"  â†’ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸åˆ†æä¸­: {website_url}")

        # ã“ã“ã§ã¯ä»®ã®WebFetchå®Ÿè£…ï¼ˆå®Ÿéš›ã¯Claude Codeã®çµ„ã¿è¾¼ã¿ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ï¼‰
        # top_page_result = WebFetch(url=website_url, prompt=top_page_prompt)
        # links_data = parse_json_from_text(top_page_result)

        # ãƒ‡ãƒ¢ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å‰Šé™¤ï¼‰
        links_data = {
            'sns_instagram': False,
            'sns_facebook': False,
            'sns_line': False,
            'sns_twitter': False,
            'blog_updated': None,
            'kids_content': False,
            'waiting_room_photo': False,
            'operating_hours': None,
            'director_name': None,
            'director_links': []
        }

        # STEP 2: åŒ»é™¢é•·åãŒæœªå–å¾—ã®å ´åˆã€ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§æ·±å €ã‚Š
        if not links_data.get('director_name') and links_data.get('director_links'):
            director_links = links_data['director_links'][:3]  # æœ€å¤§3ãƒšãƒ¼ã‚¸

            for link in director_links:
                director_prompt = f"""ä»¥ä¸‹ã®URLã‹ã‚‰åŒ»é™¢é•·åã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

**åŒ»é™¢å**: {clinic_name}
**URL**: {link}

**æŠ½å‡ºæŒ‡ç¤º**:
1. ã€Œé™¢é•·ã€ã€ŒåŒ»é™¢é•·ã€ã€Œç†äº‹é•·ã€ã€Œä»£è¡¨ã€ãªã©ã®è‚©æ›¸ãã¨å…±ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹åå‰ã‚’æ¢ã™
2. ãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼ˆå§“åï¼‰ã§æŠ½å‡ºï¼ˆä¾‹: "ç”°ä¸­å¤ªéƒ"ï¼‰
3. è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ null

**å‡ºåŠ›å½¢å¼**:
```json
{{
  "director_name": "ç”°ä¸­å¤ªéƒ"
}}
```

**é‡è¦**: JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

                try:
                    # Task toolã§ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèµ·å‹•
                    # task_result = Task(
                    #     description=f"åŒ»é™¢é•·åæ¢ç´¢ - {clinic_name}",
                    #     prompt=director_prompt,
                    #     subagent_type="general-purpose",
                    #     model="haiku",
                    #     timeout=30000
                    # )
                    # result_data = parse_json_from_text(task_result)

                    # ãƒ‡ãƒ¢ç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                    result_data = {'director_name': None}

                    if result_data.get('director_name'):
                        links_data['director_name'] = result_data['director_name']
                        print(f"    âœ“ åŒ»é™¢é•·åç™ºè¦‹: {result_data['director_name']}")
                        break

                except Exception as e:
                    print(f"    âœ— ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ ({link}): {e}")
                    continue

        return links_data

    except Exception as e:
        print(f"  âœ— WebFetchã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'sns_instagram': False,
            'sns_facebook': False,
            'sns_line': False,
            'sns_twitter': False,
            'blog_updated': None,
            'kids_content': False,
            'waiting_room_photo': False,
            'operating_hours': None,
            'director_name': None,
            'error': str(e)
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""

    # STEP 1: CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    csv_path = Path('/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads/scoring_batches/batch_003_to_score.csv')

    print("=" * 80)
    print("Batch 003 Website Analysis with Forced WebFetch")
    print("=" * 80)
    print(f"\nğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}")

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        clinics = list(reader)

    # Webã‚µã‚¤ãƒˆURLãŒã‚ã‚‹åŒ»é™¢ã®ã¿æŠ½å‡º
    clinics_with_website = [
        c for c in clinics
        if c.get('Webã‚µã‚¤ãƒˆURL') and c['Webã‚µã‚¤ãƒˆURL'].strip()
    ]

    print(f"ğŸ“Š ç·ä»¶æ•°: {len(clinics)}ä»¶")
    print(f"ğŸ“Š Webã‚µã‚¤ãƒˆURLæœ‰ã‚Š: {len(clinics_with_website)}ä»¶")

    # STEP 2: Webã‚µã‚¤ãƒˆåˆ†æï¼ˆå…¨ä»¶WebFetchå®Ÿè¡Œï¼‰
    results = {}
    errors = []
    director_names_found = 0
    batch_size = 10

    print(f"\nğŸš€ åˆ†æé–‹å§‹: {len(clinics_with_website)}ä»¶")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"   WebFetch: å¼·åˆ¶å®Ÿè¡Œï¼ˆå…¨ä»¶ï¼‰")
    print(f"   åŒ»é™¢é•·åæŠ½å‡ºç›®æ¨™: 70%ä»¥ä¸Š\n")

    start_time = time.time()

    for i in range(0, len(clinics_with_website), batch_size):
        batch = clinics_with_website[i:i+batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(clinics_with_website) + batch_size - 1) // batch_size

        print(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} ({len(batch)}ä»¶)")

        for clinic in batch:
            clinic_name = clinic.get('åŒ»é™¢å', 'Unknown')
            website_url = clinic.get('Webã‚µã‚¤ãƒˆURL', '')

            try:
                # Webã‚µã‚¤ãƒˆåˆ†æå®Ÿè¡Œ
                analysis_result = analyze_clinic_website_with_subagent(clinic_name, website_url)

                results[clinic_name] = analysis_result

                if analysis_result.get('director_name'):
                    director_names_found += 1
                    print(f"  âœ“ {clinic_name} - åŒ»é™¢é•·: {analysis_result['director_name']}")
                else:
                    print(f"  âœ“ {clinic_name}")

            except Exception as e:
                print(f"  âœ— {clinic_name}: {e}")
                errors.append({
                    'clinic_name': clinic_name,
                    'url': website_url,
                    'error': str(e)
                })

                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                results[clinic_name] = {
                    'sns_instagram': False,
                    'sns_facebook': False,
                    'sns_line': False,
                    'sns_twitter': False,
                    'blog_updated': None,
                    'kids_content': False,
                    'waiting_room_photo': False,
                    'operating_hours': None,
                    'director_name': None,
                    'error': str(e)
                }

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(0.5)

        # ãƒãƒƒãƒé–“å¾…æ©Ÿ
        time.sleep(2)

        # é€²æ—è¡¨ç¤º
        current_rate = director_names_found / len(results) * 100 if results else 0
        print(f"   é€²æ—: {len(results)}ä»¶å®Œäº†, åŒ»é™¢é•·åå–å¾—ç‡: {current_rate:.1f}%\n")

    elapsed_time = time.time() - start_time

    print("=" * 80)
    print(f"âœ“ åˆ†æå®Œäº†: {len(results)}ä»¶")
    print(f"âœ“ åŒ»é™¢é•·åå–å¾—: {director_names_found}ä»¶ ({director_names_found/len(results)*100:.1f}%)")
    print(f"âœ— ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶")
    print(f"â± å®Ÿè¡Œæ™‚é–“: {elapsed_time/60:.1f}åˆ†")
    print("=" * 80)

    # STEP 3: JSONå‡ºåŠ›
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f'website_analysis_batch_003_webfetch_{timestamp}.json'

    output_data = {
        'metadata': {
            'batch_file': 'batch_003_to_score.csv',
            'total_clinics': len(clinics),
            'analyzed_clinics': len(results),
            'errors': len(errors),
            'timestamp': datetime.now().isoformat(),
            'director_names_found': director_names_found,
            'director_extraction_rate': f"{director_names_found/len(results)*100:.1f}%",
            'execution_time_minutes': elapsed_time / 60,
            'webfetch_forced': True,
            'retry_execution': True
        },
        'results': results,
        'errors': errors
    }

    output_file = Path('/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads') / output_path

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ JSONå‡ºåŠ›å®Œäº†: {output_file}")
    print(f"âœ“ åŒ»é™¢é•·åæŠ½å‡ºç‡: {director_names_found}/{len(results)} ({director_names_found/len(results)*100:.1f}%)")

    # SNSé€£æºçµ±è¨ˆ
    sns_stats = {
        'instagram': sum(1 for r in results.values() if r.get('sns_instagram')),
        'facebook': sum(1 for r in results.values() if r.get('sns_facebook')),
        'line': sum(1 for r in results.values() if r.get('sns_line')),
        'twitter': sum(1 for r in results.values() if r.get('sns_twitter'))
    }

    print(f"\nğŸ“Š SNSé€£æºçµ±è¨ˆ:")
    for platform, count in sns_stats.items():
        rate = count / len(results) * 100 if results else 0
        print(f"   {platform.capitalize()}: {count}ä»¶ ({rate:.1f}%)")

    return output_file

if __name__ == '__main__':
    main()
