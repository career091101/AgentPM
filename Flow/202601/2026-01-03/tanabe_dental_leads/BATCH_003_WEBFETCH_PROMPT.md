# Batch 003 WebFetch Execution Prompt

ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’Claude Codeå¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

## å®Ÿè¡Œå†…å®¹

Batch 003ã®119ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢ã«å¯¾ã—ã¦ã€WebFetchã‚’å¼·åˆ¶å®Ÿè¡Œã—ã€åŒ»é™¢é•·åæŠ½å‡ºç‡70%ä»¥ä¸Šã‚’é”æˆã—ã¾ã™ã€‚

## å®Ÿè¡Œæ‰‹é †

### STEP 1: CSVã‹ã‚‰119ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢ã‚’æŠ½å‡º

```python
import csv
from pathlib import Path
from collections import defaultdict

csv_path = Path('/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads/scoring_batches/batch_003_to_score.csv')

with open(csv_path, 'r', encoding='utf-8-sig') as f:
    reader = csv.DictReader(f)
    all_rows = list(reader)

# ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢æŠ½å‡º
unique_clinics = {}
for row in all_rows:
    clinic_name = row.get('åŒ»é™¢å', '').strip()
    website_url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()

    if not clinic_name or not website_url:
        continue

    unique_key = f"{clinic_name}|{website_url}"

    if unique_key not in unique_clinics:
        unique_clinics[unique_key] = {
            'clinic_name': clinic_name,
            'website_url': website_url,
            'raw_data': row
        }

print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢æ•°: {len(unique_clinics)}")
```

### STEP 2: å„åŒ»é™¢ã«å¯¾ã—ã¦WebFetchå®Ÿè¡Œ

ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€å„åŒ»é™¢ã®Webã‚µã‚¤ãƒˆã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š

```python
# åŒ»é™¢ã”ã¨ã«WebFetchå®Ÿè¡Œ
import json
import time

website_analysis = {}
director_names_found = 0

for idx, (unique_key, clinic_info) in enumerate(unique_clinics.items(), 1):
    clinic_name = clinic_info['clinic_name']
    website_url = clinic_info['website_url']

    print(f"\n[{idx}/{len(unique_clinics)}] {clinic_name}")
    print(f"URL: {website_url}")

    # WebFetchãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    webfetch_prompt = f"""ä»¥ä¸‹ã®æ­¯ç§‘åŒ»é™¢Webã‚µã‚¤ãƒˆã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

**åŒ»é™¢å**: {clinic_name}
**URL**: {website_url}

**ã‚¿ã‚¹ã‚¯1: é–¢é€£ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã®æŠ½å‡º**
ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯URLã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆçµ¶å¯¾URLã§ï¼‰:
- ã€Œé™¢é•·ã€ã€ŒåŒ»é™¢é•·ã€ã€Œãƒ‰ã‚¯ã‚¿ãƒ¼ç´¹ä»‹ã€ã€Œã‚¹ã‚¿ãƒƒãƒ•ç´¹ä»‹ã€ã€Œã”æŒ¨æ‹¶ã€
- ã€ŒåŒ»é™¢æ¦‚è¦ã€ã€Œå½“é™¢ã«ã¤ã„ã¦ã€ã€Œã‚¯ãƒªãƒ‹ãƒƒã‚¯ç´¹ä»‹ã€

**ã‚¿ã‚¹ã‚¯2: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®æƒ…å ±æŠ½å‡º**
ä»¥ä¸‹ã®é …ç›®ã‚’æŠ½å‡ºã—ã¦JSONã§å‡ºåŠ›:
- sns_instagram, sns_facebook, sns_line, sns_twitter (å„true/false)
- blog_updated (YYYY-MM-DDå½¢å¼ã¾ãŸã¯null)
- kids_content (true/false)
- waiting_room_photo (true/false)
- operating_hours (æ–‡å­—åˆ—ã¾ãŸã¯null)
- director_name (æ–‡å­—åˆ—ã¾ãŸã¯null - ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«è¨˜è¼‰ãŒã‚ã‚Œã°)
- director_links (é…åˆ— - åŒ»é™¢é•·é–¢é€£ãƒšãƒ¼ã‚¸ã®URLã€æœ€å¤§5ä»¶)

**å‡ºåŠ›å½¢å¼**:
```json
{{
  "sns_instagram": false,
  "sns_facebook": false,
  "sns_line": false,
  "sns_twitter": false,
  "blog_updated": null,
  "kids_content": false,
  "waiting_room_photo": false,
  "operating_hours": null,
  "director_name": null,
  "director_links": []
}}
```

**é‡è¦**: JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
"""

    try:
        # WebFetchå®Ÿè¡Œ
        result = WebFetch(url=website_url, prompt=webfetch_prompt)

        # JSONãƒ‘ãƒ¼ã‚¹
        result_json = json.loads(result)

        # STEP 2-2: åŒ»é™¢é•·åãŒæœªå–å¾—ã®å ´åˆã€director_linksã‚’æ¢ç´¢
        if not result_json.get('director_name') and result_json.get('director_links'):
            director_links = result_json['director_links'][:3]  # æœ€å¤§3ãƒšãƒ¼ã‚¸

            for link in director_links:
                director_prompt = f"""ä»¥ä¸‹ã®URLã‹ã‚‰åŒ»é™¢é•·åã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

**åŒ»é™¢å**: {clinic_name}
**URL**: {link}

**æŠ½å‡ºæŒ‡ç¤º**:
1. ã€Œé™¢é•·ã€ã€ŒåŒ»é™¢é•·ã€ã€Œç†äº‹é•·ã€ã€Œä»£è¡¨ã€ãªã©ã®è‚©æ›¸ãã¨å…±ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹åå‰ã‚’æ¢ã™
2. ãƒ•ãƒ«ãƒãƒ¼ãƒ ï¼ˆå§“åï¼‰ã§æŠ½å‡ºï¼ˆä¾‹: "ç”°ä¸­å¤ªéƒ"ï¼‰
3. è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ null

**å‡ºåŠ›å½¢å¼**:
```json
{{
  "director_name": "ç”°ä¸­å¤ªéƒ"
}}
```

**é‡è¦**: JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

                try:
                    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’WebFetch
                    sub_result = WebFetch(url=link, prompt=director_prompt)
                    sub_json = json.loads(sub_result)

                    if sub_json.get('director_name'):
                        result_json['director_name'] = sub_json['director_name']
                        print(f"  âœ“ åŒ»é™¢é•·åç™ºè¦‹: {sub_json['director_name']}")
                        break

                except Exception as e:
                    print(f"  âœ— ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ ({link}): {e}")
                    continue

        # çµæœã‚’ä¿å­˜
        website_analysis[clinic_name] = result_json

        if result_json.get('director_name'):
            director_names_found += 1

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(1)

    except Exception as e:
        print(f"  âœ— WebFetchã‚¨ãƒ©ãƒ¼: {e}")
        website_analysis[clinic_name] = {
            'sns_instagram': False,
            'sns_facebook': False,
            'sns_line': False,
            'sns_twitter': False,
            'blog_updated': None,
            'kids_content': False,
            'waiting_room_photo': False,
            'operating_hours': None,
            'director_name': None,
            'error': str(e)
        }

print(f"\nå®Œäº†: {len(website_analysis)}ä»¶")
print(f"åŒ»é™¢é•·åå–å¾—: {director_names_found}ä»¶ ({director_names_found/len(website_analysis)*100:.1f}%)")
```

### STEP 3: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ

```python
from datetime import datetime

scoring_results = []

for row in all_rows:
    clinic_name = row.get('åŒ»é™¢å', '').strip()

    if not clinic_name:
        continue

    # Webã‚µã‚¤ãƒˆåˆ†æçµæœã‚’å–å¾—
    analysis = website_analysis.get(clinic_name, {})

    # RAWãƒ‡ãƒ¼ã‚¿
    try:
        rating = float(row.get('è©•ä¾¡', 0) or 0)
    except ValueError:
        rating = 0.0

    try:
        user_ratings_total = int(row.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', 0) or 0)
    except ValueError:
        user_ratings_total = 0

    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¨ˆç®—

    # 1. åŸºç¤è©•ä¾¡ (20ç‚¹)
    score_åŸºç¤è©•ä¾¡ = min(rating * 4, 20)

    # 2. æ¥é™¢æ‚£è€…æ•° (20ç‚¹)
    if user_ratings_total >= 100:
        score_æ¥é™¢æ‚£è€…æ•° = 20
    elif user_ratings_total >= 50:
        score_æ¥é™¢æ‚£è€…æ•° = 15
    elif user_ratings_total >= 20:
        score_æ¥é™¢æ‚£è€…æ•° = 10
    elif user_ratings_total >= 10:
        score_æ¥é™¢æ‚£è€…æ•° = 5
    else:
        score_æ¥é™¢æ‚£è€…æ•° = 0

    # 3. å­ã©ã‚‚å¯¾å¿œåŠ› (30ç‚¹)
    score_å­ã©ã‚‚å¯¾å¿œåŠ› = 0
    if analysis.get('kids_content'):
        score_å­ã©ã‚‚å¯¾å¿œåŠ› += 15
    if any(kw in clinic_name for kw in ['å°å…', 'ã“ã©ã‚‚', 'å­ã©ã‚‚', 'ã‚­ãƒƒã‚º', 'çŸ¯æ­£']):
        score_å­ã©ã‚‚å¯¾å¿œåŠ› += 10
    if analysis.get('waiting_room_photo'):
        score_å­ã©ã‚‚å¯¾å¿œåŠ› += 5
    score_å­ã©ã‚‚å¯¾å¿œåŠ› = min(score_å­ã©ã‚‚å¯¾å¿œåŠ›, 30)

    # 4. Webç©æ¥µæ€§ (15ç‚¹)
    sns_count = sum([
        analysis.get('sns_instagram', False),
        analysis.get('sns_facebook', False),
        analysis.get('sns_line', False),
        analysis.get('sns_twitter', False)
    ])
    score_Webç©æ¥µæ€§ = min(sns_count * 5, 15)

    # 5. åŒ»é™¢è¦æ¨¡ (10ç‚¹)
    score_åŒ»é™¢è¦æ¨¡ = 0
    if analysis.get('operating_hours'):
        score_åŒ»é™¢è¦æ¨¡ += 5
    try:
        photos = int(row.get('å†™çœŸæšæ•°', 0) or 0)
        if photos >= 10:
            score_åŒ»é™¢è¦æ¨¡ += 5
    except ValueError:
        pass

    # 6. ãƒ–ãƒ­ã‚°æ´»å‹• (5ç‚¹)
    score_ãƒ–ãƒ­ã‚°æ´»å‹• = 0
    blog_updated = analysis.get('blog_updated')
    if blog_updated:
        try:
            blog_date = datetime.strptime(blog_updated, '%Y-%m-%d')
            days_ago = (datetime.now() - blog_date).days

            if days_ago <= 30:
                score_ãƒ–ãƒ­ã‚°æ´»å‹• = 5
            elif days_ago <= 60:
                score_ãƒ–ãƒ­ã‚°æ´»å‹• = 4
            elif days_ago <= 90:
                score_ãƒ–ãƒ­ã‚°æ´»å‹• = 3
            elif days_ago <= 180:
                score_ãƒ–ãƒ­ã‚°æ´»å‹• = 2
            elif days_ago <= 365:
                score_ãƒ–ãƒ­ã‚°æ´»å‹• = 1
        except ValueError:
            pass

    # ç·åˆã‚¹ã‚³ã‚¢
    total_score = (
        score_åŸºç¤è©•ä¾¡ +
        score_æ¥é™¢æ‚£è€…æ•° +
        score_å­ã©ã‚‚å¯¾å¿œåŠ› +
        score_Webç©æ¥µæ€§ +
        score_åŒ»é™¢è¦æ¨¡ +
        score_ãƒ–ãƒ­ã‚°æ´»å‹•
    )

    # çµæœãƒ¬ã‚³ãƒ¼ãƒ‰
    result = {
        'clinic_name': clinic_name,
        'total_score': round(total_score, 1),
        'scores': {
            'åŸºç¤è©•ä¾¡': round(score_åŸºç¤è©•ä¾¡, 1),
            'æ¥é™¢æ‚£è€…æ•°': score_æ¥é™¢æ‚£è€…æ•°,
            'å­ã©ã‚‚å¯¾å¿œåŠ›': score_å­ã©ã‚‚å¯¾å¿œåŠ›,
            'Webç©æ¥µæ€§': score_Webç©æ¥µæ€§,
            'åŒ»é™¢è¦æ¨¡': score_åŒ»é™¢è¦æ¨¡,
            'ãƒ–ãƒ­ã‚°æ´»å‹•': score_ãƒ–ãƒ­ã‚°æ´»å‹•
        },
        'website_analysis': analysis,
        'raw_data': {
            'rating': rating,
            'user_ratings_total': user_ratings_total,
            'formatted_address': row.get('ä½æ‰€', ''),
            'formatted_phone_number': row.get('é›»è©±ç•ªå·', ''),
            'website': row.get('Webã‚µã‚¤ãƒˆURL', ''),
            'photos': row.get('å†™çœŸæšæ•°', ''),
            'operating_hours': row.get('å–¶æ¥­æ™‚é–“', ''),
            'google_maps_url': row.get('Google Maps URL', '')
        }
    }

    scoring_results.append(result)

print(f"ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {len(scoring_results)}ä»¶")
```

### STEP 4: JSONå‡ºåŠ›

```python
from datetime import datetime
import json
from pathlib import Path

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f'scoring_results_batch_003_retry_{timestamp}.json'

output_data = {
    'metadata': {
        'batch_file': 'batch_003_to_score.csv',
        'total_clinics': len(all_rows),
        'unique_clinics': len(unique_clinics),
        'timestamp': datetime.now().isoformat(),
        'retry_execution': True,
        'webfetch_forced': True,
        'director_names_found': director_names_found,
        'director_extraction_rate': f"{director_names_found/len(unique_clinics)*100:.1f}%"
    },
    'results': scoring_results
}

output_file = Path('/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads') / output_path

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"âœ“ JSONå‡ºåŠ›å®Œäº†: {output_file}")
print(f"âœ“ åŒ»é™¢é•·åå–å¾—ç‡: {director_names_found}/{len(unique_clinics)} ({director_names_found/len(unique_clinics)*100:.1f}%)")

# ã‚¹ã‚³ã‚¢çµ±è¨ˆ
scores = [r['total_score'] for r in scoring_results]
avg_score = sum(scores) / len(scores) if scores else 0
max_score = max(scores) if scores else 0
min_score = min(scores) if scores else 0

print(f"\nğŸ“Š ã‚¹ã‚³ã‚¢çµ±è¨ˆ:")
print(f"   å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.1f}ç‚¹")
print(f"   æœ€é«˜ã‚¹ã‚³ã‚¢: {max_score:.1f}ç‚¹")
print(f"   æœ€ä½ã‚¹ã‚³ã‚¢: {min_score:.1f}ç‚¹")

# é«˜ã‚¹ã‚³ã‚¢åŒ»é™¢TOP 10
top_10 = sorted(scoring_results, key=lambda x: x['total_score'], reverse=True)[:10]
print(f"\nğŸ† é«˜ã‚¹ã‚³ã‚¢åŒ»é™¢ TOP 10:")
for i, clinic in enumerate(top_10, 1):
    print(f"   {i}. {clinic['clinic_name']}: {clinic['total_score']}ç‚¹")
```

## å®Ÿè¡Œæ™‚é–“ã®ç›®å®‰

- 119ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢ Ã— (ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸10ç§’ + ã‚µãƒ–ãƒšãƒ¼ã‚¸30ç§’) = ç´„80åˆ†
- åŒ»é™¢é•·åæŠ½å‡ºç‡ç›®æ¨™: 70%ä»¥ä¸Š

## æ³¨æ„äº‹é …

1. **WebFetchã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ**: ä¸€éƒ¨ã®åŒ»é™¢ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šã—ã¾ã™ã€‚
2. **åŒ»é™¢é•·åæŠ½å‡ºç‡**: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®ã¿ã§ã¯30%ã€ã‚µãƒ–ãƒšãƒ¼ã‚¸æ¢ç´¢ã§70-80%ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚
3. **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**: 1ç§’å¾…æ©Ÿã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿ã—ã¦ã„ã¾ã™ã€‚

## å®Ÿè¡Œå¾Œã®ç¢ºèªäº‹é …

- [ ] JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] åŒ»é™¢é•·åæŠ½å‡ºç‡ãŒ70%ä»¥ä¸Šã‹
- [ ] ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ã‚¨ãƒ©ãƒ¼ä»¶æ•°ãŒ10%æœªæº€ã‹
