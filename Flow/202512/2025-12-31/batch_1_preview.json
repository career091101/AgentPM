[
  {
    "video_id": "1WImBwiA7RA",
    "filename": "1WImBwiA7RA.md",
    "content_preview": "# Transcript: 1WImBwiA7RA\n\n- URL: https://www.youtube.com/watch?v=1WImBwiA7RA\n- Retrieved at: 2025-12-30T09:18:34+09:00\n\n## Text\n\n- [00:00] Cloud just introduced this concept\n- [00:01] called agent skills. It might look\n- [00:02] complicated but it's actually really\n- [00:04] simple ideas but super powerful and many\n- [00:06] things that it might be even bigger than\n- [00:08] MCP. So a cloud skill you can almost\n- [00:10] consider as a combination of both a prom\n- [00:12] instruction to teach agent how to do\n- [00:14] certain skills as well as a list of\n- [00:16] assets and tools like predefined\n- [00:18] functions and templates guidelines to\n- [00:20] make it produce more consistent result.\n- [00:22] But tool and assets are kind of\n- [00:23] optional. It can be as simple as just\n- [00:25] one single prompt as well. For example,\n- [00:27] for brand guideline skill, literally\n- [00:29] just one single prompt including very\n- [00:30] specific brand guidelines that you want\n- [00:32] agent to follow. Same thing here. I also\n- [00:34] have a UI design skills that just one\n- [00:37] single prompt. So all the skills start\n- [00:39] with a skill.md file including this\n- [00:41] short description explain to agent when\n- [00:43] to use this skill and this description\n- [00:44] will always be added to the agent\n- [00:46] context. So you will know what type of\n- [00:48] skills it has access to and when agent\n- [00:50] decide to call the skill this where the\n- [00:52] rest of context below will be loaded. So\n- [00:54] this part almost feel like cloud code\n- [00:56] commands. So skill.mmd is the most\n- [00:58] necessary part of skill. For more\n- [01:00] complex skill you can also include more\n- [01:02] resource. For example for this skill to\n- [01:03] generate algorithm art. They also\n- [01:05] include resource like example\n- [01:06] implementation and get agent to raise\n- [01:09] those example reference before they\n- [01:10] implement to making sure we have more\n- [01:12] consistent result. But more importantly\n- [01:14] you can also include some predefined\n- [01:15] functions like in this slack give\n- [01:17] creator skill. They already import\n- [01:19] packages and predefined functions and\n- [01:21] the instruction here basically tell the\n- [01:22] agent how to use those functions to\n- [01:24] create a nice gift out of the box. And\n- [01:26] here's why I think skills might be even\n- [01:28] better than MCPS. So MCP has been\n- [01:30] awesome way to extend agents capability\n- [01:32] by connecting agent with new MCPS can\n- [01:35] suddenly do new things that it couldn't\n- [01:36] do before. The problem is that MCP in\n- [01:38] practical is not that easy to use.\n- [01:40] Firstly, MCP can consume a whole bunch\n- [01:42] of token that is unnecessary because\n- [01:43] each MCP can contain a bundle of\n- [01:46] different tools and each tool here\n- [01:47] including the description about when to\n- [01:49] use this tool as well as input schema\n- [01:51] and all those token will be loaded to\n- [01:53] agent context regardless whether this is\n- [01:55] useful or not and more importantly quite\n- [01:57] often MCP builder will want to build the\n- [01:59] tools in a more kind of modular way so\n- [02:01] it's reusable and more composable but\n- [02:03] that also means most of MCP is not\n- [02:05] something you just connect and use you\n- [02:06] want to give agent more detailed\n- [02:08] instruction about the order of when to\n- [02:10] use which tool and that may set up more\n- [02:12] complicated. But on the other hand, the\n- [02:13] way skill is set up allow you to consume\n- [02:15] much less token but perform much more\n- [02:17] complicated tasks. Let's take chassis\n- [02:19] and MCP as example. At default, it has\n- [02:21] seven different tools for different\n- [02:23] purpose. If I load the context, you will\n- [02:25] see those chassis MCP tools already take\n- [02:27] about 4.2,000 token. But you can imagine\n- [02:30] if we turn that into a chassine skill,\n- [02:32] we can probably reduce token from\n- [02:34] 4.2,000 to just 70. And that means your\n- [02:36] agent can be equipped with many more\n- [02:38] skills and it should just work out of\n- [02:40] box because skill.md can already contain\n- [02:42] all the referencing instructions. But\n- [02:44] without further ado, let's show you\n- [02:45] example. So here under docloud/sklls\n- [02:48] we have loaded a list of different\n- [02:49] skills and there's one skill called\n- [02:50] slack gift creator which including the\n- [02:52] description the four details about how\n"
  },
  {
    "video_id": "6B0p9rCN_p0",
    "filename": "6B0p9rCN_p0.md",
    "content_preview": "# Transcript: 6B0p9rCN_p0\n\n- URL: https://www.youtube.com/watch?v=6B0p9rCN_p0\n- Retrieved at: 2025-12-30T09:40:19+09:00\n\n## Text\n\n- [00:00] Okay, so this week OpenAI released their\n- [00:02] GPA 5 codec and for the last few days\n- [00:05] I've been testing it out uh in of course\n- [00:08] codeex CLI and I got to say I'm pretty\n- [00:11] impressed by this. So I just wanted to\n- [00:12] make a short video where I try to do a\n- [00:14] change in my setup I have that I built\n- [00:17] for the last video and just see how it\n- [00:19] works out. Uh because we have an issue\n- [00:22] we want to try to uh switch a model. Uh,\n- [00:25] so I want to try to use the new GP5\n- [00:27] codeex for that today. And I just want\n- [00:29] to give my first impressions on this.\n- [00:31] What I like, what I don't like, and what\n- [00:33] I think, yeah, you can use this for if\n- [00:36] you, yeah, thinking about switching from\n- [00:38] cloud code or if you just want to try\n- [00:40] this out if you have like a chachet\n- [00:43] subscription. I'm just on the the pro\n- [00:46] version now. That's a $20 subscription\n- [00:48] on Chachet. Uh, so I am using uh logging\n- [00:52] into Codex CLI using that. So, I had a\n- [00:54] look at a few of like the benchmarks\n- [00:56] they did here, but I'm not going to\n- [00:58] focus too much on that today. Uh, I just\n- [01:00] want to try it out and see what we can\n- [01:02] do with it. So, you can see from the SWE\n- [01:04] bench, uh, they have a small bump here.\n- [01:06] So, that's uh, I think it's around the\n- [01:08] same uh, that like Opus 4.1 is on and\n- [01:12] other models, right? So, yeah, pretty\n- [01:15] good. It's a bit better on GP5. Uh, but\n- [01:17] I think uh, what I enjoyed most about\n- [01:19] Codex, uh, I'm going to kind of explain\n- [01:21] it soon. So yeah, just let get into the\n- [01:24] project and let's see what we can do\n- [01:26] with this. So in my previous video, we\n- [01:28] generated this uh setup in codeex where\n- [01:30] we use MCP servers to generate videos.\n- [01:34] So I'm not going to play this with\n- [01:35] sound, but this is like an avatar of me\n- [01:37] just created from one image. You can go\n- [01:38] back and watch the previous video. So\n- [01:40] you can see we are changing up the\n- [01:41] angles here. We are using like an AI\n- [01:43] avatar model and we are combining this\n- [01:46] with a bunch of MCP servers like 11 Labs\n- [01:49] and Nano Banana to create these types of\n- [01:51] videos. Uh but lately I had some issues\n- [01:54] this week with the Omnihuman model. So I\n- [01:57] thought we can use the new GI5 codeex to\n- [01:59] just try to switch to the cling version\n- [02:01] of this AI avatar model and just see how\n- [02:04] smooth this is going to be just to swap\n- [02:05] it over. So the way I want to do this is\n- [02:09] uh I want to go into my repo here. Here\n- [02:11] I just want to type codeex. I have\n- [02:12] updated all of this now. And if we do\n- [02:15] like a /mod now, you can see we are on\n- [02:18] the codeex medium. I'm just going to\n- [02:20] keep it at that. We could of course\n- [02:21] switch to high or low, but I'm just\n- [02:23] going to stay at medium now for now. Uh\n- [02:26] so what I want to do is uh I want to\n- [02:28] give some instructions here that we want\n- [02:30] to switch to this um cling AI avatar. So\n- [02:35] what I'm going to do first is I'm going\n- [02:37] to go grab some documentation here from\n- [02:39] the API.\n- [02:41] So, I'm just going to grab all the\n- [02:42] documentation here from this uh model,\n- [02:45] right? And I'm just going to create uh\n- [02:48] yeah, a documentation here. I'm just\n- [02:50] going to call it something like cling\n- [02:54] uh avatar.md or something from fal. So,\n- [02:57] this is the same provider. So, we\n- [02:59] already have the API key here. Uh, and\n- [03:02] now I'm just going to try to instruct\n- [03:05] uh with codeex here to switch up these\n- [03:07] two models from the omni one to the one\n- [03:09] from cling and see how smooth this goes.\n- [03:13] So let's just start by reading the new\n- [03:14] documentation. Uh, so did we call it\n- [03:16] cling avatar, right? So let's just do\n- [03:18] that. So uh I want to do a few comments\n- [03:21] on what I have liked so far and I think\n- [03:24] this new dynamic tinking uh this makes\n- [03:27] the new codeex model fit a much more\n- [03:29] responsive. You can see how quick this\n- [03:30] was, right? Uh so what I have noticed is\n- [03:33] the speed improvements have been great.\n- [03:35] Uh they had this thing about like I said\n- [03:38] dynamic thinking. So they tried to not\n"
  },
  {
    "video_id": "847eGg-X7Us",
    "filename": "847eGg-X7Us.md",
    "content_preview": "# Transcript: 847eGg-X7Us\n\n- URL: https://www.youtube.com/watch?v=847eGg-X7Us\n- Retrieved at: 2025-12-30T09:46:48+09:00\n\n## Text\n\n- [00:00] こんにちは。江です。今回はAI\n- [00:02] エージェント時代に活躍する7つの週慣と\n- [00:05] 題しまして、日々私がこの時代に楽しく\n- [00:09] 生き生きとしっかり活躍するために意識し\n- [00:12] ていること習慣を7つにまとめてご紹介し\n- [00:15] ます。この習慣を皆さんも意識的にやって\n- [00:18] もらうことによってAIにもっと慣れしん\n- [00:21] で楽しくワクワクしながら、え、AIを\n- [00:24] 使って仕事を効率化したり、新しいことが\n- [00:27] できたり、稼ぐことができるという風に\n- [00:29] 思うので、是非参考にしいただければなと\n- [00:31] 思っています。え、このチャンネルはAI\n- [00:34] エージェントに活躍するスキルやノーハを\n- [00:36] 学ぶというテーマで実践に目指した様々な\n- [00:39] AI活用ノーハウや毎週日曜15時にAI\n- [00:41] ニュースを発信しております。また今回の\n- [00:44] 資料はLINE登録いただいてAI週間と\n- [00:46] 入力いただくと消することができます。\n- [00:49] それではまずは1個目の習慣いきましょう\n- [00:50] 。1つ目は議嫌をせずにAIを試すという\n- [00:55] のが非常に重要な習慣になると思います。\n- [00:59] 今日々新しいツールが大量に出てきていて\n- [01:02] 正直もうニュースも大変だし試すのめど\n- [01:05] くさいという方が多いですよね。ただ、今\n- [01:09] 100分は一見にしか言いますが、AI\n- [01:11] ツールは実際に試してみないと分からない\n- [01:14] ことってすごく多いんですね。例えば私、\n- [01:17] え、先週のニュースでこんなニュースを\n- [01:19] 出しました。クロードにスキルという機能\n- [01:22] が出ましたよと。このスキルというのは\n- [01:24] 書いてあるように、指示やスクリプト、\n- [01:26] リソースやホルダー形式にまとめて特定の\n- [01:29] タスクを実行できますと。これによって\n- [01:31] 使用作成とかExel整理など業務用と\n- [01:34] 向けのモジュール化された専門処理が\n- [01:36] できるんです。さあ、こういったニュース\n- [01:39] はもっと分かりやすいものも含めてこの\n- [01:41] クロードスキルやった瞬間にいろんな\n- [01:43] ところでバーっと出たわけなんですけども\n- [01:45] 、このニュースを見て頭で分かってるだけ\n- [01:48] になってる人と本当に実際使ってみてどこ\n- [01:51] までできるのか、どんな可能性があるかを\n- [01:54] 試して知ってる人は大きな差が出ると思う\n- [01:56] んですね。このニュースを見てこの\n- [01:58] クロードがどのぐらいのことができて逆に\n- [02:01] 難しいのかってパッと理解できます\n- [02:03] でしょうか?さ、実際私クロードを使って\n- [02:05] みましてやってみました。今これクロード\n- [02:07] の中で私自身が自分自身でスキルという\n- [02:10] ものを作っていました。このスキルを作る\n- [02:12] のもスキルを作るためのスキルがあるので\n- [02:15] 結構簡単だったんですね。で、自分の会社\n- [02:17] のパワポテンプレを入れまして今下にある\n- [02:20] ようなニュースの情報をパワポにしてねと\n- [02:23] いう風にお願いするとどのぐらいの制度が\n- [02:26] 出たかというと実は非常に高いクオリティ\n- [02:28] で資料を作ることができたんですね。今\n- [02:31] これ結構長いので10倍速の動画にして\n- [02:34] おりますけども、自分自身の会社の\n- [02:36] パイント、テンプレートをスライドを使っ\n- [02:38] ていきながらバババッと作ってくれまして\n- [02:41] 、で、結果的にスライドができましたよと\n- [02:44] 。え、ここでできたスライドというのは今\n- [02:46] ご覧いただいてる通り、ま、これ僕が作っ\n- [02:48] ている会社のテンプレイにった形で文字数\n- [02:51] もしっかり把握した上でパワポイントに\n- [02:54] ダウンロードして完全にそのまますぐ\n- [02:56] 使えるクオリティでできちゃってるんです\n- [02:58] ね。内容を今回の場合5個のテンプレが\n- [03:01] あるんですけど、5個のテンプレとはめて\n- [03:03] もらった上で文字分割して配置してくれる\n- [03:06] 。しかも使っているのは自社の\n- [03:08] テンプレートなので、もはやこの\n- [03:10] テンプレート自体は編集も非常に簡単です\n- [03:12] し、自分の会社の提案資料やプレゼン資料\n- [03:15] に組み込むことができちゃうんですね。\n- [03:18] これを実際試してみてここまでできるんだ\n- [03:21] と分かっている人となんとなくニュースを\n- [03:23] 見てこれができそうと思ってる人は何か\n- [03:26] 問題があったりあるビジネスの現場で\n- [03:29] ツールを使わなちゃいけない時にひらめく\n- [03:31] リアリティとか裸感が全然違うわけですね\n- [03:34] 。逆にできないこととしては細かい点なん\n- [03:37] ですけどこの見出しにおいてこのえ点が出\n- [03:42] てますよね。これ本当は下の方は過剰脇け\n- [03:45] なんで点を出したいんですけど上の見出し\n- [03:47] にはいらなくてそもそもはつけてないん\n- [03:49] です。ところがこういう細かい調整は\n- [03:52] ちょっと難しくて今んところプロンプトを\n- [03:54] 頑張ってもできないのでもうちょっと\n- [03:56] しっかり蓋を分けて中を見なくちゃいけ\n- [03:58] ないなという状況にあるわけなんですけど\n- [04:01] 1回やることによってここまでは簡単に\n"
  },
  {
    "video_id": "8_liatgLkLc",
    "filename": "8_liatgLkLc.md",
    "content_preview": "# Transcript: 8_liatgLkLc\n\n- URL: https://www.youtube.com/watch?v=8_liatgLkLc\n- Retrieved at: 2025-12-30T09:49:13+09:00\n\n## Text\n\n- [00:00] This video is brought to you by\n- [00:01] Squarespace. If you're trying to build\n- [00:03] something with large language models,\n- [00:05] you know things can get messy pretty\n- [00:07] quickly. You start with one prompt, then\n- [00:10] you add a few tools, then you pass some\n- [00:12] data to another model, and before long,\n- [00:14] your logic is going to be scattered and\n- [00:16] all over the place. So, the question is,\n- [00:19] how do you structure AI systems the\n- [00:21] right way? In this video, I'm going to\n- [00:24] show you three design patterns that I've\n- [00:27] adapted for LLMs and agents that are\n- [00:30] going to help you build cleaner, more\n- [00:32] modular AI systems. Let's go. The\n- [00:35] example that I'm going to use in this\n- [00:37] video is a travel agent. And this agent\n- [00:40] can help you find hotels and flights,\n- [00:43] etc., etc. Now, there is one prompt that\n- [00:46] I'm going to start with, which is this.\n- [00:49] I want a rainy city trip within Europe.\n- [00:52] I love towers. I don't want to cross\n- [00:54] water to get there. And this is Maria\n- [00:56] who lives in Berlin. So, let's run this\n- [00:59] particular example and see what happens.\n- [01:01] The agent has decided that Maria needs\n- [01:03] to go to Paris, France, which is wrong.\n- [01:06] It should be Utre, the Netherlands.\n- [01:08] Anyway, okay, Paris. Fine. So, then the\n- [01:11] agent books a flight from Berlin to\n- [01:13] Paris, which arrives at 6:00 p.m. It\n- [01:17] looks for a hotel, which is the highest\n- [01:19] regency in Paris. I don't know if this\n- [01:21] is a good hotel or not. And then uh but\n- [01:25] it is quite expensive so hopefully it\n- [01:28] is. And then there are some activities\n- [01:29] for early evening explorers. You can\n- [01:32] visit the shopping mall, take an evening\n- [01:35] walk, uh enjoy a dinner, etc., etc. So\n- [01:39] the Asian has done like a bunch of\n- [01:41] different things here. And that's\n- [01:44] typically something that you're going to\n- [01:45] encounter in AI application, right? It's\n- [01:47] not just I send a prompt and I get back\n- [01:50] a response. I mean, we can just use a\n- [01:52] chat agent for that. We want something\n- [01:54] that's a bit more uh involved. And of\n- [01:57] course, in this particular situation,\n- [01:58] you don't want to handle everything in\n- [02:00] one massive prompt because that's simply\n- [02:03] not going to work. You need to break it\n- [02:05] down into separate agents and then each\n- [02:08] is responsible for a single step. So we\n- [02:10] have choosing a destination, we have\n- [02:12] planning the flight, recommending a\n- [02:14] hotel, and suggesting things to do. The\n- [02:17] way I've built this is that I'm\n- [02:19] implementing each of these steps as its\n- [02:21] own function. And then you can execute\n- [02:25] them in order by storing them in a list.\n- [02:27] More about that in a minute. So how is\n- [02:29] this code actually set up? I'm using\n- [02:31] byic AI for the agents. So first I load\n- [02:35] the environment variables. In this case,\n- [02:37] there's just one which is the OpenAI API\n- [02:39] key. Then we have some dependencies\n- [02:43] which is we need to know the username.\n- [02:45] We need to know the origin city and then\n- [02:48] there is a context that's basically the\n- [02:51] information that each of these agents is\n- [02:53] going to need. So things like the\n- [02:54] destination, the uh origin city, the\n- [02:57] arrival time, the hotel name, etc. And\n- [02:59] this going to be filled in step by step\n- [03:01] by the agents. Then for each step I have\n- [03:04] an agent. So in this case I have a\n- [03:06] destination agent that helps users find\n- [03:09] a ideal tra travel destination based on\n- [03:12] their preferences.\n- [03:14] Then I have a flight agent that can look\n- [03:17] for flights for a particular trip. Then\n- [03:21] I have a hotel recommendation agent that\n- [03:26] suggests a good hotel near the airport\n- [03:28] or city center. Then we have an activity\n- [03:31] agent that suggests local activities. So\n- [03:34] you have all these different agents that\n- [03:35] can do each a specific thing that also\n- [03:37] have their own system prompts that to\n- [03:40] adapt them. And then the pattern that\n- [03:41] I'm using to perform these tasks in\n- [03:44] order is the chain of responsibility.\n- [03:47] And what that pattern entails that you\n- [03:49] have a bunch of different functions that\n"
  },
  {
    "video_id": "8tg8z-Fi0MU",
    "filename": "8tg8z-Fi0MU.md",
    "content_preview": "# Transcript: 8tg8z-Fi0MU\n\n- URL: https://www.youtube.com/watch?v=8tg8z-Fi0MU\n- Retrieved at: 2025-12-30T09:51:00+09:00\n\n## Text\n\n- [00:08] Thank you everyone for coming. Um,\n- [00:09] [music] super exciting to see the energy\n- [00:11] today in this in this event. Uh, I know\n- [00:14] deep learning AI has done so much for\n- [00:16] educating the community at large. So\n- [00:18] awesome to see the energy here as people\n- [00:19] are kind of just talking about the what\n- [00:21] they're going to be building in the next\n- [00:22] frontier of AI. Um, so just a brief\n- [00:25] overview of what we're going to be\n- [00:26] talking about today. We will be covering\n- [00:29] um an overview of uh our developer\n- [00:32] platform, some of the newer features\n- [00:33] such as the cloud agent SDK, um some of\n- [00:36] the new advancements in our most recent\n- [00:37] models. Um and I'll just say that like\n- [00:40] in a lot of these different topics we're\n- [00:42] going to be covering today, we could\n- [00:43] easily spend 30 minutes on just a single\n- [00:45] one, right? So, um we're going to try to\n- [00:48] give a a broad overview of our offerings\n- [00:51] and hopefully you all come away learning\n- [00:52] something you didn't know about Claude\n- [00:54] and our and our products before. Um, so\n- [00:57] before we jump into things, I'll give a\n- [00:58] quick introduction. Uh, my name is\n- [01:00] Stanvir. I'm part of the applied AI team\n- [01:02] here at Enthropic. So I spend most of my\n- [01:04] time working with our c some of our most\n- [01:06] strategic customers who are building\n- [01:08] stuff on top of uh our models and our\n- [01:11] infrastructure. Uh, specifically\n- [01:12] focusing in the healthcare life sciences\n- [01:13] domain and separately do some also\n- [01:15] internal product research work to\n- [01:17] improve our models at the domains we\n- [01:18] care about. Um, and other product\n- [01:20] launches such as our memory system in\n- [01:22] cloud AI or the MCP registry. Hi\n- [01:25] everybody, my name is Utkar. I work in\n- [01:27] infrastructure at Anthropic supporting\n- [01:29] both the product and research sides of\n- [01:31] the company.\n- [01:35] So let's get started. Um to kick us off,\n- [01:39] I want to take you all through the suite\n- [01:41] of products that Enthropical offers to\n- [01:44] give you a quick kind of summary of\n- [01:46] things. Um and I'm going to present this\n- [01:48] in the form of a stack because I think\n- [01:50] that's the most intuitive way to think\n- [01:52] about it. Um you know at the bottom\n- [01:55] layer obviously is um what enthropic\n- [01:58] focuses on a lot which is the models. Um\n- [02:01] you guys all know that we have three\n- [02:03] classes of models. We have haiku sonnet\n- [02:06] and opus. Haiku being kind of like our\n- [02:08] most costefficient models. Opus being\n- [02:10] the top of the line our biggest and most\n- [02:13] intelligence model and sonnet being kind\n- [02:15] of like your day-to-day workhorse where\n- [02:17] we really try to strike a balance\n- [02:19] between um intelligence and cost. But\n- [02:22] you know the models by themselves the\n- [02:24] intelligence by itself is not useful\n- [02:27] unless we build things and products to\n- [02:29] bring them to the users to the world and\n- [02:31] that's where we step into the upper\n- [02:33] layers of the stack. So one layer above\n- [02:36] is kind of where we try to build the\n- [02:39] build the kind of capabilities that uh\n- [02:42] can be used to build agentic\n- [02:44] applications uh more easily. So these\n- [02:47] are things like memory that lets um you\n- [02:50] know multiple conversations basically\n- [02:52] share the same context. Um these are\n- [02:55] things like web research um or sorry web\n- [02:58] search research and orchestration that\n- [03:00] basically lets claude do more\n- [03:02] complicated tasks or have multiple sub\n- [03:04] aents kind of working in parallel um\n- [03:07] towards a single uh goal. And then\n- [03:11] finally on top of that is where we have\n- [03:13] the entropics like top level platform\n- [03:16] which are things like cloud code, cloud\n- [03:18] AI, you guys all know these. Um and then\n- [03:21] the cloud developer platform which is\n- [03:23] what we are going to be uh spending most\n- [03:25] of our time on today. This is what you\n- [03:27] guys build on top of to build your own\n- [03:29] applications. Um and in the next slide\n- [03:32] I'm going to take you through the road\n- [03:34] map for uh the developer platform.\n- [03:38] And actually, let me go back for a\n"
  }
]