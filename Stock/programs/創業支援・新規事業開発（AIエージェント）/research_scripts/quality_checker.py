#!/usr/bin/env python3
"""
Quality Checker for Founder Research Case Studies

ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã®å“è³ªãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
100ç‚¹æº€ç‚¹ã§å“è³ªã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã—ã€PASS/WARN/FAILã‚’åˆ¤å®šã€‚
"""

import os
import re
import yaml
from pathlib import Path
from typing import Dict, List, Tuple

class QualityChecker:
    """ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£å“è³ªãƒã‚§ãƒƒã‚¯"""

    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.documents_path = self.base_path / "documents"

    def check_file(self, file_path: Path) -> Dict:
        """1ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªãƒã‚§ãƒƒã‚¯"""
        result = {
            'file': file_path.name,
            'scores': {},
            'total_score': 0,
            'grade': 'FAIL',
            'issues': []
        }

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # YAML Front MatteræŠ½å‡º
            match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
            if not match:
                result['issues'].append('YAML Front MatterãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
                return result

            yaml_content = match.group(1)
            metadata = yaml.safe_load(yaml_content)

            # å„é …ç›®ã‚’ãƒã‚§ãƒƒã‚¯
            result['scores']['yaml_syntax'] = self._check_yaml_syntax(yaml_content)
            result['scores']['required_fields'] = self._check_required_fields(metadata)
            result['scores']['sources_count'] = self._check_sources_count(metadata, content)
            result['scores']['cpf_data'] = self._check_cpf_data(metadata)
            result['scores']['psf_data'] = self._check_psf_data(metadata)
            result['scores']['fact_check'] = self._check_fact_check(metadata)

            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            result['total_score'] = sum(result['scores'].values())

            # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
            if result['total_score'] >= 80:
                result['grade'] = 'PASS'
            elif result['total_score'] >= 60:
                result['grade'] = 'WARN'
            else:
                result['grade'] = 'FAIL'

        except Exception as e:
            result['issues'].append(f'ã‚¨ãƒ©ãƒ¼: {str(e)}')

        return result

    def _check_yaml_syntax(self, yaml_content: str) -> int:
        """YAMLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆ10ç‚¹ï¼‰"""
        try:
            yaml.safe_load(yaml_content)
            return 10
        except:
            return 0

    def _check_required_fields(self, metadata: Dict) -> int:
        """å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆ20ç‚¹ï¼‰"""
        required_fields = [
            'id',
            'title',
            'tier',
            ('founder', 'name'),
            ('company', 'name'),
            ('company', 'founded_year'),
            ('company', 'industry'),
            ('funding', 'total_raised'),
            ('validation_data', 'cpf'),
            ('validation_data', 'psf')
        ]

        score = 0
        for field in required_fields:
            if isinstance(field, tuple):
                # ãƒã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                value = metadata
                for key in field:
                    value = value.get(key, {}) if isinstance(value, dict) else None
                    if value is None:
                        break
                if value:
                    score += 2
            else:
                # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                if metadata.get(field):
                    score += 2

        return min(score, 20)

    def _check_sources_count(self, metadata: Dict, content: str) -> int:
        """ã‚½ãƒ¼ã‚¹æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆ20ç‚¹ï¼‰"""
        sources_count = metadata.get('quality', {}).get('sources_count', 0)

        # å‚ç…§ã‚½ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚‚ã‚«ã‚¦ãƒ³ãƒˆ
        source_section = re.search(r'## å‚ç…§ã‚½ãƒ¼ã‚¹\s+(.*?)(?=\n##|\Z)', content, re.DOTALL)
        if source_section:
            source_lines = [line for line in source_section.group(1).split('\n') if line.strip().startswith(('1.', '2.', '3.', '4.', '5.'))]
            actual_sources = len(source_lines)
            sources_count = max(sources_count, actual_sources)

        if sources_count >= 5:
            return 20
        elif sources_count >= 2:
            return 10
        elif sources_count == 1:
            return 5
        else:
            return 0

    def _check_cpf_data(self, metadata: Dict) -> int:
        """CPFãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ20ç‚¹ï¼‰"""
        cpf_data = metadata.get('validation_data', {}).get('cpf', {})

        score = 0

        # interview_countï¼ˆ5ç‚¹ï¼‰
        if cpf_data.get('interview_count') is not None:
            score += 5

        # problem_commonalityï¼ˆ5ç‚¹ï¼‰
        if cpf_data.get('problem_commonality') is not None:
            score += 5

        # wtp_confirmedï¼ˆ5ç‚¹ï¼‰
        if cpf_data.get('wtp_confirmed') is not None:
            score += 5

        # urgency_scoreï¼ˆ5ç‚¹ï¼‰
        if cpf_data.get('urgency_score') is not None:
            score += 5

        return score

    def _check_psf_data(self, metadata: Dict) -> int:
        """PSFãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ20ç‚¹ï¼‰"""
        psf_data = metadata.get('validation_data', {}).get('psf', {})

        score = 0

        # ten_x_axesï¼ˆ10ç‚¹ï¼‰
        ten_x_axes = psf_data.get('ten_x_axes', [])
        if len(ten_x_axes) >= 2:
            score += 10
        elif len(ten_x_axes) == 1:
            score += 5

        # mvp_typeï¼ˆ5ç‚¹ï¼‰
        if psf_data.get('mvp_type'):
            score += 5

        # initial_cvrï¼ˆ5ç‚¹ï¼‰
        if psf_data.get('initial_cvr') is not None:
            score += 5

        return score

    def _check_fact_check(self, metadata: Dict) -> int:
        """ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆ10ç‚¹ï¼‰"""
        fact_check = metadata.get('quality', {}).get('fact_check', '')

        if fact_check == 'pass':
            return 10
        elif fact_check == 'warn':
            return 5
        else:
            return 0

    def check_all_files(self) -> Tuple[List[Dict], Dict]:
        """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªãƒã‚§ãƒƒã‚¯"""
        results = []
        summary = {
            'total': 0,
            'pass': 0,
            'warn': 0,
            'fail': 0,
            'avg_score': 0
        }

        print("\n" + "="*60)
        print("ğŸ” å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        print("="*60 + "\n")

        for category_dir in self.documents_path.iterdir():
            if not category_dir.is_dir():
                continue

            print(f"ğŸ“ {category_dir.name}")

            for md_file in category_dir.glob("FOUNDER_*.md"):
                result = self.check_file(md_file)
                results.append(result)

                # ã‚µãƒãƒªãƒ¼æ›´æ–°
                summary['total'] += 1
                if result['grade'] == 'PASS':
                    summary['pass'] += 1
                    icon = 'âœ…'
                elif result['grade'] == 'WARN':
                    summary['warn'] += 1
                    icon = 'âš ï¸ '
                else:
                    summary['fail'] += 1
                    icon = 'âŒ'

                print(f"  {icon} {result['file']}: {result['total_score']}ç‚¹ ({result['grade']})")

                # å•é¡ŒãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                if result['issues']:
                    for issue in result['issues']:
                        print(f"     - {issue}")

        # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        if summary['total'] > 0:
            summary['avg_score'] = sum(r['total_score'] for r in results) / summary['total']

        print("\n" + "="*60)
        print("ğŸ“Š å“è³ªãƒã‚§ãƒƒã‚¯ã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"ç·ä»¶æ•°: {summary['total']}ä»¶")
        print(f"âœ… PASS (80ç‚¹ä»¥ä¸Š): {summary['pass']}ä»¶ ({summary['pass']/summary['total']*100:.1f}%)")
        print(f"âš ï¸  WARN (60-79ç‚¹): {summary['warn']}ä»¶ ({summary['warn']/summary['total']*100:.1f}%)")
        print(f"âŒ FAIL (60ç‚¹æœªæº€): {summary['fail']}ä»¶ ({summary['fail']/summary['total']*100:.1f}%)")
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {summary['avg_score']:.1f}ç‚¹")
        print("="*60 + "\n")

        return results, summary

    def generate_quality_report(self, results: List[Dict], summary: Dict) -> str:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆMarkdownç”Ÿæˆ"""
        md = "# ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£å“è³ªãƒ¬ãƒãƒ¼ãƒˆ\n\n"

        from datetime import datetime
        md += f"**ä½œæˆæ—¥**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"

        md += "## ã‚µãƒãƒªãƒ¼\n\n"
        md += "| æŒ‡æ¨™ | å®Ÿç¸¾ |\n"
        md += "|------|:----:|\n"
        md += f"| ç·ä»¶æ•° | {summary['total']}ä»¶ |\n"
        md += f"| âœ… PASS (80ç‚¹ä»¥ä¸Š) | {summary['pass']}ä»¶ ({summary['pass']/summary['total']*100:.1f}%) |\n"
        md += f"| âš ï¸  WARN (60-79ç‚¹) | {summary['warn']}ä»¶ ({summary['warn']/summary['total']*100:.1f}%) |\n"
        md += f"| âŒ FAIL (60ç‚¹æœªæº€) | {summary['fail']}ä»¶ ({summary['fail']/summary['total']*100:.1f}%) |\n"
        md += f"| å¹³å‡ã‚¹ã‚³ã‚¢ | {summary['avg_score']:.1f}ç‚¹ |\n\n"

        # FAIL/WARNãƒªã‚¹ãƒˆ
        md += "## è¦æ”¹å–„ãƒ•ã‚¡ã‚¤ãƒ«\n\n"

        fail_files = [r for r in results if r['grade'] == 'FAIL']
        if fail_files:
            md += "### âŒ FAILï¼ˆ60ç‚¹æœªæº€ï¼‰\n\n"
            md += "| ãƒ•ã‚¡ã‚¤ãƒ« | ã‚¹ã‚³ã‚¢ | å•é¡Œç‚¹ |\n"
            md += "|---------|:------:|-------|\n"
            for r in sorted(fail_files, key=lambda x: x['total_score']):
                issues = ', '.join(r['issues']) if r['issues'] else 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'
                md += f"| {r['file']} | {r['total_score']}ç‚¹ | {issues} |\n"
            md += "\n"

        warn_files = [r for r in results if r['grade'] == 'WARN']
        if warn_files:
            md += "### âš ï¸  WARNï¼ˆ60-79ç‚¹ï¼‰\n\n"
            md += "| ãƒ•ã‚¡ã‚¤ãƒ« | ã‚¹ã‚³ã‚¢ | æ”¹å–„æ¨å¥¨ |\n"
            md += "|---------|:------:|----------|\n"
            for r in sorted(warn_files, key=lambda x: x['total_score']):
                # ä½ã‚¹ã‚³ã‚¢é …ç›®ã‚’ç‰¹å®š
                low_scores = [k for k, v in r['scores'].items() if v < 10]
                improvements = ', '.join(low_scores) if low_scores else 'è»½å¾®æ”¹å–„'
                md += f"| {r['file']} | {r['total_score']}ç‚¹ | {improvements} |\n"
            md += "\n"

        # é«˜å“è³ªãƒ•ã‚¡ã‚¤ãƒ«
        pass_files = [r for r in results if r['grade'] == 'PASS']
        if pass_files:
            md += "## âœ… é«˜å“è³ªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ80ç‚¹ä»¥ä¸Šï¼‰\n\n"
            md += "| ãƒ•ã‚¡ã‚¤ãƒ« | ã‚¹ã‚³ã‚¢ |\n"
            md += "|---------|:------:|\n"
            for r in sorted(pass_files, key=lambda x: x['total_score'], reverse=True)[:20]:
                md += f"| {r['file']} | {r['total_score']}ç‚¹ |\n"
            md += "\n"

        return md


if __name__ == "__main__":
    # å®Ÿè¡Œ
    base_path = "/Users/yuichi/AIPM/aipm_v0/Stock/programs/å‰µæ¥­æ”¯æ´ãƒ»æ–°è¦äº‹æ¥­é–‹ç™ºï¼ˆAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰/projects/Founder_Research"

    checker = QualityChecker(base_path)
    results, summary = checker.check_all_files()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_md = checker.generate_quality_report(results, summary)

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report_path = Path(base_path) / "quality_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_md)

    print(f"ğŸ“„ å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
