---
title: "You can also find multi key management is a challenge but like I told you with roundroin example you..."
video_id: "ZpC1FZ2yAFA"
video_url: "https://www.youtube.com/watch?v=ZpC1FZ2yAFA"
speaker: "Unknown"
channel: "Unknown"
date: ""
duration: ""
tags:
  - "AI"
  - "LLM"
  - "OpenAI"
topics:
  - "LLM Development"
  - "Prompt Engineering"
  - "Tool Integration"
summary: |
  You can also find multi key management
  is a challenge but like I told you with
  roundroin example you can stay within
key_points:
  - "You can also find multi key management"
  - "is a challenge but like I told you with"
  - "roundroin example you can stay within"
  - "you can use them or you can go across"
  - "model to respond then you can set it up"
  - "keyword based routing which is kind of"
  - "highlight is a query based routing. So"
  - "by open AAI 68% of the queries are"
category: "AI & Technology"
confidence_level: "high"
---

# Transcript: ZpC1FZ2yAFA

- URL: https://www.youtube.com/watch?v=ZpC1FZ2yAFA
- Retrieved at: 2025-12-30T11:26:09+09:00

## Text

- [00:04] You can also find multi key management
- [00:07] is a challenge but like I told you with
- [00:08] roundroin example you can stay within
- [00:12] your allowed tier limits by adding
- [00:14] multiple providers from same uh multiple
- [00:16] keys from same provider say openai and
- [00:19] uh add them as two different providers
- [00:21] and same LLM but with roundroin routing
- [00:24] you can use them or you can go across
- [00:26] multi-provider right some from geminy
- [00:29] some from Azour and so on. If like I
- [00:33] mentioned if you get spikes certain time
- [00:35] of the day between 10 and 1 and if you
- [00:38] want during that time your faster model
- [00:41] to respond and then other times of the
- [00:43] day you want your slower but reasoning
- [00:46] model to respond then you can set it up
- [00:48] with time based routing. We also have
- [00:50] keyword based routing which is kind of
- [00:52] pretty common and uh basic use case but
- [00:56] the most interesting one I want to
- [00:57] highlight is a query based routing. So
- [00:59] 68% of the queries as per the study done
- [01:01] by open AAI 68% of the queries are
- [01:04] simpleton simplistic queries like what
- [01:06] is the capital of this or uh who was
- [01:08] prime minister of that country and so on
- [01:10] right so these are called simplistic
- [01:12] queries they don't need a lot of
- [01:13] reasoning okay versus solving a problem
- [01:17] uh with lot of complexity with per say
- [01:20] jet uh engine air flow equations right
- [01:23] that will require some reasoning
- [01:24] capabilities so based on your query if
- [01:26] it's a simple query send it to a simple
- [01:28] LM if it is a complex query send it to a
- [01:32] more complex reasoning LLM that can
- [01:34] handle those. This will not only reduce
- [01:36] your latencies but also reduce your cost
- [01:39] and improve the performance. If 68% of
- [01:42] your queries and again I don't know
- [01:43] what's your use case particularly but if
- [01:45] many of majority of your queries are
- [01:47] simplistic you might want to use simpler
- [01:49] smaller language models right it is your
- [01:52] choice you don't have to use the hammer
- [01:55] uh for every single job you can choose
- [01:58] your tools wisely and those are the
- [02:00] choices you have with different routing
- [02:02] options now you'll ask me hey can I do
- [02:04] query based routing along with timebased
- [02:06] routing yes there are those options as
- [02:08] well querybased plus fallback or
- [02:10] fallback track with timebased routing.
- [02:11] Those are different options that you'll
- [02:13] have. So you can combine them for your
- [02:16] different use cases as well.
- [02:19] Let's talk about the single pane of
- [02:21] glass for builders, right? So you want
- [02:23] if you run an application, right? That
- [02:25] same application similar to the one I
- [02:27] showed you, right? You make a call and
- [02:28] you get the response back. You want to
- [02:30] track that request. What happened to
- [02:32] every single request that was made to an
- [02:34] LLM? You want to ideally see the uh open
- [02:38] telemetry logs and your waterfall
- [02:40] visualization so can so that you can
- [02:43] actually see what is taking longer time
- [02:45] right and then have a performance
- [02:47] analytics dashboard comprehensive
- [02:49] dashboard I'm going to show you uh that
- [02:51] dashboard as well that not only lists
- [02:53] performance of each model but what are
- [02:55] your costs what are your savings right
- [02:56] and so on so this is what the waterfall
- [02:59] looks like for a any particular query
- [03:02] this is just it shows you that there are
- [03:03] 10 spans in this particular request ID
- [03:06] the total response time was 7.47 seconds
- [03:09] and then prompt was rendered in less
- [03:10] than uh one microcond and then majority
- [03:14] of the call was to the provider. It took
- [03:17] 7.46 for six seconds and then we
- [03:20] actually added the guardrails but
- [03:21] guardrails did not take much long time
- [03:24] and then we got the response back Right.
