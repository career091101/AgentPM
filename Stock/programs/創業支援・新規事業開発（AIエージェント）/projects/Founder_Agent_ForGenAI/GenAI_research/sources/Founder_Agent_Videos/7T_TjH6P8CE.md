---
title: "YouTube Video: 7T_TjH6P8CE"
video_id: "7T_TjH6P8CE"
video_url: "https://www.youtube.com/watch?v=7T_TjH6P8CE"
speaker: "Tim Hwang"
channel: "Unknown"
date: ""
duration: ""
tags:
  - "YouTube"
  - "Transcript"
  - "AI Agent"
  - "Startup"
  - "LLM"
topics:
  - "AI Agent"
  - "Startup"
  - "LLM"
summary: |
  It was interesting to me that a couple of people reported that it's still hallucinating and it still really likes to give answers rather than say that it doesn't know the answers, although it's not so...
key_points:
  - "minds of technology to distill down what's in important in"
  - "for the very first time is Marve Univar, Director, Agentic"
  - "attack using Claude. But first we've got Eily with the"
  - "see what our experts think of Google's Gemini 3.0. First,"
category: "General"
confidence_level: "medium"
transcript_type: "YouTube Auto-generated"
language: "en-ja-mixed"
source: "Founder_Agent_Videos"
---


# Transcript: 7T_TjH6P8CE

- URL: https://www.youtube.com/watch?v=7T_TjH6P8CE
- Retrieved at: 2025-12-30T09:45:00+09:00

## Text

- [00:01] It was interesting to me that a couple of people
- [00:04] reported that it's still hallucinating and it still really likes
- [00:07] to give answers rather than say that it doesn't know
- [00:10] the answers, although it's not so psychopathic about it. But
- [00:13] it still really likes to give answers. So it's an
- [00:15] interesting combo. All that and more on today's Mixture of
- [00:19] Experts. I'm Tim Hwang and welcome to Mixture of Experts.
- [00:27] Each week MOE brings together a panel of the finest
- [00:30] minds of technology to distill down what's in important in
- [00:32] the latest news in artificial intelligence. Joining us today are
- [00:36] three incredible panelists. We've got Marina Danielewski, Senior Research scientist,
- [00:39] Gabe Goodhart, Chief Architect, AI Open Innovation. And joining us
- [00:43] for the very first time is Marve Univar, Director, Agentic
- [00:46] Middleware and Applications Research, AI. All right, lots of interesting
- [00:51] topics for today's episode. We're going to talk a little
- [00:53] bit about, of course, the drop of Gemini 3, some
- [01:03] attack using Claude. But first we've got Eily with the
- [01:06] news. Hi, I'm Illy McConnell, a tech news writer for
- [01:14] IBM Think. Here are this week's AI headlines. Microsoft and
- [01:18] chipmaker Nvidia will partner with AI startup Anthropic in a
- [01:22] $15 billion AI infrastructure deal. Carnegie Mellon researchers discovered that
- [01:27] AI agents fail a shocking 70 to of the time
- [01:32] on corporate real world tasks. IBM and the Ultimate Fighting
- [01:36] Championship have launched an AI driven live alert platform that
- [01:40] delivers real time records and milestones during UFC events to
- [01:44] viewers. OpenAI announced ChatGPT for teachers, a version of its
- [01:49] popular chatbot for K12 educators. For more subscribe to the
- [01:54] Think newsletter linked in the show notes. And now let's
- [01:56] see what our experts think of Google's Gemini 3.0. First,
- [02:04] I want to start with the big news of the
- [02:06] week which is the launch of Gemini 3. So long
- [02:11] rumored, long teased, but finally out. And it's a remarkable
- [02:16] model. I mean from some of the benchmarks Google is
- [02:19] reporting explosively good performance on. I think what has been
- [02:22] considered some of the most difficult evals and benchmarks that
- [02:25] are out there. So huge leaps on humanities last exam,
- [02:28] really big jumps on arc AGI. But I guess maybe
- [02:32] let's just kind of start with like the vibe check
- [02:35] I guess. Marina, have you had a chance to play
- [02:36] with the model yet? I'm curious about what you think
- [02:38] about it and if it feels like substantively very different
- [02:41] from what came before I. Haven'T played with it. I've
- [02:43] looked at a few digests about it. It does seem
- [02:46] like there's a lot of interest in making the more
- [02:49] complicated benchmarks be something that's handleable. It was interesting to
- [02:53] me that a couple of people reported that it's still
- [02:56] hallucinating and it still really likes to give answ rather
- [02:59] than say that it doesn't know the answers. Although it's
- [03:02] not so psychophantic about it. But it still really likes
- [03:05] to give answers. Right. It's still making mistakes. Yeah. It
- [03:09] doesn't like to admit that it doesn't know something and
- [03:11] maybe that's saying something about this new set of models.
- [03:14] Yeah, for sure. And Gabe, quick question. Would you recommend
- [03:18] Gemini 3? Have you played around with it yet? Yeah,
- [03:21] I've played just a little bit with it this morning
- [03:23] and I don't know. I think my take on this
- [03:26] is we're really starting to see sort of the ecosystem
- [03:30] moats evolving. Like I think this is a necessary step
- [03:34] for Google in their AI ecosystem to have a model
- [03:38] that is at par or better than all of their
- [03:40] competitors so that they can truly claim to be running
- [03:44] ahead in the front of offerings here. But what really
- [03:47] struck me about the announcement was that they actually took
- [03:50] a swing at a piece of differentiation because frankly a
- [03:53] really great model is not that differentiated anymore. It's like
- [03:56] that line but. But for what? I want to use
- [03:59] a model for what most people want to use a
- [04:01] model for. We don't need something better than what we've
- [04:03] already got. I thought the thing about the Anti Gravity
- [04:08] editing platform was really interesting because it actually looked like
- [04:12] something novel that they're adding to their ecosystem that you
- [04:17] can't get anywhere else. And in particular, you know, the
- [04:20] idea of an agentic IDE is not at all new.
- [04:23] There are well known startups out there doing that. There
- [04:26] are open source ways to pull that together on your
- [04:29] own. The part that I think was novel here was
- [04:32] the intentional transition to framing it as a management of
- [04:36] agents problem and tying this all back to would I
- [04:40] recommend the model itself? I haven't played with this capability,
- [04:44] but a colleague of mine has already. And the idea
- [04:47] of being able to launch a fleet of delegate worker
- [04:51] agents that can all work on separate tasks in parallel
- [04:55] and you can manage them is something that I think
- [04:58] has some real compelling chops. So if the model can
- [05:02] actually hold up to that level of independence and you
- [05:05] know, parallel analysis, it could be a real breakthrough there
- [05:09] on a net new capability that you couldn't get with
- [05:12] any other ecosystem. So I'm really excited to try that
- [05:15] out and to see where it goes. But from a
- [05:17] pure model perspective, I'll play with it. Yeah, for sure.
- [05:22] Yeah, I think that's kind of the one. The really
- [05:23] interesting things coming out of all this is like, you
- [05:26] know, I think we used to marvel even earlier this
- [05:28] year. We're like oh my God, new model, incredible benchmarks,
- [05:31] like look at all this progress. But here we are,
- [05:33] you know, sitting in November of 2025 and we're like
- [05:36] eh, it's the benchmarks, whatever, awesome. The science is amazing
- [05:41] and I can accomplish the same set of tasks that
- [05:45] I had before with a much, much smaller model running
- [05:49] on my laptop. So Marva, do you want to talk
- [05:51] a little bit about antigravity? I did think that this
- [05:53] was sort of like the big interesting differentiator on the
- [05:56] announcement was to say, hey, you know, we acquired Windsurf,
- [05:59] we're going to do kind of our ide and it's
- [06:02] going to be an agentic ide. What's your take? I
- [06:05] mean, where is this all going? And I guess if
- [06:07] you want to give our listeners an intuition of like
- [06:08] why Google is even investing in that kind of differentiation,
- [06:12] I think Gabe alluded. A little bit to it, right.
- [06:15] The ecosystem play. But I think this, I haven't played
- [06:18] with antigravity. I play with the model and I'll tell
- [06:21] or share my experience. But I think they're aiming for
- [06:25] the advanced tool use so the whole agentic applications and
- [06:29] making tool calls more robust and also increase the modalities.
- [06:33] Right. Like they're claiming you can do editor, terminal, web
- [06:37] browser and like many different execution modes. And this means
- [06:41] you can plan code, execute, verify different tasks more autonomously.
- [06:46] And I think agents in anti gravity will also generate
- [06:49] artifacts. That's what they claim. So this can task lists,
- [06:53] plans, screenshots, I think browser recordings and they can also
- [07:03] when you want to take agents to next stage beyond
- [07:06] benchmarks, from academic benchmarks to put it out there to
- [07:09] reality. So it's quite promising. But I did play with
- [07:13] the model, I didn't play with the antigravity platform yet.
- [07:16] So is Marina said it's not hallucinating fully, but just
- [07:20] like other big models, it's really, really good at with
- [07:23] the initial prompt, like the way you describe your first
- [07:25] set of things. And they have a build section. Right.
- [07:28] Like you can build artifacts, you can build UI elements.
- [07:31] So I asked Gemini to create me a workout plan
- [07:33] and an interactive dashboard to track my workout sessions. And
- [07:37] I told my weight, height, age to customize for myself.
- [07:40] And the very first UI that was really nice and
- [07:44] literally like I was multitasking in a meeting and it
- [07:46] like locally I was able to build in streamlit in
- [07:49] less than two minutes. Then I asked it to add
- [07:51] some more, you know, personalized pictures, like motivation pictures, customize
- [07:55] with my name. And then I realized it added a
- [07:57] reminder section saying that I should eat high nutrition food
- [08:01] after the workout to grow. To grow. I'm a mother
- [08:06] with two children. If this was for my kids, I
- [08:08] think it would make more sense. But for me, I'm
- [08:11] not growing. It totally messed that part up. It already
- [08:14] knew my age, so it was way past, I'm way
- [08:16] past growing age. So again, and the overall performance I
- [08:19] think on benchmarks are quite impressive and the claims they
- [08:23] are making. And I think this excited me because I
- [08:26] think this is the largest capability jump we've seen in
- [08:28] a few months. Right. Like, it's nice, but it has
- [08:31] some flows which I personally experienced in my first UI
- [08:35] dashboard that I built with. It when Marina, hopefully you
- [08:37] can help me kind of square this circle here because
- [08:40] I think it's really interesting that your first reaction to
- [08:43] this model is still hallucinating. And we kind of have
- [08:47] this very funny sort of split screen experience of these
- [08:50] models where they are just kind of performing better and
- [08:53] better against these benchmarks. But yet our kind of everyday
- [09:03] that? Is it true that the more powerful models just
- [09:07] simply hallucinate more or is this kind of just like.
- [09:09] I'm just kind of curious about. They're seemingly really strong
- [09:12] in some things, but remain kind of like amazingly weak
- [09:15] in other domains. Right. So I'm going to agree with
- [09:17] what Gabe said, which is the whole point of, well,
- [09:19] plenty of tasks. I don't need this really large model
- [09:21] and I can do a better thing with a smaller
- [09:23] model because hopefully we're trying to finally getting beyond this
- [09:27] idea that we're going to have one model to rule
- [09:30] them all. This never should have been a goal. It's
- [09:32] never going to work for an LLM with the architecture
- [09:34] that we have. What you want is a suite. Either
- [09:37] you give them different instructions or different preferences or whatever.
- [09:41] If you wanted to do better on the these very
- [09:43] complicated benchmarks which really want the model to think through
- [09:47] a lot of Things generate a lot of thoughts and
- [09:49] attempts and whatever. Then you don't want it to be
- [09:51] reticent and sit there saying, I don't know anything. I'm
- [09:54] going to twiddle my thumbs because I don't have a
- [09:55] citation to offer you. These are different tasks. This is
- [09:59] the same thing as if you were to have the
- [10:01] statistical tension between precision and recall. You're going to do
- [10:04] better in one, you're going to do worse than the
- [10:06] other. This is going to be a consistent thing. Use
- [10:08] them for different things. And the fact that the more
- [10:12] interesting thing now is automation, the more interesting thing now
- [10:14] is really the multimodality. Yeah. Lean into that because that
- [10:18] really is more interesting. Having one model is always going
- [10:20] to do the best job at giving you the right
- [10:21] information. Why? Review your Karl Marx, review your division of
- [10:25] labor. Let's reinvent normal civilization of people working together. That's
- [10:32] going to be more effective. It always will be. Yeah,
- [10:35] yeah. And I think it's kind of funny where this
- [10:37] is all resolving too, because I agree, one of the
- [10:40] things that was like, oh man, this is going to
- [10:42] change everything was one model to rule them all. But
- [10:45] if we end up in a world where, I don't
- [10:47] know, we have very specialized agents for very specialized kinds
- [10:50] of tasks, are we back to Appland again? Are we
- [10:53] back to software again? In some ways we're kind of
- [10:56] reconstructing applications like specialized software again, which I guess is
- [11:02] like everything old is new again in some sense. I
- [11:05] think it's a healthy tension. I mean, I think frankly,
- [11:07] one of the reasons we're in the AI moment. We
- [11:09] are, is that the pendulum really swung with the introduction
- [11:13] of Transformers and suddenly you didn't need a complicated suite
- [11:16] of Software to get 80% of the solution. And that
- [11:20] was a real game changer. Right. I think what we're
- [11:22] seeing here is there's. I mean, the general purpose populace
- [11:26] is still going to use one chat window. Right. They're
- [11:29] going to jump to a chat window and they're going
- [11:31] to enter some things. Now if that chat window becomes
- [11:35] an increasingly complicated software machine behind the hood, the user
- [11:39] doesn't need to know. So the interface change of one
- [11:42] model to rule them all I think is sticky. I
- [11:44] don't think that's going anywhere but the actual implementation behind
- [11:47] the scenes. I think we've already seen that with the
- [11:51] GPT 5 series. I think we will almost certainly see
- [11:55] it with other Frontier model offerings, let's call them that,
- [12:01] and I think we'll see the open equivalent of a
- [12:05] software Stack emerging that allows you to ensemble models for
- [12:09] specific parts of your workflow and specific elements of how
- [12:12] you want this all to work together, exposing that nice
- [12:15] single entry point that users want to interact with. So
- [12:19] I think it's a healthy tension. The nice thing here
- [12:22] as a software architect you get an abstract interface which
- [12:25] is your chat box and then you get to implement
- [12:27] it however you want. So we'll iterate on that implementation
- [12:31] because we're software folks and we'd like to do that.
- [12:33] But I think we'll swing back and forth a little
- [12:38] bit on the complexity behind the scenes. Yeah, for sure.
- [12:40] I think it'll be so funny if we end up
- [12:41] in a few years people being like, well rather than
- [12:43] a chat window, what if we had like a desktop
- [12:45] with like icons you can click on and it's just
- [12:47] like we'll be back to where we were. So. Yep.
- [12:54] Two very interesting announcements out of IBM recently about agents
- [13:02] life really the last few months and I think particularly
- [13:06] on Gabe's last comment. I understand one of the announcements
- [13:09] is around a project called Kuga C U G A
- [13:12] which is billed as an enterprise ready generalist agent. So
- [13:17] do you want to talk a little bit about kind
- [13:18] of like what the team was working on and thinking
- [13:20] about for this, for this launch? Sure. Happy to. As
- [13:22] you said, this has been my life since the launch
- [13:25] and it's been quite. I think we got good feedback
- [13:27] as well. We're trying to become enterprise ready general estate
- [13:31] and it's not an easy, easy task to take on.
- [13:34] But where we started is like everybody else who starts
- [13:38] to build enterprise ready agents, you start from some simple
- [13:42] traditional ways, build a domain specific agent. So maybe React
- [13:46] Kodak the simple pattern that you take and then you
- [13:49] start evolving it to oh, my task is too complex
- [13:52] and my single agent cannot handle this. So let me
- [13:55] go and build a task decomposer on top. Oh, this
- [13:58] is now becoming this multi agent architecture where you have
- [14:01] the layer up top that, that picks the right sub
- [14:04] agent to do. It's a, I mean classical engineering design
- [14:07] principles because it's easier to distribute to the sub agents.
- [14:10] We believe it's going to work faster. And then what
- [14:12] we realized is we're not the only one that does
- [14:14] this. Like my peer groups in IBM Research, when they
- [14:17] build sophisticated agents they go through this experience as well.
- [14:20] Let's start simple and then it all of a sudden
- [14:22] becomes this very complex. Exactly. And then we stepped back
- [14:27] and we thought like maybe we can create a generalized
- [14:30] version of this where people can jumpstart with using KUG
- [14:35] architecture rather than building all these things by themselves. Right.
- [14:38] So we can give Kuga, which is this multi agent
- [14:41] supervisory layer already embedded and a multi agent architecture and
- [14:46] people can configure it for their own domain and users.
- [14:50] So Radiodon is a traditional way like build domain specific
- [14:53] agent, evolve it, do some custom benchmarks and a long
- [15:04] bring your own domain onboard your own tools and configure
- [15:07] your own domain to do your own benchmarks and then
- [15:10] deploy. So that's our vision. It's open, it's outside in
- [15:14] the open now for people to try and give us
- [15:17] feedback and see if it works for their domain. So
- [15:19] we're very excited that we launched in the open so
- [15:21] we can capture if what we experimented in research actually
- [15:26] can be mimicked in the real world application uses. Yeah,
- [15:30] that's really exciting. And I think one of the things
- [15:32] that we've been watching really closely here at MOE is
- [15:35] what I love about the kind of agent competition world
- [15:37] is right now we're very much in the world of
- [15:40] norm setting. We're doing it this way. We hope you
- [15:43] do it this way as well. And I think there's
- [15:46] various projects that are more or less successful at attempting
- [15:50] to build those standards. I think what's really intriguing, and
- [15:52] Gabe, I'm curious if you want to talk about this
- [15:54] in the context of Kuga, is it seems here what's
- [15:57] really intriguing, Marvi, I'm hearing you right, is that everybody
- [16:00] starts by building an agent and they all discover exactly
- [16:04] the same problems over and over again. And everybody's going
- [16:07] through that process of rediscovery right now. I guess. Gabe,
- [16:09] that's pretty promising from the point of view of. Okay,
- [16:12] let me just shortcut this. Here's a standard framework. Yeah,
- [16:16] I've been also thinking a lot about this, having a
- [16:19] lot of conversations with different teams building different components. And
- [16:23] one of the things that really seems to be true
- [16:26] is that there are emerging slots for an abstract architecture
- [16:30] for agents in open source and presumably in closed source.
- [16:34] But we don't know how those tools are implemented necessarily.
- [16:37] The generalist agent is absolutely one of those slots. And
- [16:40] having an open offering that's configurable and permissively licensed is
- [16:45] a really awesome place for people to start collaborating and
- [16:47] building. On top of this tool management is another big
- [16:51] piece of this and it just seems to be coming
- [16:55] up over and over again that this sort of emergent
- [16:57] architecture is there and to the point about refining and
- [17:01] iterating on the actual agentic architecture itself. The analogy that
- [17:05] I keep coming to when I talk about this stuff
- [17:08] is if I asked anybody out there working at a,
- [17:12] well, really any company, please go build me a REST
- [17:15] API server for X, Y and Z. I wouldn't have
- [17:19] to tell them the architecture of that thing. I wouldn't
- [17:22] have to tell them what programming language to use. I
- [17:23] wouldn't have to tell them like it's just kind of
- [17:26] a well established pattern that everybody knows how to do
- [17:29] if they've ever touched cloud software. Agents aren't there yet,
- [17:32] but as many people have said, you know, 2020, the
- [17:35] year of the agent. I think by the end of
- [17:37] 2025 we're going to be close to actually hitting that
- [17:40] point where we can just say, hey, build an agent
- [17:42] for this. And everybody just knows what you mean. And
- [17:44] as you exactly described it, Merve, I think the decomposition
- [17:50] is exactly that step from I got it running in
- [17:53] flask on my local machine with HTTP to now I've
- [17:57] got a server that has middleware for authentication and serves
- [18:01] TLS and can actually be horizontally replicated. Those are the
- [18:06] steps you take when you're building, building a microservice after
- [18:08] you get your demo app running. The same thing you're
- [18:10] describing with Kuga is exactly what people are hitting after
- [18:14] they get their first react agent off the ground. You
- [18:16] mentioned like, oh, here is the agent, go use it.
- [18:18] We're really trying not to put people to like, okay,
- [18:23] this is Kuga is this and you have to use
- [18:25] it. It's also flexible. The configuration piece makes it, I
- [18:28] think easier for people to okay, I need this, but
- [18:32] I can configure it this way. And also what we
- [18:34] did is, which is I think maybe it's a good
- [18:37] time to introduce the altk, which is the Agent Lifecycle
- [18:40] development toolkit that we also released in the open. We
- [18:43] componentize Kuga and we build different components to support Kuga's
- [18:47] different capabilities like memory guardrails and other things that makes
- [18:52] Kuga function in the real world. But some people may
- [18:55] not want to start from Kuga. They may still have
- [18:57] their own sophisticated agent that they built and they don't
- [19:00] want to move it to Kuga. So they can reuse
- [19:02] these components under this Agent Lifecycle toolkit and if they
- [19:06] want the memory piece and they can take it and
- [19:08] apply to their agent and this is Again, like democratizing
- [19:12] and not really pushing people to use. This is what
- [19:14] I have. This is it works and use it. No,
- [19:17] there's flexible design. You can take the different components and
- [19:20] apply to your current agentic implementation if you want to
- [19:23] improve certain aspects of your agent. Marina, I want to
- [19:25] go back to kind of the comment that you made
- [19:27] a little bit earlier when we were talking about Gemini
- [19:30] 3 and kind of this movement to sort of like
- [19:32] more specialized agents. Over time it kind of strikes me
- [19:35] that we will almost reproduce human org structures in agents
- [19:39] because it's generally this agent is kind of like it's
- [19:41] the middle manager. Right. Its role is to manage other
- [19:45] agents. Do you think that's kind of where we're headed
- [19:47] ultimately is like we're moving away from one agent to
- [19:50] rule them all. But there will still be these kind
- [19:52] of generalist agents and really their role will be sort
- [19:55] of that middle manager, I guess, in the org chart
- [20:04] biology to software, you have this combination of hubs and.
- [20:08] And spokes. There's a real reason that you end up
- [20:10] settling. And maybe it's going to be a different number
- [20:12] of spokes, more hubs, fewer hubs. But sort of like
- [20:15] as you figure out the way to solve a particular
- [20:16] problem, that's still the place that you solve. You need
- [20:19] some specialists and you need somebody doing the managing and
- [20:22] the planning. So yeah, what's interesting about this era is
- [20:28] that we are able to go faster, further than we
- [20:32] thought. But if you take that 10,000 foot view, it's
- [20:34] still a. All right, I've got a task I maybe
- [20:37] can do. I'll think the spokes part a lot faster.
- [20:40] But you still somewhere in there need a hub where
- [20:42] you say, okay, this is what you do next. This
- [20:44] is how you know that you're done. This is what
- [20:45] you try. It's very natural and very correct. The technology
- [20:49] to get us to go faster is great and it's
- [20:52] very exciting. But yeah, this is the normal pattern of
- [20:55] problem solving. Yeah. Like the future is kind of like
- [20:58] figuring out how you staff your project with different kinds
- [21:00] of agents. It almost feels like. And maybe a couple
- [21:04] of people in there just to keep an eye on.
- [21:06] There's some actual humans in there. So I guess maybe
- [21:11] a last point, Marv, where are you headed next with
- [21:13] all this? I know you had said this is your
- [21:15] life since the launch, but where does Kuga, where does
- [21:18] Altk go next? So just like Gemini 3 benchmark results.
- [21:22] So we started with Kuga and then we went out
- [21:24] there and found the most challenging and most representative benchmark
- [21:27] that we can go after, which was WebArena and AppWorld.
- [21:31] So we were number one on both of them for
- [21:33] a long time. And we kept like, oh, let's keep
- [21:35] our position as number one. But no, I think it's
- [21:38] very different to have of keep our position in benchmarks
- [21:41] as number one versus putting it out there and hearing
- [21:44] directly from the users where it breaks. How latency is
- [21:47] for example, a problem right now. Apparently like when we
- [21:49] built Kuga, we really focused on the accuracy and how
- [21:53] good Kuga behaves like listening the task. But latency is
- [21:57] one of the requirements. For example, for us, that came
- [21:59] from the real users when we launched outside that said
- [22:02] like, okay, this is too slow for me to use.
- [22:05] So we have a bunch of things that we capture
- [22:07] from the community that we would like to incorporate. But
- [22:10] also when I mentioned like the altk, which is the
- [22:13] core components, that helps, I think agents or agent builders
- [22:17] boost their agent performance. There are a couple things that
- [22:20] we're working actively on. One of them is the memory
- [22:23] that I mentioned. And I'm not talking about storage and
- [22:25] data structures. I'm talking about what can you make out
- [22:28] of this memory, like what you want to remember, what
- [22:30] you want to forget. And what is the middle ground
- [22:32] that you want to keep learning from. Because some tool
- [22:37] combinations may never work and then you already did it
- [22:39] and your trajectory is you have this like it's saved
- [22:42] in memory and can you bring it up and do
- [22:45] some self learning for Kuga or other agents? And the
- [22:48] other one is the consistency, like this is extremely important
- [22:51] and there is not a single definition of consistency in
- [22:53] the literature. When you look, people define maybe sometimes with
- [22:56] repeatability, but in the I think enterprise setting, especially also
- [23:00] in the consumer, setup is important. Like you don't want
- [23:03] your agent to do something in a way one day
- [23:05] and a very different way in another day. So how
- [23:08] can you bring this consistency to the real world agents,
- [23:11] therefore they are within their own world, consistent with their
- [23:17] behavior and they don't throw really ridiculous answers one day
- [23:21] when you ask the same question. So these are the
- [23:23] two main topics that we are working. I know. And
- [23:26] we're excited that we're making progress towards getting the real
- [23:29] feedback from the community and also advancing the capabilities of
- [23:33] Kuga with these components. That's great. Well, we'll have to
- [23:36] track the project. How do people find out more about
- [23:38] it if they want to keep up with Your work?
- [23:39] Sure. There's kuga.dev. this is the Kuga website where you
- [23:43] can go to GitHub and learn all the blog posts
- [23:45] and other things. And we have ALTK AI. So that's
- [23:48] the individual components that constitutes and helps KUGA perform better.
- [23:52] So if they go to these two websites, they're all
- [23:55] good. Nice. Yeah, those are the solid tlds right there.
- [24:03] Well, I'm going to move us on to our next
- [24:05] topic. This is kind of a recurring theme for us
- [24:08] in 2025 and I think it's kind of a sort
- [24:11] of interesting ongoing stuff set of discussions about how AI
- [24:14] will impact the economy. And I think overall, I would
- [24:18] say the discussion has matured, I think over the course
- [24:21] of the last 12 months. I think in the beginning
- [24:24] of the year we were still very much like all
- [24:26] the jobs are going to disappear because of AI. And
- [24:29] I think now we're getting more to the mode of
- [24:31] like, well, let's do an eval on that. We're approaching
- [24:34] it in a very machine learning way. So OpenAI announced
- [24:39] a benchmark that they call GDP Valley. And essentially what
- [24:43] they're trying to do is to say, well, we have
- [24:46] all sorts of benchmarks on trying to evaluate AI capabilities.
- [24:50] But one count against a lot of these benchmarks is
- [24:52] that they don't tend to be very realistic. Not frequently.
- [25:01] and solve complex math theory problems. What they do is
- [25:06] they basically curate a set of tasks from a number
- [25:08] of actual professions and they evaluate whether or not AI
- [25:12] is able to kind of produce outputs on par with
- [25:15] a human expert. And they run this as a way
- [25:19] of kind of trying to get an assessment of, well,
- [25:21] what are the effects of AI going to be on
- [25:23] the economy, particularly against these sort of economically valuable tasks.
- [25:28] And there's some interesting results. I think the big headline
- [25:30] is that even though it's a benchmark for OpenAI, they
- [25:34] discover that Claude Opus 4.1 is the strongest performer against
- [25:37] these tasks and in some cases are able to reach
- [25:40] sort of near parity with human experts. And so I
- [25:45] guess maybe, Gabe, maybe I'll turn it to you. I'm
- [25:47] curious what you read from these types of results. Are
- [25:50] we still kind of back where we are like December
- [25:52] 2024, which is like, oh God, AI is going to
- [25:54] replace all the jobs. These are certainly really impressive results.
- [25:58] But how do you parse through it? Yeah, I mean,
- [26:01] I think in professional settings the promise of AI has
- [26:05] been take Away the stuff I don't want to do
- [26:07] so I can spend more time on the stuff I
- [26:09] do want to do. I think it's really easy to
- [26:15] poke holes at benchmarks because measuring things is really difficult.
- [26:20] So I want to upfront say, like, this is a
- [26:22] really good stab at a new aspect of benchmarking. And
- [26:28] I think especially the reliance on human experts is important
- [26:33] in this space. The holes that I saw immediately looking
- [26:38] at this were they're still doing this as basically one
- [26:42] shot artifact creation as the, the benchmark, which I don't
- [26:46] know about you guys, but most of the time I
- [26:47] spend at my job doing things I don't want to
- [26:50] do is stuff that involves like investigation, asynchronous this, that
- [26:54] and the other thing. And in fact, when it comes
- [26:57] time to create artifacts, that's the stuff I do want
- [26:59] to do. Right? Writing code is my happy place, even
- [27:02] if I'm using an assistant for it. But what is
- [27:05] less happy is walking around trying to find the correct
- [27:10] way to implement a little corner edge on the Internet.
- [27:12] Or we're still trying to look through a giant pile
- [27:15] of corporate docs to figure out the right official approved
- [27:18] tool to do a certain piece of my job. So
- [27:21] I think every benchmark makes approximations of the space so
- [27:27] that it can actually translate from a fuzzy human space
- [27:31] into math. That's just the nature of benchmarking. And in
- [27:34] this case they've made some approximations that while valuable, still
- [27:38] have some, some holes in it. I think the other
- [27:40] part of it that I saw that was interesting was
- [27:44] that they actually used human graders, at least as the
- [27:48] gold standard for evaluating. Because, you know, even if you
- [27:52] put aside the fact that these are sort of canned
- [27:55] problems, you know, very well curated canned problems, but still
- [27:58] canned problems, it's very hard to say is the answer
- [28:02] here. You're right, because at the end of the day
- [28:05] you need a thumbs up or a thumbs down or
- [28:07] at best a, you know, value between 0.0 and 1.0
- [28:11] to say how well did this thing do? And that's
- [28:15] much harder the more complex the thing that it's trying
- [28:17] to do is. So using a human grader is in
- [28:21] one sense the right approach. But the whole reason we're
- [28:24] in this Genai boom is that we figured out ways
- [28:26] to not have humans labeling data. And this sounds like
- [28:30] we're back to humans evaluating data. So they also created
- [28:33] like a proxy to this with a another model that
- [28:36] could proxy what their human graders did. But now you've
- [28:39] got AI evaluating AI and you're kind of in a
- [28:41] recursive loop there. So I think it's really interesting to
- [28:47] see this try to tackle real world problems. The other
- [28:49] piece that they didn't really articulate very well was amongst
- [28:54] the classes of problems that a given profession has to
- [28:56] tackle, is this tackling the hardest ones or the simplest
- [28:59] ones? Right. Responding to email, responding to slack is not
- [29:02] the most mentally challenging thing I do but it's a
- [29:04] lot of what I do all day and I'm sure
- [29:08] the same is true for a lawyer or a doctor
- [29:10] or a nurse or anyone in a professional capacity spends
- [29:14] a lot of their time in the long tail of
- [29:18] less mentally taxing work. So I'd be curious if there's
- [29:23] any way that they sort of evaluate these tasks against
- [29:27] is this the low lift or is this the high
- [29:30] lift that they're trying to evaluate against Reyna, you're. Smiling
- [29:33] through Gabe's explication of gdp. Val, curious about your take.
- [29:37] What I'm hearing from Gabe is better but maybe not
- [29:40] good enough right at like I can think assessing what
- [29:43] is ultimately like a very complex thing like what is
- [29:45] a job, you know. So yeah so I really found
- [29:48] it interesting diving into this. First of all props to
- [29:51] whoever in their comms or marketing or whatever team thought
- [29:55] several months ahead of what the write up headlines were
- [29:57] going to be be because the headlines of AI can
- [30:01] now do half of our jobs. Great. It's not what
- [30:05] you're supposed to get from this but like fantastic how
- [30:07] they chose the jobs, the fact that they actually went
- [30:09] to the BLS types of jobs and how they went
- [30:14] about it. Very, very nice choice. So I like what
- [30:16] they're trying to do very much so if you actually
- [30:19] go and read through the data points which they made
- [30:22] available on Hugging Face very good with the transparency it's
- [30:25] mostly planning tasks, summarization tasks, you know the kind of
- [30:29] things that actually alums are pretty good at. And reading
- [30:32] between the lines it did seem to me that they
- [30:33] had a lot of different submissions and they sort of
- [30:35] narrowed them down, narrowed them down, narrowed them down until
- [30:37] they had a set of maybe similar kind of looking
- [30:41] tasks. Even though it was across a number of jobs
- [30:43] and it's we're still only talking about a couple hundred
- [30:46] tasks that really got made. So I think that that
- [30:50] is, that is first of all a point that it's
- [30:52] only going to be be so many and I completely
- [30:55] agree with Gabe. There's going to be some tasks that
- [30:56] are harder, some tasks that are easier. When you look
- [30:58] at the prompts themselves as I Read them. They looked
- [31:01] to me very detailed, very refined, somebody with a very
- [31:04] clear idea of what they want. So I think a
- [31:06] lot of sort of pre work already goes into. Before
- [31:09] you ask, hey, write me this summary, Write me this
- [31:11] schedule. I have prepared some files for you. I have
- [31:13] prepared some reference Excel, some sites that I need you
- [31:16] to go to. So there's a lot of sort of
- [31:18] pre planning and then this does the. All right, fine,
- [31:21] put it all together. Probably this still helps a good
- [31:24] amount. And it also, if anything, would give an example
- [31:27] to people who don't understand, what does it mean? What
- [31:29] kind of jobs can the AI be kind of decent
- [31:31] at, which is, look at these kind. Please read these
- [31:33] 200 and take a look at what is the kind
- [31:36] of thing that this is actually pretty good at. So
- [31:38] I like that part of it. Now, as far as
- [31:40] the valuation goes, look, it's pairwise comparisons. Is it the
- [31:43] most sophisticated, detailed thing you could dive into? No. If
- [31:47] I had to guess, having, you know, done evaluations for
- [31:49] years and years is they probably tried a variety of
- [31:51] other things and got a lot of noise because the
- [31:54] artifact that you get out of each of these is
- [31:56] going to be very detailed and very noisy and very
- [31:59] difficult even for a human to say. This part is
- [32:01] really better. This part is really not that much better.
- [32:04] No, it's going to be like this, like this, like
- [32:05] this. So they finally just ended up going with a
- [32:07] win rate. So I wouldn't really listen to the headlines.
- [32:11] I would look in interest at the paper, especially the
- [32:14] appendices, where they really go into the process of how
- [32:17] they did the evaluations. And again, what does it mean
- [32:19] with the automatic grader? Not the automatic grader. I'll add
- [32:22] one more thing. These are all prompts. So there is
- [32:27] not a lot of breaking it down. Exactly what Gabe
- [32:29] said, Exactly what we're just talking about with Merve of
- [32:32] first do this, then do this. Here's an agentic plan,
- [32:34] here's this, this, this, this, this kind of thing we
- [32:36] can do. This is a prompt and it primarily would
- [32:39] like the model to present something akin to maybe a
- [32:43] plan that perhaps you would then execute on downstream. So
- [32:46] again, this is a very particular type of task. It
- [32:48] may be interesting in the future to say actually, let's
- [32:51] have multiple models thrown at this with multiple capabilities, maybe
- [32:56] even specific models, et cetera. Now, is evaluation of that
- [32:59] going to be harder Exponentially, which is probably why their
- [33:04] path at this benchmark was the way that it was.
- [33:07] It was very intelligently done, very ready, ready for headlines,
- [33:11] ready for write ups ready for all of that. That.
- [33:13] But I do like the interest in real world tasks
- [33:17] and the lens that this can hopefully show on. When
- [33:20] we talk about AI taking our jobs, which jobs. I
- [33:23] wish there were more write ups that dove into the
- [33:25] actual data. I find that more interesting than the final
- [33:28] evaluation. Yeah, absolutely. That was a great analysis. Marvit, maybe
- [33:33] a final question for you. It strikes me that there's
- [33:37] really interesting tension in what Marina said, which is you
- [33:39] want the eval to measure real world tasks risks, but
- [33:42] it turns out the real world is really, really messy.
- [33:45] And so methodologically it's just very hard to do a
- [33:48] good eval here. It's kind of a part of me
- [33:50] that's like we're going to spend a lot of time
- [33:52] in the next few years trying to develop increasingly sophisticated
- [33:55] evaluations to measure the economic impact of AI. But also
- [33:59] the economic impact of AI is just going to kind
- [34:01] of happen to us as well. And I'm kind of
- [34:04] curious about what you think about the value of this
- [34:08] kind of exercise. There's almost a point of view which
- [34:10] is is we can spend a lot of time trying
- [34:11] to create better and better proxies, but we're also just
- [34:15] in the middle of it right now and I guess
- [34:17] the effect of AI on the economy we're just about
- [34:19] to see. And so I guess the kind of question
- [34:21] is, do you think these evals are more important with
- [34:24] time, less important with time as kind of a research
- [34:26] area? Where should we be going with this kind of
- [34:28] work? Well, I think just when we were discussing this,
- [34:33] it made me remember I had a client conversation. Another
- [34:36] client is a friend of mine last week in Istanbul.
- [34:38] He is one of the CEOs of one of the
- [34:40] most leading hospitals in Istanbul. And he told me that
- [34:44] it makes him very uncomfortable to see that each doctor
- [34:47] has a medical secretary attending patient visits. This is one
- [34:50] of those 44 occupations they include in the GTP. Well,
- [34:53] this is exactly the description of the they are ready
- [34:57] the people who wants to adopt LLMs because there are
- [35:00] certain tasks. This is a perfect use of LLM. We'll
- [35:03] listen to conversation and it will summarize. Summarization is a
- [35:08] very popular use case. And also if you just take
- [35:11] this task, fine, we can benchmark and evaluate. But there
- [35:14] is implication on other things. There are other added benefits.
- [35:18] Likely a bench would vary almost impossible to measure. If
- [35:23] you consider these other aspects of this particular example that
- [35:27] I give that you can drive insights from doctor patient
- [35:30] conversation. You can extract, for example, the best practices like
- [35:35] One doctor may be doing something that is working more
- [35:37] for specific certain diseases for subset of patients. You can
- [35:41] do better process optimizations like supply planning for a surgery
- [35:45] if you know what's happening in a conversation or customize
- [35:48] follow up contents. Like many many different other added benefits
- [35:50] you can try, but each of them it's so difficult
- [35:53] to measure. To Marina's point, even non human evaluations are
- [35:57] not that easy. Now we are adding this human component
- [36:00] and derivatives of these one One occupation that I gave
- [36:05] as an example, but it's real. I had this conversation
- [36:08] last week with a person who is looking to implement
- [36:11] this in their hospital. So now it comes to where
- [36:15] the real world is heading versus what do we benchmark?
- [36:19] How do we benchmark when you implement these systems in
- [36:21] the real? You can also track the edit benefits as
- [36:24] a roi, like how much better you are in your
- [36:27] supply management for your surgeries and so forth. So you
- [36:30] can have KPIs that can help you understand I guess
- [36:33] the added benefit. But on the paper, just like pure
- [36:36] scientifically looking and trying to mimic and understand what the
- [36:40] combination of things that these tasks can lead and then
- [36:43] try to benchmark against all of them with some data,
- [36:46] whether it's human annotated or AI annotated to me is
- [36:49] going to be extremely difficult and overwhelming exercise. But I
- [36:52] do like this because it's helping us from getting out
- [36:54] of our academic mindsets when we compare the models and
- [36:58] usages to really real world examples and where industries are
- [37:05] starting to adapt and change. Yeah, I love that. It's
- [37:07] kind of like almost like the eval is sort of
- [37:10] also just a useful exercise in terms of being like
- [37:12] how do we decompose this task anyways? Like what do
- [37:15] we do with our days, you know, is like an
- [37:18] important part of it. Great. I'm going to move us
- [37:23] on to our last topic of the day, which I
- [37:25] think is actually like now that I think of it
- [37:27] weirdly related to what we were just talking about. So
- [37:31] the final news is this really interesting story that was
- [37:34] actually tackled in more depth in the Security Intelligence podcast.
- [37:38] Chris Hay, a frequent MoE panelist, was on there debriefing
- [37:42] it. I'll give the summary of the story and then
- [37:45] I'd actually love to link it to specifically what we
- [37:47] were just talking about. Anthropic disclosed basically that they discovered
- [37:51] that an actor, which they believe to be a state
- [37:53] actor, was misusing Claude to launch a sophisticated cyber attack.
- [37:59] And they have this long blog post which is kind
- [38:01] of very interesting, breaking down what they discovered about the
- [38:03] Attack hack. But I think the thing that really kind
- [38:05] of stood out to me was this quote which I'll
- [38:07] just read, which is quote, the threat actor was able
- [38:10] to use AI to perform 80 to 90% of the
- [38:12] campaign with human intervention required only sporadically, perhaps four to
- [38:17] six critical decision points per hacking campaign. And as far
- [38:20] as I know, this is kind of the first real
- [38:22] dead to rights example of what people have been kind
- [38:24] of theorizing as like vibe hacking. Sort of the idea
- [38:28] that all of this agentix technology at some point point
- [38:30] gets used for good stuff, it also gets used for
- [38:33] bad stuff and I guess to link it to what
- [38:36] we. Were talking about earlier. Marvey, maybe I'll kick it
- [38:38] back to you kind of as our kind of agent
- [38:40] expert here. It does feel like if we had not
- [38:44] GDP val, but for evil tasks it really does seem
- [38:47] like AI is making a real impact in cybersecurity, cyber
- [38:51] attack operations right now. I guess the question to be
- [38:54] asked is do you think agents are going to favor
- [38:56] illegitimate cases faster than legitimate ones? Well, so even though
- [39:00] I work on agentic systems, I'm not a security expert,
- [39:03] but I did chat about this one of my co
- [39:05] workers, Ian Molloy, who is leading our agent security work
- [39:09] and his own words were like it will be extremely
- [39:11] difficult, maybe impossible to prevent malicious use of these agents
- [39:15] while preserving their legitimate use because it's designed that way.
- [39:19] Right? We want them to be flexible, we want them
- [39:21] to listen instructions, we want them to do what we
- [39:25] tell them to do. On the other hand, when we
- [39:27] tell the malicious things, like with these current alignment approaches,
- [39:30] I think it's going to be impossible. But what we
- [39:33] can do, I think security researchers have been predicting this
- [39:36] from the beginning of the LLM era and not even
- [39:38] before agents, because you can also do this model attacks
- [39:41] without the agents. But what I love is the anthropic
- [39:46] had I think the perfect telemetry and monitoring to be
- [39:50] able to talk about this and show what happened. And
- [39:53] I think we can instrument our systems with, with this
- [39:56] observability layers or some monitoring capabilities that is going to
- [40:02] be basically showing us transparently what is happening in the
- [40:05] system. And you can maybe revert back or talk about
- [40:09] it and learn about what happened. I think that's, I
- [40:12] think to me we can't control these things. We can
- [40:16] build additional components for security like authentication, other things and
- [40:20] maybe for enterprise settings that might be able to even
- [40:23] to me easier than broad use because you can add
- [40:26] some controls when you implement agents in the enterprise setting,
- [40:29] but in the Broad use, you just do what the
- [40:32] user says. But I think if we instrument the systems
- [40:36] with these components and which one of them could be,
- [40:41] as I mentioned, observability and the robust telemetry and monitoring
- [40:45] systems, that's what I think can bring trust to use
- [40:49] users while they're using these very powerful or small models,
- [40:54] whatever they do to feel comfortable and trustworthy for these
- [40:59] systems. Gabe, I was joking with a friend which is
- [41:02] that there's probably a team at OpenAI which is kind
- [41:05] of relieved that they weren't involved in this attack, but
- [41:07] also kind of jealous that they chose Claude vs OpenAI.
- [41:11] I guess I have a kind of interesting question here,
- [41:13] which is why, why not use open source here? It
- [41:17] feels like incredibly risky for an actor of this kind
- [41:20] to go with a model that's provided through the cloud
- [41:24] is monitored the way that CLAUDE is monitored, it's sort
- [41:28] of very interesting. Do you have a hypothesis on why
- [41:31] that's the case versus saying we're going to run our
- [41:33] own on premise solution, if you will, to run this
- [41:37] attack? My answer is that they probably are. This is
- [41:41] just the ones that got caught, you know, and I
- [41:43] think to your point about OpenAI feeling bummed that they
- [41:46] weren't part of it, maybe they just don't have the
- [41:47] telemetry to notice. Like I think it's, it's. We've seen
- [41:52] one example of this and, and had it exposed to
- [41:56] the sunlight, but that does not mean that this is
- [41:58] the only one out there. In fact, I strongly suspect
- [42:00] and they, they mentioned in the article that it is
- [42:02] not the only one out there. So I do think,
- [42:05] you know, there is generally a, the frontier models are
- [42:13] generally closed. Even now that we have extremely capable open
- [42:17] models like the recent Kimi K2 thinking and minimax models,
- [42:22] they're extremely hard to run at scale. So depending on
- [42:26] the division of labor, if you've got a team that's
- [42:28] the experts in the cyber hacking side of things, they
- [42:32] probably aren't the experts in running the expensive GPU rigs
- [42:37] to run these extremely large models. And I think what
- [42:40] you get with running all of this through a frontier
- [42:43] model is the hands off nature of it versus trying
- [42:47] to put this together more piecemeal. The interesting thing too
- [42:51] is, you know, in the cyber security domain, the attackers
- [42:54] always have the advantage unless they are targeting one very
- [42:57] specific needle in the haystack. Right. If their goal is
- [42:59] just to go get what they can, steal what's available,
- [43:03] there's no penalty for screwing up, right? You just keep
- [43:06] trying, right? So this Same set of actors may very
- [43:08] well also be banging on an open source version. And
- [43:12] one using GPT Star and one, you know, like, just
- [43:16] try them all, why not, right? Like what's the downside
- [43:19] other than it costing money? But the, you know, the
- [43:22] defenders have the much more difficult task of catching everything
- [43:27] that slips through, right? Like you have to have, have
- [43:29] ironclad practices to avoid being, you know, in the spotlight
- [43:34] of these things. So screwing up has huge penalties. The
- [43:38] thing that did strike me as interesting in all of
- [43:40] this, just to, to look at that defender view of
- [43:42] all of it, is that eventually, even though the agent
- [43:46] was taking a lot of the decision making and the
- [43:50] sort of scripting and the basics of how do we
- [43:53] craft this attack, it's still all exploited standard vulnerabilities, right?
- [43:59] Like, it seemed to me like most of what they
- [44:01] were doing was looking for systems that were running exposed
- [44:05] and vulnerable versions of known exploits and then exploiting them.
- [44:09] So if anything, this just puts a finer point on
- [44:11] enterprises needing to stay up to date with their CVE
- [44:14] patch fixes and just all the best security practices that
- [44:18] we've all had hammered into us, just really do them
- [44:21] for real do them like someone's gonna find it really
- [44:23] doable them. So I, I think, you know, the defensive
- [44:27] side of this, there's sort of the, the AI element
- [44:30] of the defense, which is like, how in the world
- [44:32] do we catch these things? What do we put into
- [44:34] our AI systems as we're building them to make sure
- [44:36] that they can't be exploited? And then there's just the
- [44:39] good old fashioned cybersecurity of like patch your software people
- [44:42] like do it for real or you're going to get
- [44:45] hacked. So I, I think it's an all of the
- [44:47] above strategy on the defense side. But if anything, this
- [44:51] just makes it more urgent that that patch fix timeline
- [44:55] tightens up. I love that we're kind of landing on
- [45:02] place, which is just like at the end of the
- [45:03] day these are not very complex tasks that are being
- [45:06] implemented. Marina, I'll give you the last word if you
- [45:10] have any hot takes before we close the episode today.
- [45:12] Nah, I agree with Gabe that again, I'm not a
- [45:15] cybersecurity expert, but it seemed like here it was crazy,
- [45:18] not creativity, but yes, scale, where it was just try
- [45:21] the same well known things, but just try them a
- [45:23] lot faster than a human could do them. Okay, so
- [45:26] probably at least a lot of those things could be
- [45:27] done. One thing I kind of liked was the anthropic
- [45:30] team slipped in there that they actually used Claude to
- [45:33] analyze the logs of what was going on. So something
- [45:37] that then comes to mind as well. Should we maybe
- [45:39] be thinking that the models themselves know what they would
- [45:43] use if they were turned to evil, so they're more
- [45:45] likely to catch themselves themselves than what a human might
- [45:48] come up with? And then the different models have different
- [45:50] biases. So maybe they could be to have OpenAI models,
- [45:53] check the anthropic models and try to see what you
- [46:02] but at the end of the day, it's almost like
- [46:05] the tasks like. I agree with Gabe, just do the
- [46:07] basics. Because now you can just have your basics broken
- [46:10] a lot faster. So you really need to do this.
- [46:12] To get the basics right. Please, please. Well, with that
- [46:16] bit of very good advice, we'll close the episode today.
- [46:18] Marina, Gabe, Marv, thank you for joining us for the
- [46:21] show today. That's all the time we have and thanks
- [46:23] for joining all you listeners. If you enjoyed what you
- [46:25] heard, you can get us on Apple Podcasts, Spotify and
- [46:27] podcast platforms everywhere. And we'll see you all next week
- [46:30] on Mixture of Experts.