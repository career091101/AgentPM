---
title: "This seems like a really simple way to evade detection. You put in a random time in between keystrok..."
video_id: "iaZS1jer8MY"
video_url: "https://www.youtube.com/watch?v=iaZS1jer8MY"
speaker: "Unknown"
channel: "Unknown"
date: ""
duration: ""
tags:
  - "AI"
  - "Agents"
  - "Automation"
  - "Programming"
  - "Tutorial"
  - "Startup"
topics:
  - "AI Agents"
  - "Tool Integration"
  - "Workflow Automation"
  - "Product Development"
summary: |
  This seems like a really simple way to evade detection.
  You put in a random time in between keystrokes. This
  should have been done 10 years ago. Right? But on
key_points:
  - "This seems like a really simple way to evade detection."
  - "You put in a random time in between keystrokes. This"
  - "should have been done 10 years ago. Right? But on"
  - "the other hand, why is the detection software looking at"
  - "I'm your host, Matt Kaczynski. And joining me today, Chris"
  - "here with me, folks. Today we are talking about the"
  - "AI governance gap, malware that acts like a person, how"
  - "social engineers are manipulating stock prices and ballooning bottom bug"
category: "AI Agent Development"
confidence_level: "high"
---

# Transcript: iaZS1jer8MY

- URL: https://www.youtube.com/watch?v=iaZS1jer8MY
- Retrieved at: 2025-12-30T15:57:01+09:00

## Text

- [00:00] This seems like a really simple way to evade detection.
- [00:04] You put in a random time in between keystrokes. This
- [00:06] should have been done 10 years ago. Right? But on
- [00:08] the other hand, why is the detection software looking at
- [00:13] speed of key inputs as a metric to determine human
- [00:16] versus not human? All that and more on security intelligence.
- [00:24] Hello, and welcome to Security Intelligence, IBM's weekly cybersecurity podcast
- [00:30] where we break down the most interesting stories stories in
- [00:32] the field with the help of our panel of experts.
- [00:35] I'm your host, Matt Kaczynski. And joining me today, Chris
- [00:39] Thomas, AKA Space Rogue X Force Global lead of technical
- [00:43] eminence and part of the not the Situation Room podcast,
- [00:46] and sridharmapidi, IBM fellow cto IBM Security. Thanks for being
- [00:51] here with me, folks. Today we are talking about the
- [00:54] AI governance gap, malware that acts like a person, how
- [00:58] social engineers are manipulating stock prices and ballooning bottom bug
- [01:02] bounties. But first, let's talk about malicious AI agents. Now,
- [01:11] there's been a lot of talk about how attackers could
- [01:13] weaponize AI agents, and we're finally starting to see it
- [01:17] happen for real, or at least in proofs of concept.
- [01:20] And two in particular came out last week that I'd
- [01:23] like to talk about. The first is from researchers at
- [01:26] Datadog who identified a technique they call Kofish because it
- [01:30] takes advantage of Microsoft Copilot Studio. Attackers can basically use
- [01:34] it to build malicious AI agents that secretly steal oauth
- [01:39] tokens in the background. The second was from researchers at
- [01:43] Palo Alto who reported on what they call agent session
- [01:46] smuggling. This uses the agent to agent communication protocol to
- [01:51] secretly transmit malicious commands to a target agent. Basically, the
- [01:55] protocol allows two agents to talk to each other and
- [01:57] the user doesn't necessarily see what they're saying. So if
- [01:59] you've got a malicious one, you it can secretly say
- [02:01] some nasty stuff that makes the other agent do some
- [02:04] bad things. So I want to start by with you,
- [02:08] actually, Chris. Is this a case of legitimate tools being
- [02:11] put to illegitimate ends or is it a deeper flaw?
- [02:13] What do you think's going on? I think it's a
- [02:15] big part of that. I mean, the criminals are going
- [02:16] to use whatever tools they have available, right? Because criminals
- [02:19] are going to crime and if they have AI tools
- [02:22] available, they're going to use those AI tools. The advantage
- [02:24] that the criminals have here is that they can just
- [02:26] sort of experiment and play around with stuff and throw
- [02:29] stuff at the wall and see what works. And some
- [02:31] of it works. And I'm sure there's a whole bunch
- [02:33] of stuff they've tried that hasn't worked. That we haven't
- [02:35] seen because it didn't work. Makes sense. Sridhar, you have
- [02:38] any thoughts to add on that? We are going to
- [02:40] see a number of such attacks in the future. Right.
- [02:44] If you look at, if you rewind a little bit
- [02:46] from Black Hat timeframe, we saw Gemini, which has got
- [02:50] very similar attack as this agent session smuggling. And we
- [02:53] also saw Echo Leaks which had a very similar kind
- [03:03] think if I step back, I kind of look at
- [03:05] agents being autonomous and that creates problems of oversight. Agents
- [03:10] can be coerced into doing something like, you know, social
- [03:14] engineering of humans, social engine of agents. And also agents
- [03:18] are non deterministic in nature. And attackers, you'll see will
- [03:22] take advantage of these principles more and more as we
- [03:26] look forward in the next few years to come. Yeah,
- [03:29] I'm glad you brought up the social engineering angle because
- [03:31] I thought of that. That too. Right. And what's interesting
- [03:33] is that both of these kind of illustrate social engineering
- [03:36] but in slightly different ways. Right. The first is a
- [03:38] little more classic, the Kofish anyway because it, it preys
- [03:42] on users trust of Microsoft. Right. You think, oh, an
- [03:45] agent hosted on Microsoft, that's got to be perfectly legitimate.
- [03:48] Right. And of course somebody could be using it for
- [03:50] illegitimate ends. The other one that's really interesting though is
- [03:52] that the, the agent to agent attack, the, the agent
- [03:55] session smuggling. It's almost like socially engineering an AI agent.
- [03:59] Right. Like you're kind of tricking the good agent with
- [04:01] your malicious agent. I was wondering if you folks had
- [04:04] any thoughts about this new frontier when it comes to
- [04:08] being able to now socially engineer some of our technology
- [04:11] maybe in a way that we couldn't in the past.
- [04:13] And let's start with you Sridhar, because you brought up
- [04:16] the social engineering thread. Do you have any thoughts on
- [04:17] that? I think it's about making sure that we scope
- [04:20] the agent. Right. I mean agents cannot be doing everything
- [04:24] you have to put blinders. We talked about a good
- [04:26] analogy of social engineering. So the question is, how do
- [04:29] you put blinders on the agent such that it is
- [04:32] only doing certain things for certain individuals, either on behalf
- [04:36] of an individual or in behalf of an agent, or
- [04:39] autonomously, but being able to scope it extremely fine tune
- [04:43] it to either time, either resource, actions, either scope in
- [04:49] terms of location that will limit the agent from getting
- [04:54] coerced into doing things that he's not supposed to do
- [04:57] through social engineering. Absolutely. Chris, what about you any thoughts
- [05:01] there? I haven't seen anything yet where one agent is
- [05:04] specifically manipulating another agent, but I can totally see that
- [05:08] that's where that where things are headed, right? Like I
- [05:10] said, criminals are going to crime, they're going to use
- [05:12] whatever tools they have. And if they have an agent
- [05:14] as a tool that they can use, they will absolutely
- [05:17] apply that against another agent to try to manipulate it,
- [05:20] coerce it, get it to do things that maybe it's
- [05:22] not supposed to do. So we have to have those
- [05:24] blinders in place where we have to be able to
- [05:26] create those agents so that they can't get out of
- [05:29] their little sandbox. And no matter how much the attackers
- [05:32] try to manipulate them, they still can't give up the
- [05:35] information that the attacker wants. I think I do want
- [05:37] to pause you for a second. Right. And before we
- [05:41] leave this topic, I think this is the tip of
- [05:44] the iceberg, right? If I look at the agent behavior,
- [05:49] the attackers exploiting this autonomous behavior for additional threat privileges
- [05:54] because that is the easiest thing to attack, right? Chris,
- [05:58] what did you guys publish? 30% of the attacks that
- [06:02] you're seeing it through valid credentials. So it is so
- [06:05] easy to go and get these things, valid credentials, and
- [06:10] launch an attack. But as you go beyond that, right,
- [06:14] go one level, beyond the tip of the iceberg to
- [06:16] the next level. This is where you can see red
- [06:19] teaming and blue teaming have to come together. This is
- [06:22] somewhere you see agents being non deterministic. So they will
- [06:27] drift and attackers will take advantage of this drift. So
- [06:32] this to me is a beginning of a new class
- [06:35] of attacks that we will see. And you are seeing
- [06:38] this only because it is so much easier to use
- [06:41] valid credentials to attack versus trying to do something, which
- [06:44] is rocket science. Yeah, I think that's a really good
- [06:47] point. I'm glad you brought that up because that ties
- [06:49] into conversations we've had in the past, doesn't it, Sridhar,
- [06:51] about how when AI agents enter the equation, how you
- [06:55] approach identity and access management kind of has to change
- [06:57] a little bit, right? Yeah. And so I'm glad you
- [07:01] brought that up because this is one of those cases
- [07:03] where you're taking that credential based attack which we've seen
- [07:06] launched against human employees and now you're launching it against
- [07:09] a new kind of employee, a mechanical one, basically, right?
- [07:13] I mean, criminals and hackers alike are both very lazy
- [07:16] people at the root core. Right? We're lazy. And anything
- [07:20] we can do to make our job easier is what
- [07:22] we do. And having valid credentials and makes the job
- [07:25] so much Easier. So anything that can get the attacker
- [07:28] the valid credentials is definitely where they're going to go
- [07:30] first. So do, is it, do you think it's easier
- [07:32] then to get valid credentials from an agent than it
- [07:35] is from a human employee? Or is it just a
- [07:37] similar level of challenge, but it's just a different way
- [07:39] of going about it? How do we think about that?
- [07:41] I think we have to think about it. And if
- [07:43] you use the current techniques, I think you're talking about
- [07:45] how we have to fundamentally think about changing some of
- [07:48] our behavior as well as how we do things right.
- [07:52] Today we do a really good job of authenticating human
- [07:55] beings. We do two factor authentication, multi factor authentication, all
- [07:58] the cool things, right? But machines don't have that, so
- [08:03] we tend to use functional IDs, which are basically giving
- [08:06] them over privilege instead. I think what we should be
- [08:10] thinking about is to identify the agent with some level
- [08:14] of identity. Just like you would identify a human being,
- [08:17] right? End of the day, agents are your next level
- [08:19] of insiders. So just like you would identify a human
- [08:23] being, you have to identify an agent. Once you identify,
- [08:27] you have to do the same thing that we do
- [08:29] with humans, authenticate them, right? And then you figure out
- [08:32] how to scope what that agent can do, both the
- [08:36] good and the bad. And while you're doing that, that's
- [08:40] when you can think about a very, very fine level
- [08:44] of granularity, of observability, so that we can then monitor
- [08:49] all the behaviors and be able to detect anomalous behavior
- [08:52] very quickly. Chris, I thought I saw you start to
- [08:54] say something. Do you have anything to add there? No,
- [08:56] I'm just agreeing with basically everything that's being said here.
- [08:59] We have to identify the agents, then authenticate them and
- [09:04] give them the appropriate permissions that they need, just like
- [09:06] we do with our human users, right? So even though
- [09:08] the user's not specifically human, they still need to follow
- [09:11] the same identity and authentication processes or tailored for AI
- [09:15] agents. Not the same, obviously, but they need to be
- [09:19] identified and authenticated properly. So it sounds like the way
- [09:21] we treat human identities and non human identities is going
- [09:24] to get closer and closer over time. Is that accurate
- [09:26] to say? Yeah, and the only thing is the scale
- [09:30] is exponentially different than humans versus non humans. So back
- [09:37] to your point, right? You cannot use all existing tools.
- [09:40] Existing processes may apply, but you're talking about a different
- [09:43] magnitude of scale. So you need to think about automation,
- [09:46] you need to think about being proactive. You know, you
- [09:48] think about a lot of things that are more dynamic
- [09:51] in nature than static in nature. So maybe that then
- [09:53] is a good segue into the AI governance question, because
- [09:57] we have this AI governance gap right now, right? And
- [10:05] this is coming out of both research that IBM has
- [10:09] conducted and an article from IBM's Judith Aquino, kind of
- [10:12] breaking some of that research down for us. Now, organizations
- [10:15] are deploying AI tools faster than they can develop robust
- [10:18] risk governance frameworks for those tools, according to IBM's AI
- [10:23] at the Core 2025 research report. The kind of key
- [10:26] figure here, at least as far as I'm concerned, is
- [10:28] that 72% of businesses surveyed said they have integrated AI
- [10:33] into at least one business function. But only 23.8% of
- [10:37] businesses surveyed said they have extensive governance frameworks in place.
- [10:40] That's a pretty big gap between who's deployed and who's
- [10:42] got extensive governance in place. And so I want to
- [10:46] head throw back to you again here, Sridhar, you know,
- [10:48] because again, this touches on things we just talked about
- [10:50] and conversations we've had before. This about the gap between
- [10:54] what we when we deploy AI and the governance that
- [10:56] needs to catch up. Why does such a gap exist?
- [10:59] Why does governance lag behind deployment pretty much every time
- [11:02] we introduce something new? I think we've seen this movie
- [11:04] so many times, right? We've seen this movie so many
- [11:07] times. Most recent movie has been the cloud playbook. You
- [11:10] know, deploy fast, govern later, get breach in between. Right.
- [11:13] As simple as that. It's the same rinse and repeat
- [11:16] story we've seen like, you know, a few times. Right.
- [11:19] And the reason that that happens is because of the
- [11:22] innovation. End of the day, businesses have to innovate. They
- [11:25] have to go and make themselves relevant. Right? We all
- [11:30] move to cloud for the operational benefits, efficiencies, et cetera.
- [11:35] AI is going to help us with productivity, not just
- [11:37] with the employees, but also consumers, ease of use, et
- [11:40] cetera. And they have to leverage that. As a result,
- [11:43] you've got innovation which is primarily driven by the applications
- [11:47] team accelerating, and then meanwhile you've got the risks trying
- [11:52] to catch up. And if you don't balance it really
- [11:55] well, that's why you keep seeing the gap keep widening
- [11:59] up. Widening, right? Absolutely. Chris, I'm wondering if you have
- [12:03] thoughts on the kind of cybersecurity implications of these gaps
- [12:06] when they pop up? Kind of like Sridhar said, it's
- [12:08] like deploy, get hacked in between and then finally get
- [12:10] to some governance. What do you think about that? This
- [12:13] is a similar playbook. Like we're repeating the same story
- [12:16] that we've repeated with bring your own Device, cloud, work
- [12:20] from home. Now we have AI and we implement the
- [12:24] technology first and then we like, oh wait, we got
- [12:26] to put some rules around this and figure out exactly
- [12:29] how we're supposed to do this. And a lot of
- [12:31] times we look at, okay, just make it work and
- [12:33] then we'll figure it out later. And that it's that
- [12:36] gap in between that the attacker looks for because we,
- [12:40] they see that gap, they, they see an opportunity for
- [12:43] them to leverage that lack of governance so that they
- [12:47] can get into your organization and do what it is
- [12:51] that they do. And this is a common problem that
- [12:53] we've had with new technologies and AI isn't any different
- [12:57] here. So we, it's important to recognize that and realize
- [13:04] that your AI is now a risk and you need
- [13:07] to be. If you don't have the governance in place,
- [13:11] then try to mitigate that risk as much as you
- [13:13] can by network design, authentication, et cetera, other tools that
- [13:18] you have until your governance can catch up. No, that's
- [13:21] it. You cannot ban AI, you cannot ban devices. Right.
- [13:25] You cannot ban, you cannot ban from employees working from
- [13:28] home. Right? Employees will be employees and they'll use it
- [13:33] anyway. So we have to, to Chris's point, I think
- [13:36] we have to the choices to make it secure, enablement
- [13:42] or have a blind exposure, that's the choices that you
- [13:46] have. One of the things that I talk about is
- [13:51] security is becoming more and more distributed and it's become
- [13:55] more and more shared responsibility. It's not the question of,
- [13:59] okay, the security Persona owns security and the application teams
- [14:03] don't own security, and hence let me go run with
- [14:06] it and come back and catch it, but instead be
- [14:10] able to have a mechanism by which we can start
- [14:14] thinking about finally learning from all of these movies that
- [14:17] we've seen into making it a shared responsibility. Right. I
- [14:22] kind of call it like a guardrails versus checkpoints or
- [14:25] gates. When you have a guardrail, fine. You can define
- [14:29] a policy which says, sure, on this speedway you can
- [14:33] go at 55 miles even if it's a 50 mile
- [14:35] speed limit. That's okay, right? As long as you don't
- [14:38] jump the guardrails. But the other hand, if you put
- [14:42] a lot of checkpoints, you know what happens at checkpoints,
- [14:44] right? Like at toll gates, there's a long traffic jam.
- [14:47] So that's a cultural change that we have to think
- [14:50] about it. We have tools for sure, but that's the
- [14:52] cultural gap that I'm hoping that at some point we
- [14:55] will learn. A lot of times people look at security
- [14:58] even as practitioners who. They're the know people. Oh, you
- [15:02] can't do that. It's bad. It's not secure. You can't
- [15:04] do it. And I've always tried to look at it
- [15:06] like, no, that's security's job is to say yes and
- [15:09] figure out how to do it securely. And to Siddharth's
- [15:12] point, it is a cultural change that we have to
- [15:14] look at here and that it's everybody's responsibility to think
- [15:18] of these security, how can I do this and be
- [15:20] secure at the same time? Not just how do I
- [15:23] do it and rush it out the door. So it
- [15:26] is everybody's responsibility, but it should be also everybody's job
- [15:30] to figure out, yes, we can do this and we
- [15:32] can do it securely, not just no, because it's not
- [15:35] secure. Yeah, I'm glad you both brought that up because,
- [15:37] you know, that was kind of what I was going
- [15:38] to ask. Is that like, you know, is it, I
- [15:41] don't know, maybe the word I was looking for is
- [15:42] responsible, right? Is it responsible to deploy this technology before
- [15:45] you have governance? And it seems like that's completely the
- [15:47] wrong question to ask because realistically, like you said, Sridhar,
- [15:50] and you, Chris, you can't ban this stuff. People are
- [15:52] going to use it. So you can either kind of
- [15:54] stand there and try to stop them from doing something
- [15:56] they're going to do or. Or you can enable them
- [16:02] me to another question, and it's kind of a big
- [16:04] one. And, you know, I'm sorry to spring it on
- [16:06] you, but one of the things I often hear from
- [16:09] people when I talk to them about, you know, sort
- [16:11] of enabling everybody to be more secure in an enterprise
- [16:14] context is how you can give all this kind of
- [16:17] security training and half of it just doesn't stick with
- [16:20] people. Right. They just don't follow it. So do you
- [16:22] have any thoughts on what this culture change looks like
- [16:25] to make this kind of distributed shared responsibility model actually
- [16:29] work? Any thoughts there? I think part of it is
- [16:31] understanding the risk, understanding the risk to the business. Right.
- [16:35] I mean, that risk is a, you know, fine, it
- [16:38] is probably a gray word, but at the same point,
- [16:41] if it makes it relevant to the application teams, right.
- [16:45] This is my sensitive data, or I'm holding the sensitive
- [16:48] data for my clients that I'm serving. How do you
- [16:51] understand that? In a manner that shows that they're taking
- [16:56] a risk by not looking at certain security vulnerabilities? For
- [17:00] example, I Think if you understand that cleanly then it's
- [17:07] very similar to saying that sure, I don't want to
- [17:09] buy insurance right now, but if you show the likelihood
- [17:12] of insurance or likelihood of a storm or a flood
- [17:18] next to an ocean versus the likelihood on in a
- [17:21] mountain, right. It may change, it may change the thinking
- [17:25] to say I may want to get flood insurance. Right.
- [17:27] So that awareness is number one and number two I
- [17:32] feel is gamification. Right? Bit of a gamification will help
- [17:38] in terms of like how do we security, including myself.
- [17:43] Right. I will poke holes at myself first. We tend
- [17:46] to make it very complex, right? We tend to make
- [17:49] it very complex. Like we talked about OAuth 10 minutes
- [17:52] ago. Right. It is so hard to set up the
- [17:55] entire delegated flows within OAuth. It's not for the weak
- [17:58] hearted and that's one of the reasons why people don't
- [18:01] embrace it as easily. So how do you make it
- [18:04] simple? How do you gamify a little bit? How do
- [18:06] you make it fun so that you can then say,
- [18:09] hey, using a very simple analogy, here's my risk thermometer.
- [18:15] Here's my mitigation thermometer. Let me show you where that
- [18:19] is and give you some indication of how much risk
- [18:22] you're taking or not taking can probably influence people from
- [18:26] not rushing forward. I agree. I mean to changing the
- [18:29] security mindset or the security culture that we have in
- [18:32] organizations so that it's everybody's responsibility. And you touched on
- [18:37] training. We need to totally revamp our training regimens. We
- [18:41] all have the same multiple guests, how to identify a
- [18:46] phishing email, bad grammar and other things. And in the
- [18:50] age of AI, all that training is really no longer
- [18:55] effective. The techniques and tools that you use as an
- [18:58] individual to try to identify this risk are totally different.
- [19:02] Now AI makes a perfectly sounding email. There's no spelling
- [19:06] mistakes. So trying to use that old multiple guest training
- [19:12] and people just click through it as fast as they
- [19:13] can. We all do. Right? Because we all have the
- [19:15] same training. We got to get back to work. So
- [19:18] making it gamified and making it so that the user
- [19:21] can identify the risk, not necessarily the telltale signs of
- [19:26] an attack, but what's the risk to me? What's the
- [19:30] risk to the organization? How do I mitigate that risk?
- [19:33] That's the sort of training that we need to, to
- [19:35] integrate into our people. And then again, every time I
- [19:38] talk about training, don't rely on it. It can't. It's
- [19:41] the last. It's not the first line of defense. It's
- [19:43] not the last. It's one more tool in the toolbox.
- [19:46] A lot of companies will say, oh, I trained all
- [19:48] my people. We're secure. That's not how it works. Okay,
- [19:52] so just. Yeah, yeah. As somebody, you know, who was
- [19:56] not a sort of cybersecurity professional, Right. And who so
- [19:59] spent, you know, years and years and years of my
- [20:02] life taking those trainings as the employee, it's true, I
- [20:05] didn't pay any attention to them, right. But now that
- [20:08] I've come into this realm where I kind of do
- [20:10] this podcast with folks like you guys and I learn
- [20:12] the concepts behind this stuff, it is so fascinating. And
- [20:15] so I do think that, like, if you actually teach
- [20:17] people the concepts and not just, you know, the scolding,
- [20:20] hey, make sure you change your password. I do think
- [20:22] you start to get somewhere. You know what I mean?
- [20:24] It's like you said, Sridhar, give people an actual understanding
- [20:26] of the level of risk that they are taking. And
- [20:29] then they'll be like, hey, you know what? I understand
- [20:31] this in a real context to do something about it,
- [20:33] you know. Teach him how to fish. Not exactly. Exactly.
- [20:36] Teach him how to fish. Don't give him the fish.
- [20:43] Let's move on then to our next topic. Today we
- [20:46] are going to continue a little bit on this theme
- [20:47] of blurring the lines between people and non human entities,
- [20:51] if you will, with the malware that acts like a
- [20:54] human. Specifically, I'm talking about a newly discovered banking Trojan
- [20:59] nicknamed Herodotus, which evades behavioral detection systems by timing text
- [21:05] inputs to look more like a human being typing. Right?
- [21:08] Now, this comes from threat fabric. They're the ones who
- [21:10] found it. And, you know, in a lot of ways,
- [21:12] Herodotus is very much like your standard banking Trojan. It
- [21:15] gets in, it steals credentials, remote access, yada, yada, yada.
- [21:18] But the one little wrinkle here that I thought that
- [21:20] caught my eye was that, you know, in order to
- [21:23] make it seem. In order to evade some of these
- [21:25] behavioral detection systems, instead of just kind of inputting text
- [21:28] all at once, Herodotus would take the text the hackers
- [21:32] wanted to input, split it into characters, and enter characters
- [21:35] one by one on a timing delay to make it
- [21:37] seem like keyboard, you know, fingers on a keyboard. And,
- [21:39] you know, look, I'm a non technical person largely, but
- [21:42] I thought this was interesting. But I want to ask
- [21:44] more technical people, and I'll start with you, Chris. Is
- [21:46] this impressive? Is this as clever as I think or
- [21:48] is this not really that big a deal? What do
- [21:50] you see here? Both. I'm surprised that it took this
- [21:54] long. Why is this the first one that we're seeing
- [21:57] to do this? This seems like a really simple way
- [21:59] to evade detection. You put in a random time in
- [22:02] between keystrokes. Like, why isn't. I mean, it's 2026. This
- [22:06] should have been done 10 years ago. Right. So in
- [22:09] that case, yeah, this is kind of cool and interesting
- [22:11] because somebody's finally figured it out. But on the other
- [22:13] hand, why is the detection software looking at speed of
- [22:20] key inputs as a metric to determine human versus not
- [22:24] human? Like, I hope there are some other metrics in
- [22:26] there that it's also looking at, because key input, like
- [22:29] that's a known heuristic that you can identify individuals by
- [22:33] is how they type. Like that's a known thing. So,
- [22:37] yes, this is both amazingly amazing and both. Yeah, whatever.
- [22:43] Next. Yeah, I'm actually surprised as well. Right. As we
- [22:47] always think that our adversaries are way ahead of the
- [22:52] defenders because they work together, they're more opportunistic, they're more
- [23:01] we actually have a product in identity space which looks
- [23:05] at a combination of your subject, which is a person,
- [23:10] your action, which is a resource, your network activity, your
- [23:13] environment, your behavior, which is not just the keystrokes and
- [23:16] mouse movement, but also the fact that I'm doing a
- [23:19] $30 transaction versus $3,000 transaction, puts them all through the
- [23:23] ringer, and then tells you whether you want to do
- [23:25] MFA or not. An mfa, it calculates the risk. Right.
- [23:29] So that is, we've had it for six, seven years
- [23:34] right now in production, and IBM uses it. Right. So
- [23:38] I'm surprised that right now we are looking at something
- [23:41] similar, which is probably one dimension. So maybe they will
- [23:48] get exponentially faster. I see that eventually the attackers are
- [23:53] going to start thinking the same way that we've been
- [23:55] thinking, which is space bar or keystroke measurement is one
- [24:01] dimension. They will probably look at other dimensions so that
- [24:05] they are able to then go and provide multiple parameters
- [24:11] to be able to circumvent the fact that this is
- [24:14] a bot versus a human? Yeah, I'm glad that you
- [24:17] kind of brought that up, because that was what I
- [24:18] was wondering. Right. Is this kind of the beginning maybe
- [24:22] of a little bit of your classic kind of arms
- [24:25] race? Right. Like you said, Sridhar, we have these kinds
- [24:27] of behavioral detection systems that are pretty complex, these a
- [24:30] lot of different factors. Maybe the hackers just stumbled onto
- [24:33] one of them, but maybe they'll start to use more
- [24:35] of them and then Are we entering a world where
- [24:37] these things are going to keep escalating? I don't know.
- [24:39] What do you think? Is that where we're headed? Will
- [24:40] we see more of this humanized malware? I think there's
- [24:42] two dimensions over here. One is definitely, definitely more humanized
- [24:47] malware. And not just so humanized malware, but there's a
- [24:49] sister side of it, which is automated red agents. Going
- [24:53] back to the agent discussion that we had a few
- [24:56] minutes ago. Right. But I think that's one dimension. The
- [24:59] other dimension is also, end of the day, the attackers
- [25:03] are running a business. It's a return of investment. Why
- [25:06] are they doing that now? They're not dumb. Right. As
- [25:09] smart as we think we are, we are not. At
- [25:11] least I'm not. Right. I think they're doing it because
- [25:15] not everybody using the multidimensional risk analysis. It's very similar
- [25:20] to mfa. How many people actually use mfa? Not a
- [25:25] new technology, but I'm surprised that not many people use
- [25:28] msa. I see your head nodding. Right. So it's the
- [25:31] same thing. Right. Not many vendors or not many organizations
- [25:37] are using multidimensional way of evaluating if there's human or
- [25:43] not human. Most people are still stuck on captcha or
- [25:46] maybe some traditional ways of doing that, and maybe that's
- [25:50] what they're going after. Right. So there's two dimensions that
- [25:52] I look at. One dimension is where the maturity of
- [25:56] the market is. And again, this is a precursor to
- [25:59] something which is, you know, going to explode as well.
- [26:02] So, yeah, I've been yelling at people to turn on
- [26:04] MFA ever since I started. Ever since I started covering
- [26:07] cybersecurity and learned how bad passwords are at keeping you
- [26:10] safe. I try to yell at everybody in my life,
- [26:12] turn on mfa, and they don't always listen to you.
- [26:15] Chris, any thoughts on your end here on what we
- [26:17] could expect from this humanized malware trend? Well, I mean,
- [26:20] it's the old cat and mouse game, right? We put
- [26:23] in a defense. The bad guys or criminals figure a
- [26:26] way around the defense, and then we put in another
- [26:28] defense, and then the bad guys, criminals figure out another
- [26:31] way around the defense. So it just kind of goes
- [26:33] back and forth. Do they have an advantage? Are they
- [26:37] further ahead than us? A little bit, maybe. But then
- [26:40] we catch up and pass them, and it kind of
- [26:42] goes back and forth. So this is, like I said,
- [26:44] this is both novel and not novel. I'm surprised it
- [26:48] took this long, and I'm kind of interesting to see
- [26:51] what they come up with next to try to bypass
- [26:53] some other humanistic heuristics that we have. Let's move on
- [26:57] then to our next story and talk about a very
- [27:00] interesting smishing attack, one that's happening on a level that
- [27:03] I personally haven't seen before. And this is a smishing
- [27:06] attack that manipulates stock prices. This is a, a campaign
- [27:14] that Fortra uncovered of a pretty large smishing network that
- [27:19] is sending out these messages to basically try to steal
- [27:22] people's brokerage accounts. And then once they get into the
- [27:25] compromised account, they manipulate stock prices to make some money.
- [27:29] Now, I'm going to quote Alexis Obert Fortra to explain
- [27:32] it because again, this is a kind of thing I
- [27:34] have not seen before. So I'm going to use her
- [27:35] words. In these scenarios, the threat actor will liquidate any
- [27:40] existing investments made by the victim and reallocate the funds
- [27:44] to low liquidity stocks, often penny stocks or IPOs. Then
- [27:48] they will artificially inflate the stock price by purchasing large
- [27:52] amounts. And once at a profitable level, they will sell
- [27:55] off the holdings to gain a financial profit before withdrawing
- [27:58] any earnings using mobile wallets. Again, I personally have just
- [28:03] never seen social engineering on this kind of scale before.
- [28:06] You know, I've seen people get their individual bank accounts
- [28:09] hit, but like to manipulate the markets. That's a little
- [28:12] bit scary to me. I don't know. What about you
- [28:14] folks? Any thoughts there, Sridhar? What are your feelings on
- [28:16] this thing? I want to step back right now. Granted.
- [28:19] It sounds really, really cool. Right. I have not seen
- [28:22] this either. Right. But I've seen similar things. But if
- [28:25] I step back, the fundamental thing over here is stealing
- [28:29] passwords. And that is the easiest thing that one can
- [28:34] do today. And it's pains to say that. Right. But
- [28:38] that is a reality. Right. Why go and jump the
- [28:41] walls when we can go and cut the wire fence
- [28:44] with a wire cutter and get into the property? So
- [28:47] I think to me, we are seeing a beginning of
- [28:52] how you can take advantage of compromised passwords, whether through
- [28:57] phishing or whether through buying hundreds of them or hundred
- [29:01] thousands of them at the Dark web. That's easy to
- [29:03] do these days. Once you do that, rather than trying
- [29:08] to go do a ransomware attack, which takes a long
- [29:10] time, the return on investment may not be as much.
- [29:14] I look at this as an opportunity to say, hey,
- [29:16] why don't I go and take care of tens of
- [29:21] thousands of brokerage accounts that gives you a few million
- [29:24] dollars very quickly that I can manipulate the market. So
- [29:27] the return on investment is way higher than having to
- [29:30] live in a network for a year. Before I see
- [29:32] that I'm somewhere. Right. So I'm thinking like an attacker,
- [29:38] right, for a change. But I have to think there
- [29:40] so that I can then start defending. Yeah, I, you
- [29:43] know, that's a really good point. And it's almost like
- [29:45] basically every story we've covered so far is sort of
- [29:48] the same theme that like everything old is new again,
- [29:50] right. It's like you said, this is just, I had
- [29:52] not thought about that. This is another kind of password
- [29:54] attack, right? It's put to really neat ends. But at
- [29:56] the end of the day, what are you doing? You're
- [29:58] stealing a password. You're getting in there. You're stealing a
- [29:59] password and using a person's account. Right. Chris, your thoughts
- [30:03] there? We've seen similar attacks with crypto, right. People trying
- [30:06] to get the passwords, get in the crypto account and
- [30:09] liquidate the account and move the fund somewhere else. And
- [30:12] we have seen some of this with, with brokerage accounts
- [30:15] too, but it's usually liquidation and get out, right? The
- [30:18] manipulate the markets with the penny stocks. That's a new
- [30:21] angle. That's an extra step. But at the, the, it's
- [30:25] still a password attack. It's still, you know, a Trojan
- [30:30] that we're going after. Bank, we're going for the money,
- [30:32] right? Whether it's crypto, bank account, brokerage account, they're going
- [30:35] for the money. The added step that's going on here
- [30:38] is that instead of just liquidation and get out, they're.
- [30:41] They're trying to make even more money before they can
- [30:44] liquidate. So, yeah, this is just again, another step, another
- [30:48] evolution that we're seeing in the criminal mind as they,
- [30:51] they take it to the next level. And more reason
- [30:54] to turn on that MFA again, right? It's like you
- [30:56] said, you want to keep people out, just turn on
- [31:03] do want to mention MFA is not a panacea, right.
- [31:05] It's not a guarantee. There are ways that a really
- [31:08] smart attacker can bypass mfa, but it's, it's another step.
- [31:12] It gets rid of the low level, attacks the ankle
- [31:15] biters, as we call them, and makes it more difficult.
- [31:18] And by making it more difficult, remember I said the
- [31:20] attacker is lazy. They're going to go to someone else
- [31:23] because you've made it. Oh, I'm not dealing with mfa.
- [31:25] I'm going to go to this other account. That's what
- [31:27] you want. Protect yourself, let somebody else be the victim.
- [31:29] MFA is absolutely required. I think there's no question without.
- [31:34] But again, too much of MFA also causes distractions. Right.
- [31:38] I think. And that's one of the reasons and one
- [31:41] of the avenues that attackers use. Right. MFA fatigue. Instead,
- [31:46] I think we should think about in 2026 and 2025
- [31:49] and 2026 moving forward. Right. We need to start thinking
- [31:53] about the behavioral analytics like we talked about in the
- [32:05] person? Whether it's a bot or a human, what device
- [32:08] it's coming from, known or unknown, what is the network
- [32:11] or the environment? Have I seen this before or not
- [32:15] seen this before? And the industry is actually doing a
- [32:18] really good job with a open specification called Shared Signals
- [32:22] over here. Right. As a part of the OpenID, if
- [32:25] you use something like that and collaborate on the shared
- [32:29] signals, whether it's identity related IOCs or IOBs or any
- [32:33] other things, the more data that you have, the better
- [32:37] you can do a risk evaluation to be able to
- [32:40] stop this. So I think it's not just MFA is
- [32:42] a means to an end and it is a result
- [32:44] for sure. But how you get to MFA has to
- [32:47] be behavioral analytics. That's a really good point on both
- [32:50] ends. And yeah, I'm glad you said also Chris, that
- [32:52] you know, the MFA is not a, it's not a
- [32:54] panacea, right. It's not going to fix every, it's not
- [32:56] going to stop everything because to be quite frank, I
- [32:58] believe in this attack. One of the things that's interesting
- [33:01] about it is that it does involve stealing one time
- [33:04] passwords, right? They have. And you know, again it's safer
- [33:07] to have that because it puts an extra obstacle but
- [33:09] it's not totally uncrackable. So you need those like you
- [33:13] said, those behavioral signals, Sridhar, that become. And when you
- [33:16] have a bunch of them, you can't fake those as
- [33:18] easily, you know. Yeah. So let's move on then to
- [33:22] our final story for the day, folks talking about bug
- [33:25] bounties getting bigger. This is a report from Bloomberg that
- [33:34] reports that bug bounty programs are skyrocketing in both popularity
- [33:39] and the amounts they're paying out, hitting some all time
- [33:42] highs. For example, HackerOne paid out 81 million over the
- [33:46] past year, which is its single highest year on record
- [33:50] and a 13% increase over the previous year. So pretty
- [33:53] significant jump. And what's particularly interesting to me here is
- [33:56] that in an era of AI when you have things
- [33:58] like Google Code Mender coming out and all this stuff
- [34:01] where people are like hey, we're going to automate the
- [34:03] ability to find your bugs. To see such a human
- [34:06] driven activity like this taking off even more, I just
- [34:10] thought that was kind of interesting and not necessarily what
- [34:12] I would expect. Chris, I want to start with you,
- [34:14] you know, as a hacker. What are your thoughts on,
- [34:17] on, on this kind of thing right now? I got
- [34:19] a lot of thoughts on bu. Bounties. Let me just,
- [34:22] I'll try to keep it to this particular topic today.
- [34:25] The big numbers that you're seeing for these specific bounties
- [34:29] are for very specific, very difficult to exploit, hard to
- [34:34] find bugs. They're not your run of the mill AI
- [34:38] finds 100 bugs in an hour type bugs. These are
- [34:41] the types of bugs that a state sponsored actor would
- [34:44] pay a lot of money for. And so the reason
- [34:47] for these big bounties are to keep them out of
- [34:49] the hands of the state sponsored actor, right. So that
- [34:53] they're not used against dissidents or on mass surveillance of
- [35:03] that AI is able to find, those do not pay
- [35:06] out anywhere near as much. But these big numbers also
- [35:10] make great headlines which helps the companies that run bug
- [35:13] bounty programs saying that, you know, all these, we're paying
- [35:17] out all this money, come find bugs and become a
- [35:19] millionaire. That's not really how it works. It takes a
- [35:22] lot of hard work to make a decent money at
- [35:24] bug bounty and it's a kind of a grind. But
- [35:28] if you have the skill set, yeah, you can make
- [35:30] some money there. But at the same time you have
- [35:32] companies who now have to, who are paying for this
- [35:35] service as well. And then you have attackers who are
- [35:38] using AI to find bugs and are flooding bug bounty
- [35:41] programs. This is a whole nother topic when it comes
- [35:44] to open source. And now I'm kind of going off
- [35:47] into some of my other topics areas so I'll leave
- [35:49] it at that. Yeah, there's big money here that can
- [35:53] be made, but it's difficult to make it. Yeah, we'll
- [35:55] have to have you come back and talk about this
- [35:56] in more depth with Sridhar. A whole show on bug
- [35:58] bounties. That's good to know. Sridhar, your thoughts on the
- [36:01] kind of state of bug bounties today? I can't seem
- [36:04] to recall that the name of a movie, right, where
- [36:06] this person is paid really, really high amount of money
- [36:10] to hack out of a prison. Right. And I kind
- [36:15] of look at it like that, right, that yes, you
- [36:18] are legitimate burglar, but you can make more money doing
- [36:23] ethically and legally than by being on the other side.
- [36:27] Right. So I kind of look at in two dimensions.
- [36:30] Like you've seen my theme. Now, one dimension is for
- [36:35] the attackers, like I said. Right. I know. Jokes aside,
- [36:38] this is a legitimate and illegal way of getting really
- [36:44] well paid and putting the best minds to that to
- [36:47] say you can actually make a good living out of
- [36:49] this, Use expertise from Space Rogue and all the things
- [36:57] that we do. As an example, the other dimension is
- [37:00] from a company perspective, it is a small sum of
- [37:05] money as insurance to pay for what may be a
- [37:08] much larger right for paying a million dollars in bug
- [37:12] bounty versus $10 million in ransomware. I'll take the first
- [37:16] option any day. So as a result, you see both
- [37:20] of those coming together into a perfect storm to increase
- [37:25] the momentum. There's a desire to do more bug bounty
- [37:30] legally and there's a desire to pay more because that's
- [37:33] an insurance. Right. That's where you see this more and
- [37:36] more on the increase. Now, having said that, for both
- [37:42] sides, bug bounty alone is not sufficient. I think Chris
- [37:46] was also saying that MFA is not the panacea. It's
- [37:49] not the only thing necessary. Bug bounty is part of
- [37:53] the resilience program of the overall resilience program. Basically, you
- [37:57] have to do all these things together other and. And
- [38:00] bug bounty should actually be the last thing you do
- [38:03] for all the stuff that you may have missed that
- [38:05] you did check for. So, yeah, to second. Second your
- [38:10] opinion there, it's not the only thing you should be
- [38:13] doing. It is one more tool in the toolbox. Given
- [38:16] their kind of ability to act as like you said,
- [38:17] this kind of last kind of line of like insurance
- [38:21] of like, hey, we did everything we could to find
- [38:22] this thing. If there's something still out there, we'll give
- [38:24] you a reward for finding it. Do you think we're
- [38:26] going to see the bug bounty programs kind of stick
- [38:28] around or do you see a day that, I don't
- [38:30] know, the AI gets good enough that this kind of
- [38:32] thing goes away? Any thoughts there? I think it's going
- [38:35] to evolve, right? I mean, our attackers are going to
- [38:37] evolve with more and more automated agents for doing attacks.
- [38:42] Right. Think of it as automated red teaming. And it
- [38:47] learn on the fly, it learn all the ttps, it
- [38:49] learn all the vulnerabilities. And while you go across get
- [38:52] a cup of coffee, you'll probably have an exploit and
- [38:55] probably the code generated to leverage that exploit as an
- [38:58] example. Right. So I think what we will see is
- [39:03] probably more and More purple teaming, Right? Not just a
- [39:08] mechanism to go and say, let me go and do
- [39:11] this automated testing and then come back and fix it.
- [39:14] But the speed in which this happens requires you to
- [39:17] have some sort of a blue agent which is able
- [39:20] to go and fight AI versus AI. And then individuals
- [39:26] then have to figure out how to govern those in
- [39:29] a manner that they can keep up with the speed.
- [39:32] I think in the short term it's hard to forecast
- [39:36] out 15, 20 years, but in the short term, I
- [39:40] think there's still going to be a need and a
- [39:42] requirement for manual review of code for the weird chaining
- [39:48] of bugs together and finding that weird edge cases that
- [39:52] AI is just not going to find for now. Right.
- [39:56] I have no idea what's going to happen in 20
- [40:04] teaming and other security aspects, if you want those edge
- [40:07] cases, if you want to find that weird chaining where
- [40:10] you're putting five bugs together to gain access, you really
- [40:14] need a human to do that. If you need a
- [40:15] surface level stuff, yeah, you need to check a box,
- [40:19] get your AI agent in there and do your red
- [40:22] team to check your box. But I hope at least
- [40:26] we're still going to need humans for a little while.
- [40:29] What I'm worried about though is that the AI is
- [40:33] going to take all the low level stuff and we're
- [40:35] going to run out of people expert enough to do
- [40:37] the human stuff. What I say, Chris, is AI is
- [40:41] going to help us with speed and accuracy. No question,
- [40:44] no caution on that. Right. But I think the human
- [40:47] ingenuity and creativity will always remain with us so that
- [40:50] when you combine it, that's when good stuff happens. I
- [40:53] hate to leave you all on that question on that
- [40:55] slightly apocalyptic scenario, but that is all the time we
- [40:59] have for today. So thank you Sridhar and Chris for
- [41:02] being here. Thank you to our list, listeners and viewers
- [41:05] and folks. Don't forget to check out the special episode
- [41:07] we released last week, how to Break into an Office,
- [41:10] which features our very own Stephanie Carruthers. Find it on
- [41:14] Apple, Spotify and audio platforms everywhere. As always, subscribe to
- [41:18] Security Intelligence wherever podcasts are found and stay safe out
- [41:22] there. there.
