#!/usr/bin/env python3
"""
è¨˜äº‹å†…ã®YouTubeåŸ‹ã‚è¾¼ã¿å‹•ç”»ã‚’æŠ½å‡ºã—ã¦Markdownã«è¿½è¨˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import json
import re
from pathlib import Path
from urllib.parse import urljoin, urlparse, parse_qs
import requests
from bs4 import BeautifulSoup
import time

def load_session_with_cookies(cookies_path):
    """ã‚¯ãƒƒã‚­ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    cookies_json = json.loads(Path(cookies_path).read_text(encoding='utf-8'))
    cookies_data = cookies_json.get('cookies', cookies_json)  # 'cookies'ã‚­ãƒ¼ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
    for cookie in cookies_data:
        session.cookies.set(
            name=cookie['name'],
            value=cookie['value'],
            domain=cookie.get('domain', ''),
            path=cookie.get('path', '/')
        )
    return session

def extract_youtube_urls(soup):
    """HTMLã‹ã‚‰YouTubeåŸ‹ã‚è¾¼ã¿URLã‚’æŠ½å‡º"""
    youtube_urls = []

    # iframeã‚¿ã‚°ã‹ã‚‰YouTube URLã‚’æŠ½å‡º
    iframes = soup.find_all('iframe')
    for iframe in iframes:
        src = iframe.get('src', '')
        if 'youtube.com/embed' in src or 'youtube-nocookie.com/embed' in src:
            # åŸ‹ã‚è¾¼ã¿URLã‹ã‚‰é€šå¸¸ã®watch URLã«å¤‰æ›
            video_id_match = re.search(r'/embed/([a-zA-Z0-9_-]+)', src)
            if video_id_match:
                video_id = video_id_match.group(1)
                watch_url = f"https://www.youtube.com/watch?v={video_id}"
                youtube_urls.append(watch_url)

    # aã‚¿ã‚°ã‹ã‚‰YouTube URLã‚’æŠ½å‡º
    links = soup.find_all('a', href=True)
    for link in links:
        href = link['href']
        if 'youtube.com/watch' in href or 'youtu.be/' in href:
            youtube_urls.append(href)

    # é‡è¤‡ã‚’å‰Šé™¤
    return list(dict.fromkeys(youtube_urls))

def format_youtube_markdown(urls):
    """YouTube URLã‚’Markdownå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not urls:
        return ""

    markdown_lines = ["\n## ğŸ“º YouTubeå‹•ç”»\n"]
    for idx, url in enumerate(urls, 1):
        markdown_lines.append(f"{idx}. [{url}]({url})")

    return "\n".join(markdown_lines) + "\n"

def process_article_youtube(session, metadata_path, md_path):
    """è¨˜äº‹ã®YouTubeåŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡ºã—ã¦Markdownã«è¿½è¨˜"""
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    metadata = json.loads(metadata_path.read_text(encoding='utf-8'))
    article_url = metadata.get('url')

    if not article_url:
        return 0

    # æ—¢å­˜ã®Markdownã‚’èª­ã¿è¾¼ã¿
    existing_markdown = md_path.read_text(encoding='utf-8')

    # ã™ã§ã«YouTubeå‹•ç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if '## ğŸ“º YouTubeå‹•ç”»' in existing_markdown:
        print(f"  â­ï¸  Already has YouTube section: {md_path.name}")
        return 0

    # è¨˜äº‹HTMLã‚’å–å¾—
    try:
        response = session.get(article_url, timeout=30)
        response.raise_for_status()
        html = response.text
    except Exception as e:
        print(f"  âš ï¸  Failed to fetch {article_url}: {e}")
        return 0

    # YouTube URLã‚’æŠ½å‡º
    soup = BeautifulSoup(html, 'html.parser')
    youtube_urls = extract_youtube_urls(soup)

    if not youtube_urls:
        return 0

    # Markdownã«è¿½è¨˜
    youtube_markdown = format_youtube_markdown(youtube_urls)
    updated_markdown = existing_markdown.rstrip() + "\n" + youtube_markdown
    md_path.write_text(updated_markdown, encoding='utf-8')

    print(f"  âœ… Added {len(youtube_urls)} YouTube URL(s): {md_path.name}")
    return len(youtube_urls)

def main():
    cookies_path = "../data/cookies/d_1d2d_cookies.json"
    articles_dir = Path("../data/d_1d2d_articles/articles")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    session = load_session_with_cookies(cookies_path)

    # ã™ã¹ã¦ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    json_files = sorted(articles_dir.glob("*.json"))
    print(f"ğŸ“š Processing {len(json_files)} articles for YouTube embeds...\n")

    total_videos = 0
    articles_with_videos = 0

    for json_path in json_files:
        md_path = json_path.with_suffix('.md')

        if not md_path.exists():
            continue

        videos_found = process_article_youtube(session, json_path, md_path)
        if videos_found > 0:
            total_videos += videos_found
            articles_with_videos += 1

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
        time.sleep(0.5)

    print(f"\nâœ… Complete!")
    print(f"   Articles with YouTube: {articles_with_videos}/{len(json_files)}")
    print(f"   Total YouTube URLs: {total_videos}")

if __name__ == "__main__":
    main()
