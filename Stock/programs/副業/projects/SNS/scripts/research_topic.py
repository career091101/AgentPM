#!/usr/bin/env python3
"""
Research Topic Skill Implementation
Webèª¿æŸ»ã§ãƒˆãƒ”ãƒƒã‚¯ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã€åå¯¾æ„è¦‹ã€å°‚é–€å®¶è¦‹è§£ã‚’åé›†
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

# WebSearchæ©Ÿèƒ½ã¯ClaudeCode LLMã§å®Ÿè¡Œï¼ˆã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®ã¿ï¼‰


def extract_research_topics(top_tweets_file: Path) -> List[Dict[str, Any]]:
    """
    Top 10ãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰èª¿æŸ»ã™ã¹ããƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º

    Args:
        top_tweets_file: Top 10ãƒ„ã‚¤ãƒ¼ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        èª¿æŸ»ãƒˆãƒ”ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    print(f"ğŸ“– Reading top tweets from: {top_tweets_file}")

    try:
        with open(top_tweets_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ Error: File not found: {top_tweets_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Error: Invalid JSON format: {e}")
        sys.exit(1)

    top_tweets = data.get('top_tweets', [])
    print(f"âœ… Loaded {len(top_tweets)} top tweets")

    # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆClaudeCode LLMã§åˆ¤æ–­ï¼‰
    topics = []
    for i, tweet in enumerate(top_tweets, 1):
        topic = {
            'tweet_id': tweet['tweet_id'],
            'username': tweet['username'],
            'text': tweet['text'][:200],  # æœ€åˆã®200æ–‡å­—
            'rank': i,
            'engagement_score': tweet.get('engagement_score', 0),
            'research_priority': 'high' if i <= 3 else 'medium' if i <= 6 else 'low'
        }
        topics.append(topic)

    return topics


def prepare_research_output(topics: List[Dict[str, Any]], output_file: Path) -> Dict[str, Any]:
    """
    èª¿æŸ»çµæœã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™

    Args:
        topics: èª¿æŸ»ãƒˆãƒ”ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        output_file: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    """
    output_data = {
        'metadata': {
            'researched_at': datetime.now().isoformat(),
            'research_method': 'ClaudeCode WebSearch + LLM analysis',
            'total_topics': len(topics),
            'high_priority_count': len([t for t in topics if t['research_priority'] == 'high']),
            'research_categories': [
                'latest_news',
                'fact_check',
                'opposing_views',
                'expert_opinions'
            ]
        },
        'research_findings': {}
    }

    print(f"\nğŸ“Š Research topics prepared:")
    print(f"  - Total topics: {output_data['metadata']['total_topics']}")
    print(f"  - High priority: {output_data['metadata']['high_priority_count']}")
    print(f"  - Research categories: {len(output_data['metadata']['research_categories'])}")

    return output_data, topics


def display_topics(topics: List[Dict[str, Any]]):
    """èª¿æŸ»ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º"""
    print("\n" + "="*70)
    print("ğŸ“ Topics to research")
    print("="*70)

    for topic in topics:
        priority_icon = "ğŸ”¥" if topic['research_priority'] == 'high' else "âš¡" if topic['research_priority'] == 'medium' else "ğŸ’¡"
        print(f"\n{priority_icon} Rank {topic['rank']} ({topic['research_priority']} priority)")
        print(f"  @{topic['username']} (engagement: {topic['engagement_score']})")
        print(f"  {topic['text'][:100]}...")

    print("\n" + "="*70)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / "data"

    # æœ€æ–°ã®top_10_ai_tweets_ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    top_tweets_files = sorted(
        data_dir.glob("top_10_ai_tweets_*.json"),
        key=lambda f: f.stat().st_mtime,
        reverse=True
    )

    if not top_tweets_files:
        print("âŒ Error: No top_10_ai_tweets file found")
        print("   Please run extract_top_tweets.py first")
        sys.exit(1)

    input_file = top_tweets_files[0]

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    date_str = input_file.stem.replace('top_10_ai_tweets_', '')
    output_file = data_dir / f"research_findings_ai_{date_str}.json"

    print("\n" + "="*70)
    print("ğŸ” Research Topic Skill")
    print("="*70)
    print(f"\nğŸ“‚ Input file: {input_file.name}")
    print(f"ğŸ“‚ Output file: {output_file.name}")

    # STEP 1: ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    topics = extract_research_topics(input_file)

    # STEP 2: ãƒˆãƒ”ãƒƒã‚¯è¡¨ç¤º
    display_topics(topics)

    # STEP 3: èª¿æŸ»å®Ÿè¡Œã®æº–å‚™
    output_data, topics = prepare_research_output(topics, output_file)

    # STEP 4: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆèª¿æŸ»çµæœã¯ClaudeCode LLMã§è¿½è¨˜ï¼‰
    print(f"\nğŸ’¾ Writing prepared structure to: {output_file}")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±ã‚’å«ã‚ã¦ä¿å­˜
    output_data['topics'] = topics

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print("âœ… Research preparation completed")
    print("\nğŸ“Œ Next: ClaudeCode LLM will execute WebSearch and fill research_findings")
    print(f"   Total topics to research: {len(topics)}")
    print(f"   High priority topics: {len([t for t in topics if t['research_priority'] == 'high'])}")

    return output_data, topics


if __name__ == "__main__":
    main()
