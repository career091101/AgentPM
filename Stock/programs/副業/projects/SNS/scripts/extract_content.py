#!/usr/bin/env python3
"""
Extract Content Skill Implementation
è¨˜äº‹ãƒ»YouTubeãƒ»PDFã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
"""

import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse

# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: requests or beautifulsoup4 not installed")
    print("   Install with: pip install requests beautifulsoup4")
    REQUESTS_AVAILABLE = False


def extract_article_content(url: str) -> Optional[Dict[str, Any]]:
    """
    è¨˜äº‹URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º

    Args:
        url: è¨˜äº‹URL

    Returns:
        æŠ½å‡ºçµæœã®è¾æ›¸ã€å¤±æ•—æ™‚ã¯None
    """
    if not REQUESTS_AVAILABLE:
        return None

    try:
        print(f"  â†’ Fetching: {url[:60]}...")

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
        title = None
        if soup.find('title'):
            title = soup.find('title').text.strip()
        elif soup.find('h1'):
            title = soup.find('h1').text.strip()

        # æœ¬æ–‡æŠ½å‡ºï¼ˆä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        content_text = ""

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: article ã‚¿ã‚°
        article = soup.find('article')
        if article:
            paragraphs = article.find_all('p')
            content_text = '\n\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: main ã‚¿ã‚°
        if not content_text:
            main = soup.find('main')
            if main:
                paragraphs = main.find_all('p')
                content_text = '\n\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])

        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å…¨ã¦ã®p ã‚¿ã‚°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not content_text:
            paragraphs = soup.find_all('p')
            # é•·ã„ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã®ã¿æŠ½å‡ºï¼ˆåºƒå‘Šãƒ»ãƒŠãƒ“é™¤å¤–ï¼‰
            paragraphs = [p for p in paragraphs if len(p.text.strip()) > 50]
            content_text = '\n\n'.join([p.text.strip() for p in paragraphs[:20]])  # æœ€åˆã®20æ®µè½

        # ãƒ¡ã‚¿æƒ…å ±æŠ½å‡º
        meta_description = None
        meta_tag = soup.find('meta', attrs={'name': 'description'})
        if meta_tag and meta_tag.get('content'):
            meta_description = meta_tag['content']

        result = {
            'url': url,
            'type': 'article',
            'title': title,
            'content': content_text,
            'meta_description': meta_description,
            'word_count': len(content_text.split()) if content_text else 0,
            'extracted_at': datetime.now().isoformat(),
            'status': 'success'
        }

        print(f"  âœ… Extracted: {len(content_text)} chars, {result['word_count']} words")
        return result

    except requests.exceptions.Timeout:
        print(f"  âŒ Timeout: {url[:60]}")
        return {'url': url, 'type': 'article', 'status': 'timeout', 'error': 'Request timeout'}
    except requests.exceptions.RequestException as e:
        print(f"  âŒ Error: {str(e)[:60]}")
        return {'url': url, 'type': 'article', 'status': 'error', 'error': str(e)}
    except Exception as e:
        print(f"  âŒ Unexpected error: {str(e)[:60]}")
        return {'url': url, 'type': 'article', 'status': 'error', 'error': str(e)}


def extract_youtube_content(url: str) -> Optional[Dict[str, Any]]:
    """
    YouTube URLã‹ã‚‰å­—å¹•ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

    Note: ä»Šå›ã¯å®Ÿè£…ã‚¹ã‚­ãƒƒãƒ—ï¼ˆyoutube-transcript-apiæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
    """
    print(f"  âš ï¸  YouTube extraction not implemented yet: {url[:60]}")
    return {
        'url': url,
        'type': 'youtube',
        'status': 'not_implemented',
        'error': 'YouTube extraction requires youtube-transcript-api'
    }


def extract_pdf_content(url: str) -> Optional[Dict[str, Any]]:
    """
    PDF URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º

    Note: ä»Šå›ã¯å®Ÿè£…ã‚¹ã‚­ãƒƒãƒ—ï¼ˆpdfplumberæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
    """
    print(f"  âš ï¸  PDF extraction not implemented yet: {url[:60]}")
    return {
        'url': url,
        'type': 'pdf',
        'status': 'not_implemented',
        'error': 'PDF extraction requires pdfplumber'
    }


def extract_content(tweet_details_file: Path, output_file: Path) -> Dict[str, Any]:
    """
    ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ã‹ã‚‰ãƒªãƒ³ã‚¯ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º

    Args:
        tweet_details_file: ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°JSONãƒ•ã‚¡ã‚¤ãƒ«
        output_file: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        å‡¦ç†çµæœã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    """
    # STEP 1: ãƒ„ã‚¤ãƒ¼ãƒˆè©³ç´°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“– Reading tweet details from: {tweet_details_file}")

    try:
        with open(tweet_details_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ Error: File not found: {tweet_details_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Error: Invalid JSON format: {e}")
        sys.exit(1)

    tweet_details = data.get('tweet_details', [])
    print(f"âœ… Loaded {len(tweet_details)} tweet details")

    # STEP 2: å…¨ãƒªãƒ³ã‚¯ã‚’åé›†
    all_links = []
    for detail in tweet_details:
        links = detail.get('links', [])
        for link in links:
            link['tweet_id'] = detail['tweet_id']
            link['username'] = detail['username']
            all_links.append(link)

    print(f"\nğŸ”— Total links to extract: {len(all_links)}")

    if len(all_links) == 0:
        print("âš ï¸  No links found in tweet details")
        sys.exit(0)

    # ãƒªãƒ³ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    link_types = {}
    for link in all_links:
        link_type = link['type']
        link_types[link_type] = link_types.get(link_type, 0) + 1

    print(f"Link types:")
    for link_type, count in sorted(link_types.items()):
        print(f"  - {link_type}: {count}")

    # STEP 3: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
    print(f"\nğŸ“ Extracting content from {len(all_links)} links...")

    extracted_contents = []
    success_count = 0
    error_count = 0

    for i, link in enumerate(all_links, 1):
        print(f"\n[{i}/{len(all_links)}] Processing {link['type']}: {link['domain']}")

        link_type = link['type']
        url = link['url']

        # ã‚¿ã‚¤ãƒ—åˆ¥ã«æŠ½å‡ºé–¢æ•°ã‚’é¸æŠ
        if link_type == 'article':
            content = extract_article_content(url)
        elif link_type == 'youtube':
            content = extract_youtube_content(url)
        elif link_type == 'pdf':
            content = extract_pdf_content(url)
        else:
            content = {
                'url': url,
                'type': link_type,
                'status': 'unsupported',
                'error': f'Unsupported link type: {link_type}'
            }

        if content:
            content['tweet_id'] = link['tweet_id']
            content['username'] = link['username']
            content['domain'] = link['domain']
            extracted_contents.append(content)

            if content.get('status') == 'success':
                success_count += 1
            else:
                error_count += 1

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆå¾…æ©Ÿï¼‰
        if i < len(all_links):
            wait_time = 2  # 2ç§’å¾…æ©Ÿ
            time.sleep(wait_time)

    # STEP 4: çµæœé›†è¨ˆ
    print(f"\nâœ… Content extraction completed")
    print(f"  - Success: {success_count}/{len(all_links)}")
    print(f"  - Errors: {error_count}/{len(all_links)}")

    # STEP 5: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    output_data = {
        'metadata': {
            'processed_at': datetime.now().isoformat(),
            'source_file': tweet_details_file.name,
            'total_links': len(all_links),
            'success_count': success_count,
            'error_count': error_count,
            'link_types': link_types
        },
        'extracted_contents': extracted_contents
    }

    print(f"\nğŸ’¾ Writing output to: {output_file}")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print("âœ… Output file created successfully")

    return output_data


def display_summary(output_data: Dict[str, Any]):
    """å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    metadata = output_data['metadata']
    extracted_contents = output_data['extracted_contents']

    print("\n" + "="*70)
    print("âœ… Content extraction completed")
    print("="*70)

    print(f"\nğŸ“Š Summary:")
    print(f"  - Total links processed: {metadata['total_links']}")
    print(f"  - Success: {metadata['success_count']}")
    print(f"  - Errors: {metadata['error_count']}")

    if metadata['success_count'] > 0:
        success_rate = metadata['success_count'] / metadata['total_links'] * 100
        print(f"  - Success rate: {success_rate:.1f}%")

    # æˆåŠŸã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµ±è¨ˆ
    successful_contents = [c for c in extracted_contents if c.get('status') == 'success']

    if successful_contents:
        total_words = sum(c.get('word_count', 0) for c in successful_contents)
        avg_words = total_words / len(successful_contents) if successful_contents else 0

        print(f"\nğŸ“ Content statistics:")
        print(f"  - Total words extracted: {total_words:,}")
        print(f"  - Average words per article: {avg_words:.0f}")

        print(f"\nğŸ† Top 3 longest articles:")
        sorted_contents = sorted(successful_contents, key=lambda c: c.get('word_count', 0), reverse=True)
        for i, content in enumerate(sorted_contents[:3], 1):
            title = content.get('title', 'No title')[:50]
            word_count = content.get('word_count', 0)
            print(f"  {i}. {title}... ({word_count} words)")

    print("\n" + "="*70)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / "data"

    # æœ€æ–°ã®tweet_details_ai_ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    tweet_details_files = sorted(
        data_dir.glob("tweet_details_ai_*.json"),
        key=lambda f: f.stat().st_mtime,
        reverse=True
    )

    if not tweet_details_files:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: tweet_details_*.json
        tweet_details_files = sorted(
            data_dir.glob("tweet_details_*.json"),
            key=lambda f: f.stat().st_mtime,
            reverse=True
        )

    if not tweet_details_files:
        print("âŒ Error: No tweet_details file found")
        print("   Please run scrape_tweet_details.py first")
        sys.exit(1)

    input_file = tweet_details_files[0]

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    date_str = input_file.stem.replace('tweet_details_', '').replace('tweet_details_ai_', '')
    output_file = data_dir / f"extracted_contents_{date_str}.json"

    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå®Ÿè¡Œ
    output_data = extract_content(input_file, output_file)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    display_summary(output_data)


if __name__ == "__main__":
    main()
