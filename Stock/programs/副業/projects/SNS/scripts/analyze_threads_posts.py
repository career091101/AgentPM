#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ThreadsæŠ•ç¨¿åˆ†æžã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„: ThreadséŽåŽ»90æ—¥æŠ•ç¨¿ã®åˆ†æž
1. æŠ•ç¨¿æ•°ãƒ»ç·é–²è¦§æ•°
2. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡åˆ†æž
3. ãƒªãƒã‚¹ãƒˆ/å¼•ç”¨ã®å‚¾å‘
4. Top 5 / Bottom 5æŠ•ç¨¿
5. ãƒ†ã‚­ã‚¹ãƒˆé•·ã¨é–²è¦§æ•°ã®ç›¸é–¢
"""

import pandas as pd
import json
from pathlib import Path
from typing import Dict

def analyze_threads_posts(csv_path: str) -> Dict:
    """ThreadsæŠ•ç¨¿ç·åˆåˆ†æž"""

    df = pd.read_csv(csv_path, encoding='utf-8-sig')

    if len(df) == 0:
        return {'error': 'ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™'}

    # æŠ•ç¨¿æ—¥æ™‚ã‚’datetimeåž‹ã«å¤‰æ›
    df['æŠ•ç¨¿æ—¥æ™‚'] = pd.to_datetime(df['æŠ•ç¨¿æ—¥æ™‚'])
    df['æ›œæ—¥'] = df['æŠ•ç¨¿æ—¥æ™‚'].dt.day_name()
    df['æ™‚é–“å¸¯'] = df['æŠ•ç¨¿æ—¥æ™‚'].dt.hour
    df['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°'] = df['ãƒ†ã‚­ã‚¹ãƒˆ'].astype(str).str.len()

    # åŸºæœ¬çµ±è¨ˆ
    total_posts = len(df)
    total_views = int(df['é–²è¦§æ•°'].sum())
    avg_views = float(df['é–²è¦§æ•°'].mean())
    median_views = float(df['é–²è¦§æ•°'].median())
    avg_engagement_rate = float(df['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡'].mean())

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆè©³ç´°çµ±è¨ˆ
    avg_likes = float(df['ã„ã„ã­æ•°'].mean())
    avg_replies = float(df['è¿”ä¿¡æ•°'].mean())
    avg_reposts = float(df['ãƒªãƒã‚¹ãƒˆæ•°'].mean())
    avg_quotes = float(df['å¼•ç”¨æ•°'].mean())
    avg_shares = float(df['ã‚·ã‚§ã‚¢æ•°'].mean())

    # ãƒªãƒã‚¹ãƒˆ/å¼•ç”¨çŽ‡
    repost_rate = (df['ãƒªãƒã‚¹ãƒˆæ•°'].sum() / df['é–²è¦§æ•°'].sum() * 100) if df['é–²è¦§æ•°'].sum() > 0 else 0
    quote_rate = (df['å¼•ç”¨æ•°'].sum() / df['é–²è¦§æ•°'].sum() * 100) if df['é–²è¦§æ•°'].sum() > 0 else 0

    # æ›œæ—¥åˆ¥åˆ†æž
    weekday_analysis = {}
    for weekday in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:
        weekday_df = df[df['æ›œæ—¥'] == weekday]
        if len(weekday_df) > 0:
            weekday_analysis[weekday] = {
                'count': len(weekday_df),
                'avg_views': float(weekday_df['é–²è¦§æ•°'].mean())
            }

    # æ™‚é–“å¸¯åˆ¥åˆ†æžï¼ˆ6æ™‚é–“å˜ä½ï¼‰
    time_slot_analysis = {
        '0-6æ™‚': df[(df['æ™‚é–“å¸¯'] >= 0) & (df['æ™‚é–“å¸¯'] < 6)]['é–²è¦§æ•°'].mean(),
        '6-12æ™‚': df[(df['æ™‚é–“å¸¯'] >= 6) & (df['æ™‚é–“å¸¯'] < 12)]['é–²è¦§æ•°'].mean(),
        '12-18æ™‚': df[(df['æ™‚é–“å¸¯'] >= 12) & (df['æ™‚é–“å¸¯'] < 18)]['é–²è¦§æ•°'].mean(),
        '18-24æ™‚': df[(df['æ™‚é–“å¸¯'] >= 18) & (df['æ™‚é–“å¸¯'] < 24)]['é–²è¦§æ•°'].mean()
    }
    # NaNã‚’0ã«å¤‰æ›
    time_slot_analysis = {k: (float(v) if pd.notna(v) else 0) for k, v in time_slot_analysis.items()}

    # ãƒ†ã‚­ã‚¹ãƒˆé•·ã¨ã®ç›¸é–¢
    text_length_correlation = float(df[['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°', 'é–²è¦§æ•°']].corr().iloc[0, 1])

    # Top 5æŠ•ç¨¿
    top_5 = df.nlargest(5, 'é–²è¦§æ•°')
    top_5_posts = [
        {
            'æŠ•ç¨¿æ—¥æ™‚': str(row['æŠ•ç¨¿æ—¥æ™‚']),
            'é–²è¦§æ•°': int(row['é–²è¦§æ•°']),
            'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡': float(row['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡']),
            'ã„ã„ã­æ•°': int(row['ã„ã„ã­æ•°']),
            'ãƒªãƒã‚¹ãƒˆæ•°': int(row['ãƒªãƒã‚¹ãƒˆæ•°']),
            'å¼•ç”¨æ•°': int(row['å¼•ç”¨æ•°']),
            'ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°': int(row['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°']),
            'ãƒ‘ãƒ¼ãƒžãƒªãƒ³ã‚¯': row['ãƒ‘ãƒ¼ãƒžãƒªãƒ³ã‚¯']
        }
        for _, row in top_5.iterrows()
    ]

    # Bottom 5æŠ•ç¨¿
    bottom_5 = df.nsmallest(5, 'é–²è¦§æ•°')
    bottom_5_posts = [
        {
            'æŠ•ç¨¿æ—¥æ™‚': str(row['æŠ•ç¨¿æ—¥æ™‚']),
            'é–²è¦§æ•°': int(row['é–²è¦§æ•°']),
            'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡': float(row['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡']),
            'ã„ã„ã­æ•°': int(row['ã„ã„ã­æ•°']),
            'ãƒªãƒã‚¹ãƒˆæ•°': int(row['ãƒªãƒã‚¹ãƒˆæ•°']),
            'å¼•ç”¨æ•°': int(row['å¼•ç”¨æ•°']),
            'ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°': int(row['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°']),
            'ãƒ‘ãƒ¼ãƒžãƒªãƒ³ã‚¯': row['ãƒ‘ãƒ¼ãƒžãƒªãƒ³ã‚¯']
        }
        for _, row in bottom_5.iterrows()
    ]

    return {
        'basic_stats': {
            'total_posts': total_posts,
            'total_views': total_views,
            'avg_views': avg_views,
            'median_views': median_views,
            'avg_engagement_rate': avg_engagement_rate,
            'avg_likes': avg_likes,
            'avg_replies': avg_replies,
            'avg_reposts': avg_reposts,
            'avg_quotes': avg_quotes,
            'avg_shares': avg_shares
        },
        'engagement_trends': {
            'repost_rate': float(repost_rate),
            'quote_rate': float(quote_rate)
        },
        'weekday_analysis': weekday_analysis,
        'time_slot_analysis': time_slot_analysis,
        'text_length_correlation': text_length_correlation,
        'top_5_posts': top_5_posts,
        'bottom_5_posts': bottom_5_posts
    }

def main():
    base_path = Path('/Users/yuichi/AIPM/aipm_v0/Stock/programs/å‰¯æ¥­/projects/SNS')

    # æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    threads_dir = base_path / 'Threads'
    csv_files = list(threads_dir.glob('threads_*.csv'))

    if not csv_files:
        print("âŒ Threadsãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«fetch_threads_data.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠž
    latest_csv = sorted(csv_files, reverse=True)[0]

    print("=" * 80)
    print("ThreadsæŠ•ç¨¿åˆ†æžãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {latest_csv.name}")
    print()

    result = analyze_threads_posts(str(latest_csv))

    if 'error' in result:
        print(f"ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return

    # åŸºæœ¬çµ±è¨ˆ
    print("ã€åŸºæœ¬çµ±è¨ˆã€‘")
    print("-" * 80)
    bs = result['basic_stats']
    print(f"ç·æŠ•ç¨¿æ•°: {bs['total_posts']}ä»¶")
    print(f"ç·é–²è¦§æ•°: {bs['total_views']:,}")
    print(f"å¹³å‡é–²è¦§æ•°: {bs['avg_views']:.0f}")
    print(f"ä¸­å¤®å€¤: {bs['median_views']:.0f}")
    print(f"å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡: {bs['avg_engagement_rate']:.2f}%")
    print()
    print(f"å¹³å‡ã„ã„ã­æ•°: {bs['avg_likes']:.1f}")
    print(f"å¹³å‡è¿”ä¿¡æ•°: {bs['avg_replies']:.1f}")
    print(f"å¹³å‡ãƒªãƒã‚¹ãƒˆæ•°: {bs['avg_reposts']:.1f}")
    print(f"å¹³å‡å¼•ç”¨æ•°: {bs['avg_quotes']:.1f}")
    print(f"å¹³å‡ã‚·ã‚§ã‚¢æ•°: {bs['avg_shares']:.1f}")
    print()

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‚¾å‘
    print("ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‚¾å‘ã€‘")
    print("-" * 80)
    et = result['engagement_trends']
    print(f"ãƒªãƒã‚¹ãƒˆçŽ‡: {et['repost_rate']:.3f}%")
    print(f"å¼•ç”¨çŽ‡: {et['quote_rate']:.3f}%")
    print()

    # æ›œæ—¥åˆ¥åˆ†æž
    print("ã€æ›œæ—¥åˆ¥åˆ†æžã€‘")
    print("-" * 80)
    for weekday, stats in result['weekday_analysis'].items():
        print(f"{weekday}: {stats['count']}ä»¶ (å¹³å‡é–²è¦§æ•°: {stats['avg_views']:.0f})")
    print()

    # æ™‚é–“å¸¯åˆ¥åˆ†æž
    print("ã€æ™‚é–“å¸¯åˆ¥åˆ†æžã€‘")
    print("-" * 80)
    for time_slot, avg_views in result['time_slot_analysis'].items():
        print(f"{time_slot}: å¹³å‡é–²è¦§æ•° {avg_views:.0f}")
    print()

    # ãƒ†ã‚­ã‚¹ãƒˆé•·ã¨ã®ç›¸é–¢
    print("ã€ãƒ†ã‚­ã‚¹ãƒˆé•·ã¨ã®ç›¸é–¢ã€‘")
    print("-" * 80)
    print(f"ç›¸é–¢ä¿‚æ•°: {result['text_length_correlation']:.3f}")
    print()

    # Top 5æŠ•ç¨¿
    print("ã€Top 5æŠ•ç¨¿ã€‘")
    print("-" * 80)
    for i, post in enumerate(result['top_5_posts'], 1):
        print(f"{i}. {post['æŠ•ç¨¿æ—¥æ™‚']} - {post['é–²è¦§æ•°']:,} views | ER {post['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡']:.2f}%")
        print(f"   ã„ã„ã­: {post['ã„ã„ã­æ•°']} | ãƒªãƒã‚¹ãƒˆ: {post['ãƒªãƒã‚¹ãƒˆæ•°']} | å¼•ç”¨: {post['å¼•ç”¨æ•°']}")
        print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {post['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°']}å­—")
        print(f"   {post['ãƒ‘ãƒ¼ãƒžãƒªãƒ³ã‚¯']}")
        print()

    # Bottom 5æŠ•ç¨¿
    print("ã€Bottom 5æŠ•ç¨¿ã€‘")
    print("-" * 80)
    for i, post in enumerate(result['bottom_5_posts'], 1):
        print(f"{i}. {post['æŠ•ç¨¿æ—¥æ™‚']} - {post['é–²è¦§æ•°']:,} views | ER {post['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçŽ‡']:.2f}%")
        print(f"   ã„ã„ã­: {post['ã„ã„ã­æ•°']} | ãƒªãƒã‚¹ãƒˆ: {post['ãƒªãƒã‚¹ãƒˆæ•°']} | å¼•ç”¨: {post['å¼•ç”¨æ•°']}")
        print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {post['ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—æ•°']}å­—")
        print()

    # JSONå‡ºåŠ›
    output_path = threads_dir / 'threads_analysis_results.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ðŸ’¾ è©³ç´°çµæžœã‚’JSONã§ä¿å­˜: {output_path}")

if __name__ == '__main__':
    main()
