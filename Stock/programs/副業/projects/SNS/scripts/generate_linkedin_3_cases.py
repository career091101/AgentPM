#!/usr/bin/env python3
"""
LinkedInæŠ•ç¨¿3æ¡ˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé«˜é‡ãƒ¡ã‚½ãƒƒãƒ‰æº–æ‹ ï¼‰

Usage:
    from generate_linkedin_3_cases import generate_3_cases
    cases = generate_3_cases()
"""

import os
import json
import sys
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from anthropic import Anthropic

# .envèª­ã¿è¾¼ã¿
project_root = Path(__file__).parent.parent
load_dotenv(project_root / ".env")


def load_latest_research_findings() -> dict:
    """æœ€æ–°ã®ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    data_dir = project_root / "data"

    # research_findings_*.jsonã‹ã‚‰æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    research_files = list(data_dir.glob("research_findings_*_v2_*.json"))

    if not research_files:
        # v2ãŒãªã„å ´åˆã¯é€šå¸¸ç‰ˆ
        research_files = list(data_dir.glob("research_findings_*.json"))

    if not research_files:
        raise FileNotFoundError("research_findings_*.json not found in data/")

    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«
    latest_file = max(research_files, key=lambda f: f.stat().st_mtime)

    print(f"ğŸ“„ Using research file: {latest_file.name}")

    with open(latest_file, "r", encoding="utf-8") as f:
        return json.load(f)


def load_prompt_template() -> str:
    """é«˜é‡ãƒ¡ã‚½ãƒƒãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿"""
    prompt_file = project_root / "æŠ•ç¨¿æ–‡ä½œæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ_v6_takano_refined"

    if not prompt_file.exists():
        raise FileNotFoundError(
            f"Prompt file not found: {prompt_file}"
        )

    return prompt_file.read_text(encoding="utf-8")


def generate_3_cases_with_claude(research_findings: dict) -> list:
    """
    Claude APIçµŒç”±ã§3æ¡ˆã‚’ç”Ÿæˆ

    Args:
        research_findings: ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿

    Returns:
        list: 3æ¡ˆã®ãƒªã‚¹ãƒˆ
            [
                {"type": "æ•°å­—ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆå‹", "content": "...", "hashtags": "..."},
                {"type": "è¡æ’ƒç™ºè¨€å‹", "content": "...", "hashtags": "..."},
                {"type": "å•é¡Œæèµ·å‹", "content": "...", "hashtags": "..."}
            ]
    """
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not found in .env file")

    client = Anthropic(api_key=api_key)

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    system_prompt = load_prompt_template()

    # ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
    key_takeaways = research_findings.get("key_takeaways", {})
    if not key_takeaways:
        raise ValueError("No key_takeaways found in research_findings")

    # key_takeawaysã‹ã‚‰ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’æ§‹æˆ
    topic_text = f"""**æŠ•è³‡å‹•å‘**: {key_takeaways.get('investment_landscape', '')}

**ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ**: {key_takeaways.get('tool_comparison', '')}

**å¸‚å ´æˆé•·**: {key_takeaways.get('market_growth', '')}

**å¸‚å ´ãƒªãƒ¼ãƒ€ãƒ¼**: {key_takeaways.get('market_leaders', '')}

**ç”Ÿç”£æ€§ã®å®Ÿæ…‹**: {key_takeaways.get('productivity_reality', '')}

**2026å¹´äºˆæ¸¬**: {key_takeaways.get('2026_predictions', '')}
"""
    topic_url = "AIé–¢é€£æœ€æ–°ãƒªã‚µãƒ¼ãƒï¼ˆ2026-01-04ï¼‰"

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    user_prompt = f"""ä»¥ä¸‹ã®AIé–¢é€£ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰ã€LinkedInæŠ•ç¨¿ã‚’3æ¡ˆä½œæˆã—ã¦ãã ã•ã„ã€‚

**ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±**:
{topic_text}

**å‡ºå…¸**: {topic_url}

**è¦ä»¶**:
1. **æ¡ˆ1ï¼ˆæ•°å­—ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆå‹ï¼‰**: å…·ä½“çš„æ•°å€¤ãƒ»å€ç‡ãƒ»é‡‘é¡ã‚’æœ€å„ªå…ˆã«ã—ãŸãƒ•ãƒƒã‚¯
2. **æ¡ˆ2ï¼ˆè¡æ’ƒç™ºè¨€å‹ï¼‰**: è‘—åäººã®ç™ºè¨€ã‚„ã€Œãƒã‚¸ã§ã€ã€Œãƒ¤ãƒã„ã€ãªã©ã®å¼·ã„è¡¨ç¾ã‚’ä½¿ã£ãŸãƒ•ãƒƒã‚¯
3. **æ¡ˆ3ï¼ˆå•é¡Œæèµ·å‹ï¼‰**: èª­è€…ã®èª²é¡Œã‚„æ¥­ç•Œã®å¤‰åŒ–ã‚’å•ã„ã‹ã‘ã‚‹å½¢ã®ãƒ•ãƒƒã‚¯

**å‡ºåŠ›å½¢å¼**ï¼ˆJSONï¼‰:
```json
[
  {{
    "type": "æ•°å­—ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆå‹",
    "content": "æŠ•ç¨¿æœ¬æ–‡ï¼ˆ700-900å­—ï¼‰",
    "hashtags": "#AI #ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—"
  }},
  {{
    "type": "è¡æ’ƒç™ºè¨€å‹",
    "content": "æŠ•ç¨¿æœ¬æ–‡ï¼ˆ700-900å­—ï¼‰",
    "hashtags": "#AI #ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼"
  }},
  {{
    "type": "å•é¡Œæèµ·å‹",
    "content": "æŠ•ç¨¿æœ¬æ–‡ï¼ˆ700-900å­—ï¼‰",
    "hashtags": "#AI #çµŒå–¶"
  }}
]
```

**é‡è¦**:
- å„æ¡ˆã¯å¿…ãš700å­—ä»¥ä¸Š
- å¿…ãšå•ã„ã‹ã‘ã§çµ‚ã‚ã‚‹
- ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¯2å€‹ã¾ã§
- å‘¼ã³ã‹ã‘è¡¨ç¾ï¼ˆã€ŒçµŒå–¶è€…ã®ã‚ãªãŸã¸:ã€ç­‰ï¼‰ã¯ç¦æ­¢
"""

    print("ğŸ¤– Claude APIã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆä¸­...")

    # Claude APIå‘¼ã³å‡ºã—
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4096,
        system=system_prompt,
        messages=[{"role": "user", "content": user_prompt}],
    )

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
    response_text = response.content[0].text

    # JSONãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡º
    if "```json" in response_text:
        json_start = response_text.find("```json") + 7
        json_end = response_text.find("```", json_start)
        json_text = response_text[json_start:json_end].strip()
    else:
        json_text = response_text.strip()

    try:
        cases = json.loads(json_text)
        print(f"âœ… 3æ¡ˆç”Ÿæˆå®Œäº†!")
        return cases
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
        print(f"   Response: {response_text}")
        raise


def generate_3_cases() -> list:
    """
    3æ¡ˆã‚’ç”Ÿæˆï¼ˆãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼‰

    Returns:
        list: 3æ¡ˆã®ãƒªã‚¹ãƒˆ
    """
    print("=" * 60)
    print("LinkedInæŠ•ç¨¿ 3æ¡ˆç”Ÿæˆ")
    print("=" * 60)
    print()

    # 1. æœ€æ–°ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    research_findings = load_latest_research_findings()

    # 2. Claude APIçµŒç”±ã§3æ¡ˆç”Ÿæˆ
    cases = generate_3_cases_with_claude(research_findings)

    # 3. çµæœè¡¨ç¤º
    print("\n" + "=" * 60)
    print("ç”Ÿæˆã•ã‚ŒãŸ3æ¡ˆ:")
    print("=" * 60)
    for i, case in enumerate(cases, 1):
        print(f"\nã€æ¡ˆ{i}ã€‘ {case['type']}")
        print(f"æ–‡å­—æ•°: {len(case['content'])}å­—")
        print(f"ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: {case['hashtags']}")
        print(f"\n{case['content'][:200]}...")
        print()

    return cases


if __name__ == "__main__":
    try:
        cases = generate_3_cases()

        # data/posts_generated_3cases_YYYYMMDD.json ã«ä¿å­˜
        output_file = (
            project_root
            / "data"
            / f"posts_generated_3cases_{datetime.now().strftime('%Y%m%d')}.json"
        )
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(cases, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
