#!/usr/bin/env python3
"""
X (Twitter) Timeline Collector - Cursor-based API Interception Version

ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹APIã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ã‚·ãƒ§ãƒ³æ–¹å¼ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åé›†ã‚’è¡Œã†ã€‚
é‡è¤‡ç‡0%ã€10å€é«˜é€Ÿã‚’å®Ÿç¾ã€‚

å®Ÿè¡Œæ–¹æ³•:
    python3 scripts/collect_x_timeline_cursor.py --target 100 --output data/x_timeline_cursor_test.json

å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
    pip install playwright asyncio
    playwright install chromium
"""

import asyncio
import json
import re
import random
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import argparse


class XTimelineCursorCollector:
    """ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹APIã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ã‚·ãƒ§ãƒ³æ–¹å¼ã§Xã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åé›†"""

    def __init__(self, target_count: int = 100):
        self.target_count = target_count
        self.collected_tweets = []
        self.seen_tweet_ids = set()
        self.cursors = []
        self.api_responses = []

    async def collect(self, url: str = "https://x.com/home", cookies_file: str = None) -> List[Dict]:
        """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        from playwright.async_api import async_playwright

        print(f"ğŸš€ ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹APIåé›†é–‹å§‹ï¼ˆç›®æ¨™: {self.target_count}ä»¶ï¼‰")
        print(f"ğŸ“ URL: {url}")

        async with async_playwright() as p:
            # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼‰
            browser = await p.chromium.launch(
                headless=True,  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ
                args=['--disable-blink-features=AutomationControlled']
            )

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
            context = await browser.new_context(
                viewport={'width': 1280, 'height': 720},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )

            # ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿
            if cookies_file and Path(cookies_file).exists():
                print(f"ğŸª ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿ä¸­: {cookies_file}")
                with open(cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                await context.add_cookies(cookies)
                print(f"   âœ… {len(cookies)}ä»¶ã®ã‚¯ãƒƒã‚­ãƒ¼ã‚’è¨­å®š")

            page = await context.new_page()

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            page.on("response", lambda response: asyncio.create_task(
                self._handle_response(response)
            ))

            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
            print("ğŸ“„ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ä¸­...")
            try:
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            except Exception as e:
                print(f"   âš ï¸ ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç„¡è¦–ã—ã¦ç¶šè¡Œï¼‰: {e}")

            # åˆæœŸãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿ
            await asyncio.sleep(5)

            print(f"âœ… åˆæœŸãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚APIå‚å—é–‹å§‹...")

            # ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹åé›†ãƒ«ãƒ¼ãƒ—
            iteration = 0
            while len(self.collected_tweets) < self.target_count:
                iteration += 1
                print(f"\nğŸ“Š Iteration {iteration}: åé›†æ¸ˆã¿ {len(self.collected_tweets)}/{self.target_count}ä»¶")

                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦æ–°ã—ã„APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒˆãƒªã‚¬ãƒ¼
                await page.evaluate("window.scrollBy(0, 2000)")

                # å¾…æ©Ÿæ™‚é–“ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ï¼ˆãƒœãƒƒãƒˆæ¤œçŸ¥å›é¿ï¼‰
                wait_time = random.uniform(3, 6)
                await asyncio.sleep(wait_time)

                # æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—å›é¿ï¼‰
                if iteration > 150:
                    print("âš ï¸ æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã«åˆ°é”ã€‚åé›†çµ‚äº†ã€‚")
                    break

                # åé›†ãŒé€²ã¾ãªã„å ´åˆã®è„±å‡º
                if iteration > 10 and len(self.collected_tweets) == 0:
                    print("âŒ 10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã‚‚ãƒ„ã‚¤ãƒ¼ãƒˆåé›†ãªã—ã€‚çµ‚äº†ã€‚")
                    break

            await browser.close()

        print(f"\nâœ… åé›†å®Œäº†: {len(self.collected_tweets)}ä»¶ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯: {len(self.seen_tweet_ids)}ä»¶ï¼‰")
        return self.collected_tweets

    async def _handle_response(self, response):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†"""
        url = response.url

        # GraphQL APIã®HomeTimelineç³»ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡º
        if 'HomeTimeline' in url or 'HomeLatestTimeline' in url:
            try:
                # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
                json_data = await response.json()

                print(f"ğŸ” APIæ¤œå‡º: {url[:100]}...")

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜
                self.api_responses.append({
                    'url': url,
                    'timestamp': datetime.now().isoformat(),
                    'data': json_data
                })

                # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                tweets = self._extract_tweets_from_graphql(json_data)

                if tweets:
                    new_count = 0
                    for tweet in tweets:
                        tweet_id = tweet.get('tweet_id')
                        if tweet_id and tweet_id not in self.seen_tweet_ids:
                            self.collected_tweets.append(tweet)
                            self.seen_tweet_ids.add(tweet_id)
                            new_count += 1

                    print(f"   âœ… {new_count}ä»¶ã®æ–°è¦ãƒ„ã‚¤ãƒ¼ãƒˆè¿½åŠ ")

                # ã‚«ãƒ¼ã‚½ãƒ«å€¤ã‚’æŠ½å‡º
                cursor = self._extract_cursor(json_data)
                if cursor:
                    self.cursors.append(cursor)
                    print(f"   ğŸ“Œ ã‚«ãƒ¼ã‚½ãƒ«å–å¾—: {cursor[:50]}...")

            except Exception as e:
                print(f"   âš ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _extract_tweets_from_graphql(self, data: Dict) -> List[Dict]:
        """GraphQLãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        tweets = []

        try:
            # data.home.home_timeline_urt.instructions ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            instructions = None

            if 'data' in data:
                if 'home' in data['data']:
                    timeline = data['data']['home'].get('home_timeline_urt', {})
                    instructions = timeline.get('instructions', [])
                elif 'user' in data['data']:
                    # UserTweetsã®å ´åˆ
                    user_result = data['data']['user'].get('result', {})
                    timeline = user_result.get('timeline_v2', {}).get('timeline', {})
                    instructions = timeline.get('instructions', [])

            if not instructions:
                return tweets

            # instructionsã‹ã‚‰"TimelineAddEntries"ã‚’æ¢ã™
            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])

                    for entry in entries:
                        # tweet-XXX å½¢å¼ã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿å‡¦ç†
                        entry_id = entry.get('entryId', '')
                        if not entry_id.startswith('tweet-'):
                            continue

                        # content.itemContent.tweet_results.result
                        content = entry.get('content', {})
                        item_content = content.get('itemContent', {})
                        tweet_results = item_content.get('tweet_results', {})
                        result = tweet_results.get('result', {})

                        if not result:
                            continue

                        # legacy ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                        legacy = result.get('legacy', {})
                        if not legacy:
                            continue

                        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                        tweet_id = legacy.get('id_str', '')
                        if not tweet_id:
                            continue

                        # ãƒªãƒ„ã‚¤ãƒ¼ãƒˆãƒ»ãƒªãƒ—ãƒ©ã‚¤é™¤å¤–
                        if legacy.get('retweeted_status'):
                            continue
                        if legacy.get('in_reply_to_status_id_str'):
                            continue

                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
                        core = result.get('core', {})
                        user_results = core.get('user_results', {})
                        user_result = user_results.get('result', {})

                        # screen_nameã¯ user_result.core.screen_name ã«ã‚ã‚‹
                        user_core = user_result.get('core', {})
                        username = user_core.get('screen_name', 'unknown')

                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: legacyã‹ã‚‰ã‚‚è©¦ã™
                        if username == 'unknown':
                            user_legacy = user_result.get('legacy', {})
                            username = user_legacy.get('screen_name', 'unknown')

                        tweet = {
                            'tweet_id': tweet_id,
                            'username': username,
                            'text': legacy.get('full_text', ''),
                            'likes': legacy.get('favorite_count', 0),
                            'retweets': legacy.get('retweet_count', 0),
                            'replies': legacy.get('reply_count', 0),
                            'timestamp_text': legacy.get('created_at', ''),
                            'collected_at': datetime.now().isoformat()
                        }

                        tweets.append(tweet)

        except Exception as e:
            print(f"   âš ï¸ ãƒ„ã‚¤ãƒ¼ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return tweets

    def _extract_cursor(self, data: Dict) -> Optional[str]:
        """GraphQLãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚«ãƒ¼ã‚½ãƒ«å€¤ã‚’æŠ½å‡º"""
        try:
            instructions = None

            if 'data' in data:
                if 'home' in data['data']:
                    timeline = data['data']['home'].get('home_timeline_urt', {})
                    instructions = timeline.get('instructions', [])
                elif 'user' in data['data']:
                    user_result = data['data']['user'].get('result', {})
                    timeline = user_result.get('timeline_v2', {}).get('timeline', {})
                    instructions = timeline.get('instructions', [])

            if not instructions:
                return None

            # æœ€å¾Œã®instructionã®æœ€å¾Œã®entryã‹ã‚‰cursorã‚’å–å¾—
            for instruction in instructions:
                if instruction.get('type') == 'TimelineAddEntries':
                    entries = instruction.get('entries', [])

                    # æœ€å¾Œã®ã‚¨ãƒ³ãƒˆãƒªã‚’æ¢ã™ï¼ˆcursor-bottom-XXXï¼‰
                    for entry in reversed(entries):
                        entry_id = entry.get('entryId', '')
                        if 'cursor-bottom' in entry_id or 'cursor-showmorethreads' in entry_id:
                            content = entry.get('content', {})
                            cursor_value = content.get('value', '')
                            if cursor_value:
                                return cursor_value

        except Exception as e:
            print(f"   âš ï¸ ã‚«ãƒ¼ã‚½ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return None

    def save_results(self, output_path: str):
        """åé›†çµæœã‚’JSONå½¢å¼ã§ä¿å­˜"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        result = {
            'collected_at': datetime.now().isoformat(),
            'total_tweets': len(self.collected_tweets),
            'unique_tweets': len(self.seen_tweet_ids),
            'cursors_collected': len(self.cursors),
            'tweets': self.collected_tweets
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ çµæœä¿å­˜: {output_file}")
        print(f"   - ç·åé›†æ•°: {result['total_tweets']}ä»¶")
        print(f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯: {result['unique_tweets']}ä»¶")
        print(f"   - ã‚«ãƒ¼ã‚½ãƒ«æ•°: {result['cursors_collected']}å€‹")

    def save_debug_data(self, output_dir: str):
        """ãƒ‡ãƒãƒƒã‚°ç”¨ã«APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜"""
        debug_dir = Path(output_dir)
        debug_dir.mkdir(parents=True, exist_ok=True)

        # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¿å­˜
        api_file = debug_dir / 'api_responses.json'
        with open(api_file, 'w', encoding='utf-8') as f:
            json.dump(self.api_responses, f, ensure_ascii=False, indent=2)

        print(f"ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {api_file}")


async def main():
    parser = argparse.ArgumentParser(description='X Timeline Cursor-based Collector')
    parser.add_argument('--target', type=int, default=100, help='ç›®æ¨™åé›†ä»¶æ•°')
    parser.add_argument('--output', type=str, default='data/x_timeline_cursor_test.json', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆAPIãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¿å­˜ï¼‰')
    parser.add_argument('--url', type=str, default='https://x.com/home', help='åé›†URLï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãŠã™ã™ã‚ã‚¿ãƒ–ï¼‰')
    parser.add_argument('--cookies', type=str, default='data/x_cookies.json', help='ã‚¯ãƒƒã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')

    args = parser.parse_args()

    collector = XTimelineCursorCollector(target_count=args.target)

    # åé›†å®Ÿè¡Œ
    await collector.collect(url=args.url, cookies_file=args.cookies)

    # çµæœä¿å­˜
    collector.save_results(args.output)

    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
    if args.debug:
        output_path = Path(args.output)
        debug_dir = output_path.parent / f"{output_path.stem}_debug"
        collector.save_debug_data(str(debug_dir))


if __name__ == '__main__':
    asyncio.run(main())
