#!/usr/bin/env python3
"""
Extract Top Tweets Skill Implementation
ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦Top 10ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æŠ½å‡º
æ—¥æœ¬äºº7å‰²ã€å¤–å›½äºº3å‰²ã®æ¯”ç‡ã§æŠ½å‡º
"""

import json
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple

# ä¸–ç•Œçš„è‘—åäººã®é™¤å¤–ãƒªã‚¹ãƒˆ
EXCLUDED_USERNAMES = [
    'elonmusk',
    'billgates',
    'barackobama',
    'tim_cook',
    'realdonaldtrump',
    'jeffbezos',
    'sundarPichai',
    'satyanadella'
]

# AIé–¢é€£åˆ¤å®šã¯ClaudeCode LLMã§å®Ÿè¡Œ
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã®ã¿

def calculate_engagement_score(tweet: Dict[str, Any]) -> int:
    """
    ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    engagement_score = likes + (retweets Ã— 3) + (replies Ã— 5)
    """
    likes = tweet.get('likes', 0)
    retweets = tweet.get('retweets', 0)
    replies = tweet.get('replies', 0)

    # è² ã®å€¤ã‚’0ã¨ã—ã¦æ‰±ã†ï¼ˆç•°å¸¸ãƒ‡ãƒ¼ã‚¿ï¼‰
    likes = max(0, likes)
    retweets = max(0, retweets)
    replies = max(0, replies)

    return likes + (retweets * 3) + (replies * 5)


def filter_famous_accounts(tweets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ä¸–ç•Œçš„è‘—åäººã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’é™¤å¤–"""
    excluded_lower = [u.lower() for u in EXCLUDED_USERNAMES]
    filtered = [
        tweet for tweet in tweets
        if tweet.get('username', '').lower() not in excluded_lower
    ]
    return filtered


def is_japanese_tweet(tweet: Dict[str, Any]) -> bool:
    """
    ãƒ„ã‚¤ãƒ¼ãƒˆãŒæ—¥æœ¬èªã‹ã©ã†ã‹ã‚’åˆ¤å®š

    åˆ¤å®šåŸºæº–:
    - ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã«æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ãŒå«ã¾ã‚Œã‚‹
    - æ—¥æœ¬èªæ–‡å­—ãŒå…¨ä½“ã®20%ä»¥ä¸Šã‚’å ã‚ã‚‹

    Args:
        tweet: ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿

    Returns:
        æ—¥æœ¬èªãƒ„ã‚¤ãƒ¼ãƒˆã®å ´åˆTrue
    """
    text = tweet.get('text', '')

    if not text:
        return False

    # æ—¥æœ¬èªæ–‡å­—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰
    japanese_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]')

    # æ—¥æœ¬èªæ–‡å­—ã‚’æŠ½å‡º
    japanese_chars = japanese_pattern.findall(text)
    japanese_count = len(japanese_chars)
    total_chars = len(text.replace(' ', '').replace('\n', ''))  # ç©ºç™½ãƒ»æ”¹è¡Œã‚’é™¤ã

    if total_chars == 0:
        return False

    # æ—¥æœ¬èªæ–‡å­—ãŒ20%ä»¥ä¸Šå«ã¾ã‚Œã¦ã„ã‚Œã°Japaeseåˆ¤å®š
    japanese_ratio = japanese_count / total_chars
    return japanese_ratio >= 0.2


def split_japanese_foreign_tweets(tweets: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ—¥æœ¬äººã¨å¤–å›½äººã«åˆ†é¡

    Args:
        tweets: ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ

    Returns:
        (æ—¥æœ¬äººãƒ„ã‚¤ãƒ¼ãƒˆ, å¤–å›½äººãƒ„ã‚¤ãƒ¼ãƒˆ)ã®ã‚¿ãƒ—ãƒ«
    """
    japanese_tweets = []
    foreign_tweets = []

    for tweet in tweets:
        if is_japanese_tweet(tweet):
            tweet['is_japanese'] = True
            japanese_tweets.append(tweet)
        else:
            tweet['is_japanese'] = False
            foreign_tweets.append(tweet)

    return japanese_tweets, foreign_tweets


def extract_top_tweets(input_file: Path, output_file: Path, top_n: int = 10) -> Dict[str, Any]:
    """
    ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Top Nãƒ„ã‚¤ãƒ¼ãƒˆã‚’æŠ½å‡º

    Args:
        input_file: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_file: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        top_n: æŠ½å‡ºä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰

    Returns:
        å‡¦ç†çµæœã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    """
    # STEP 1: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“– Reading timeline data from: {input_file}")

    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ Error: File not found: {input_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Error: Invalid JSON format: {e}")
        sys.exit(1)

    tweets = data.get('tweets', [])
    total_tweets = len(tweets)

    if total_tweets == 0:
        print("âš ï¸  Warning: No tweets found in the data")
        sys.exit(1)

    print(f"âœ… Loaded {total_tweets} tweets")

    # STEP 2: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—
    print("\nğŸ”¢ Calculating engagement scores...")
    for tweet in tweets:
        tweet['engagement_score'] = calculate_engagement_score(tweet)

    # STEP 3: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆä¸–ç•Œçš„è‘—åäººé™¤å¤–ï¼‰
    print("\nğŸ” Filtering famous accounts...")
    initial_count = len(tweets)
    filtered_tweets = filter_famous_accounts(tweets)
    filtered_count = len(filtered_tweets)
    excluded_count = initial_count - filtered_count

    print(f"âœ… Filtered: {filtered_count} tweets (excluded {excluded_count} famous accounts)")

    # STEP 4: æ—¥æœ¬äººãƒ»å¤–å›½äººã®åˆ†é¡
    print(f"\nğŸŒ Classifying Japanese/Foreign tweets...")
    japanese_tweets, foreign_tweets = split_japanese_foreign_tweets(filtered_tweets)

    print(f"âœ… Japanese tweets: {len(japanese_tweets)}")
    print(f"âœ… Foreign tweets: {len(foreign_tweets)}")

    # STEP 5: å„ã‚«ãƒ†ã‚´ãƒªå†…ã§ã‚½ãƒ¼ãƒˆï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢é™é †ï¼‰
    japanese_sorted = sorted(
        japanese_tweets,
        key=lambda t: (t['engagement_score'], t.get('timestamp_text', '')),
        reverse=True
    )

    foreign_sorted = sorted(
        foreign_tweets,
        key=lambda t: (t['engagement_score'], t.get('timestamp_text', '')),
        reverse=True
    )

    # STEP 6: 7:3æ¯”ç‡ã§æŠ½å‡ºï¼ˆæ—¥æœ¬äºº7ä»¶ã€å¤–å›½äºº3ä»¶ï¼‰
    print(f"\nğŸ† Extracting top {top_n} tweets (7 Japanese, 3 Foreign)...")

    japanese_count = int(top_n * 0.7)  # 7ä»¶
    foreign_count = top_n - japanese_count  # 3ä»¶

    top_japanese = japanese_sorted[:japanese_count]
    top_foreign = foreign_sorted[:foreign_count]

    # ä¸è¶³åˆ†ã®èª¿æ•´ï¼ˆæ—¥æœ¬äººã¾ãŸã¯å¤–å›½äººãŒè¶³ã‚Šãªã„å ´åˆï¼‰
    if len(top_japanese) < japanese_count:
        shortage = japanese_count - len(top_japanese)
        print(f"âš ï¸  Warning: Only {len(top_japanese)} Japanese tweets available (need {japanese_count})")
        # å¤–å›½äººã‹ã‚‰ä¸è¶³åˆ†ã‚’è£œå……
        top_foreign = foreign_sorted[:foreign_count + shortage]

    if len(top_foreign) < foreign_count:
        shortage = foreign_count - len(top_foreign)
        print(f"âš ï¸  Warning: Only {len(top_foreign)} Foreign tweets available (need {foreign_count})")
        # æ—¥æœ¬äººã‹ã‚‰ä¸è¶³åˆ†ã‚’è£œå……
        top_japanese = japanese_sorted[:japanese_count + shortage]

    # åˆè¨ˆTop 10ãƒ„ã‚¤ãƒ¼ãƒˆ
    top_tweets = top_japanese + top_foreign

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢é †ã«å†ã‚½ãƒ¼ãƒˆï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ï¼‰
    top_tweets = sorted(
        top_tweets,
        key=lambda t: (t['engagement_score'], t.get('timestamp_text', '')),
        reverse=True
    )

    actual_count = len(top_tweets)
    actual_japanese_count = len([t for t in top_tweets if t.get('is_japanese')])
    actual_foreign_count = actual_count - actual_japanese_count

    print(f"âœ… Extracted {actual_count} tweets ({actual_japanese_count} Japanese, {actual_foreign_count} Foreign)")

    if actual_count < top_n:
        print(f"âš ï¸  Warning: Only {actual_count} tweets available (less than {top_n})")

    # STEP 7: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸
    print("\nğŸ“ Adding metadata...")

    # Top tweetsã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨URLã‚’ä»˜ä¸
    for rank, tweet in enumerate(top_tweets, start=1):
        tweet['rank'] = rank
        username = tweet.get('username', '')
        tweet_id = tweet.get('tweet_id', '')
        tweet['url'] = f"https://x.com/{username}/status/{tweet_id}"

    # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    output_data = {
        "metadata": {
            "processed_at": datetime.now().isoformat(),
            "source_file": input_file.name,
            "total_tweets": total_tweets,
            "filtered_tweets": filtered_count,
            "top_tweets_count": actual_count,
            "japanese_tweets_count": actual_japanese_count,
            "foreign_tweets_count": actual_foreign_count,
            "japanese_ratio": actual_japanese_count / actual_count if actual_count > 0 else 0,
            "filter_criteria": {
                "excluded_usernames": EXCLUDED_USERNAMES,
                "target_japanese_ratio": 0.7,
                "min_engagement_score": top_tweets[-1]['engagement_score'] if top_tweets else 0
            }
        },
        "top_tweets": top_tweets
    }

    # STEP 6: ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    print(f"\nğŸ’¾ Writing output to: {output_file}")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print("âœ… Output file created successfully")

    # STEP 8: å“è³ªæ¤œè¨¼
    print("\nâœ… Quality validation:")

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢å¦¥å½“æ€§
    all_scores_valid = all(t['engagement_score'] >= 0 for t in top_tweets)
    print(f"  - All engagement scores valid: {all_scores_valid}")

    # ã‚¹ã‚³ã‚¢ãŒé™é †ã«ä¸¦ã‚“ã§ã„ã‚‹
    scores = [t['engagement_score'] for t in top_tweets]
    is_sorted = all(scores[i] >= scores[i+1] for i in range(len(scores)-1))
    print(f"  - Scores are sorted: {is_sorted}")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    tweet_ids = [t['tweet_id'] for t in top_tweets]
    no_duplicates = len(tweet_ids) == len(set(tweet_ids))
    print(f"  - No duplicate tweet IDs: {no_duplicates}")

    # è‘—åäººé™¤å¤–ç¢ºèª
    usernames_lower = [t.get('username', '').lower() for t in top_tweets]
    excluded_lower = [u.lower() for u in EXCLUDED_USERNAMES]
    no_famous = not any(u in excluded_lower for u in usernames_lower)
    print(f"  - No famous accounts included: {no_famous}")

    # URLå½¢å¼ç¢ºèª
    all_urls_valid = all(t['url'].startswith('https://x.com/') for t in top_tweets)
    print(f"  - All URLs valid: {all_urls_valid}")

    # æ—¥æœ¬äºº/å¤–å›½äººæ¯”ç‡ç¢ºèª
    japanese_ratio_check = actual_japanese_count >= int(top_n * 0.6)  # 60%ä»¥ä¸Šãªã‚‰OKï¼ˆè¨±å®¹ç¯„å›²ï¼‰
    print(f"  - Japanese ratio acceptable (â‰¥60%): {japanese_ratio_check} ({actual_japanese_count}/{actual_count} = {actual_japanese_count/actual_count*100:.1f}%)")

    return output_data


def display_summary(output_data: Dict[str, Any]):
    """å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    metadata = output_data['metadata']
    top_tweets = output_data['top_tweets']

    print("\n" + "="*60)
    print("âœ… Top 10 tweets extracted successfully (7:3 ratio)")
    print("="*60)

    print("\nğŸ“Š Summary:")
    print(f"  - Total tweets processed: {metadata['total_tweets']}")
    print(f"  - Filtered tweets: {metadata['filtered_tweets']} (excluded {metadata['total_tweets'] - metadata['filtered_tweets']} famous accounts)")
    print(f"  - Japanese tweets: {metadata['japanese_tweets_count']}/{metadata['top_tweets_count']} ({metadata['japanese_ratio']*100:.1f}%)")
    print(f"  - Foreign tweets: {metadata['foreign_tweets_count']}/{metadata['top_tweets_count']} ({(1-metadata['japanese_ratio'])*100:.1f}%)")

    scores = [t['engagement_score'] for t in top_tweets]
    print(f"  - Top {len(top_tweets)} engagement scores: {', '.join(map(str, scores))}")

    if scores:
        avg_score = sum(scores) / len(scores)
        print(f"  - Average engagement score (Top {len(top_tweets)}): {avg_score:.1f}")

    print(f"  - Output file: {metadata['source_file'].replace('x_timeline_', 'top_10_tweets_')}")

    print("\nğŸ† Top 3 Preview:")
    for i, tweet in enumerate(top_tweets[:3], start=1):
        username = tweet.get('username', 'unknown')
        score = tweet['engagement_score']
        is_japanese = tweet.get('is_japanese', False)
        lang_flag = "ğŸ‡¯ğŸ‡µ" if is_japanese else "ğŸŒ"
        text = tweet.get('text', '')[:50] + "..." if len(tweet.get('text', '')) > 50 else tweet.get('text', '')
        print(f"{i}. {lang_flag} @{username} ({score} pts) - \"{text}\"")

    print("\n" + "="*60)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹è¨­å®š
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / "data"

    # æœ€æ–°ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ—¥ä»˜å½¢å¼YYYYMMDDã§å§‹ã¾ã‚‹ã‚‚ã®å„ªå…ˆï¼‰
    timeline_files = list(data_dir.glob("x_timeline_202*.json"))

    if not timeline_files:
        print("âš ï¸  No timeline files with date format found, trying all timeline files...")
        timeline_files = list(data_dir.glob("x_timeline_*.json"))

    if not timeline_files:
        print("âŒ Error: No timeline data found")
        sys.exit(1)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ã®ã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
    timeline_files = sorted(timeline_files, key=lambda f: f.stat().st_mtime, reverse=True)

    input_file = timeline_files[0]

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆx_timeline_YYYYMMDD.json â†’ top_10_tweets_YYYYMMDD.jsonï¼‰
    date_str = input_file.stem.replace('x_timeline_', '')
    # YYYYMMDD_HHMMSSå½¢å¼ã®å ´åˆã€_HHMMSSã‚’å‰Šé™¤
    if '_' in date_str:
        date_str = date_str.split('_')[0]

    output_file = data_dir / f"top_10_tweets_{date_str}.json"

    # å‡¦ç†å®Ÿè¡Œ
    output_data = extract_top_tweets(input_file, output_file)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    display_summary(output_data)


if __name__ == "__main__":
    main()
