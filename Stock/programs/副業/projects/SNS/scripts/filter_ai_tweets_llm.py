#!/usr/bin/env python3
"""
AIé–¢é€£åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° - LLMåˆ¤å®šç‰ˆ
LLMåˆ¤å®šç”¨ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€Claude Code LLMã«ã‚ˆã‚‹åˆ¤å®šã‚’å¾…æ©Ÿ
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any


def prepare_llm_judgment_data(tweets: List[Dict[str, Any]], output_file: Path) -> None:
    """
    LLMåˆ¤å®šç”¨ã®ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ

    Args:
        tweets: ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    print(f"\nğŸ“ Preparing LLM judgment data...")

    # LLMåˆ¤å®šç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    judgment_template = {
        "instruction": """ä»¥ä¸‹ã®10ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã«ã¤ã„ã¦ã€AIãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹é–¢é€£ã‹ã©ã†ã‹ã€å„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’0-3ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€è©•ä¾¡åŸºæº–ã€‘
- 3ç‚¹: LLM, ChatGPT, Claude, GPT, Gemini, transformer, RAG, ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ç­‰ã®æ˜ç¤ºçš„ãªAIæŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹
- 2ç‚¹: OpenAI, Anthropic, DeepMindç­‰ã®AIä¼æ¥­åãŒæ˜è¨˜ã•ã‚Œã€æŠ€è¡“çš„ãªè©³ç´°ãŒã‚ã‚‹
- 1ç‚¹: æ©Ÿæ¢°å­¦ç¿’ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã€äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã€è‡ªå‹•åŒ–ãŒä¸»é¡Œ
- 0ç‚¹: ä¸Šè¨˜ã„ãšã‚Œã«ã‚‚è©²å½“ã—ãªã„ï¼ˆä¸€èˆ¬ãƒ“ã‚¸ãƒã‚¹ã€æ”¿æ²»ã€æ ªå¼æŠ•è³‡ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¨ãƒ³ã‚¿ãƒ¡ç­‰ï¼‰

ã€é‡è¦ãªæ³¨æ„ã€‘
- Elon Muskã®æ”¿æ²»è³‡é‡‘æ´åŠ©ã€æˆåŠŸè¦å› ç­‰ã®è‡ªå·±å•“ç™ºã¯0ç‚¹
- æ ªå¼æŠ•è³‡ã€ä¼æ¥­ã®å¤§æ ªä¸»æƒ…å ±ã¯0ç‚¹
- YouTubeãƒãƒ£ãƒ³ãƒãƒ«åç›ŠåŒ–ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•ã¯0ç‚¹
- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã€ã‚¬ã‚¸ã‚§ãƒƒãƒˆã€è£½å“ç´¹ä»‹ã¯0ç‚¹
- ç›®æ¨™é”æˆã‚·ã‚¹ãƒ†ãƒ ã€è‡ªå·±å•“ç™ºã¯0ç‚¹
- æŠ•è³‡ä¸€èˆ¬ã€æ ªä¾¡è¦‹é€šã—ã¯0ç‚¹
- ãƒ­ãƒœãƒƒãƒˆï¼ˆOptimusç­‰ï¼‰ã®ã¿ã§AIæŠ€è¡“è¨€åŠãªã—ã¯1ç‚¹ï¼ˆAIå‘¨è¾ºæŠ€è¡“ï¼‰

ã€å›ç­”å½¢å¼ã€‘
å¿…ãšä»¥ä¸‹ã®JSONé…åˆ—å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
[
  {"tweet_id": "ID1", "score": 0, "reason": "ç†ç”±ã‚’20æ–‡å­—ä»¥å†…ã§"},
  {"tweet_id": "ID2", "score": 3, "reason": "ç†ç”±ã‚’20æ–‡å­—ä»¥å†…ã§"},
  ...
]
""",
        "tweets": []
    }

    # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç°¡ç•¥åŒ–ã—ã¦è¿½åŠ 
    for tweet in tweets:
        judgment_template["tweets"].append({
            "tweet_id": tweet.get('tweet_id'),
            "username": tweet.get('username'),
            "text": tweet.get('text'),
            "rank": tweet.get('rank'),
            "engagement_score": tweet.get('engagement_score')
        })

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    print(f"ğŸ’¾ Writing LLM judgment data to: {output_file}")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(judgment_template, f, ensure_ascii=False, indent=2)

    print("âœ… LLM judgment data created successfully")
    print(f"\nğŸ“Œ Next step:")
    print(f"   Run Claude Code to judge AI relevance:")
    print(f"   cat {output_file} | claude --model haiku")


def apply_llm_judgment(
    original_tweets: List[Dict[str, Any]],
    judgment_results: List[Dict[str, Any]],
    min_score: int = 1
) -> List[Dict[str, Any]]:
    """
    LLMåˆ¤å®šçµæœã‚’é©ç”¨ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    Args:
        original_tweets: å…ƒã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
        judgment_results: LLMåˆ¤å®šçµæœãƒªã‚¹ãƒˆ
        min_score: æœ€ä½ã‚¹ã‚³ã‚¢

    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
    """
    print(f"\nğŸ¤– Applying LLM judgment results (min_score: {min_score})...")

    # tweet_id -> åˆ¤å®šçµæœã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
    judgment_map = {
        result['tweet_id']: result
        for result in judgment_results
    }

    ai_tweets = []
    non_ai_tweets = []

    for tweet in original_tweets:
        tweet_id = tweet.get('tweet_id')
        judgment = judgment_map.get(tweet_id)

        if not judgment:
            print(f"âš ï¸  Warning: No judgment found for tweet {tweet_id}")
            tweet['ai_relevance_score'] = 0
            tweet['ai_relevance_reason'] = "åˆ¤å®šãªã—"
            non_ai_tweets.append(tweet)
            continue

        score = judgment.get('score', 0)
        reason = judgment.get('reason', '')

        # ãƒ„ã‚¤ãƒ¼ãƒˆã«AIé–¢é€£åº¦æƒ…å ±ã‚’è¿½åŠ 
        tweet['ai_relevance_score'] = score
        tweet['ai_relevance_reason'] = reason

        if score >= min_score:
            ai_tweets.append(tweet)
            print(f"   âœ… PASS - @{tweet.get('username')} (score: {score}, reason: {reason})")
        else:
            non_ai_tweets.append(tweet)
            print(f"   âŒ REJECT - @{tweet.get('username')} (score: {score}, reason: {reason})")

    print(f"\nâœ… AI-related tweets: {len(ai_tweets)}/{len(original_tweets)} ({len(ai_tweets)/len(original_tweets)*100:.1f}%)")
    print(f"   - Score 3: {len([t for t in ai_tweets if t['ai_relevance_score'] == 3])}")
    print(f"   - Score 2: {len([t for t in ai_tweets if t['ai_relevance_score'] == 2])}")
    print(f"   - Score 1: {len([t for t in ai_tweets if t['ai_relevance_score'] == 1])}")
    print(f"   - Score 0 (rejected): {len(non_ai_tweets)}")

    return ai_tweets


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("Usage:")
        print("  Step 1 (Prepare): python filter_ai_tweets_llm.py prepare <input_json> <judgment_file>")
        print("  Step 2 (Apply):   python filter_ai_tweets_llm.py apply <input_json> <judgment_result_json> <output_json> [min_score]")
        print("\nExample:")
        print("  python filter_ai_tweets_llm.py prepare top_10_tweets.json llm_judgment_input.json")
        print("  # Then run: cat llm_judgment_input.json | claude --model haiku > llm_judgment_result.json")
        print("  python filter_ai_tweets_llm.py apply top_10_tweets.json llm_judgment_result.json top_10_ai_tweets.json 1")
        sys.exit(1)

    mode = sys.argv[1]

    if mode == "prepare":
        # Step 1: LLMåˆ¤å®šç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        if len(sys.argv) < 4:
            print("Error: Missing arguments for prepare mode")
            print("Usage: python filter_ai_tweets_llm.py prepare <input_json> <judgment_file>")
            sys.exit(1)

        input_file = Path(sys.argv[2])
        judgment_file = Path(sys.argv[3])

        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        print(f"ğŸ“– Reading input file: {input_file}")

        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            print(f"âŒ Error: File not found: {input_file}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"âŒ Error: Invalid JSON format: {e}")
            sys.exit(1)

        tweets = data.get('top_tweets', [])

        if not tweets:
            print("âš ï¸  Warning: No tweets found in the data")
            sys.exit(1)

        print(f"âœ… Loaded {len(tweets)} tweets")

        # LLMåˆ¤å®šç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        prepare_llm_judgment_data(tweets, judgment_file)

    elif mode == "apply":
        # Step 2: LLMåˆ¤å®šçµæœé©ç”¨
        if len(sys.argv) < 5:
            print("Error: Missing arguments for apply mode")
            print("Usage: python filter_ai_tweets_llm.py apply <input_json> <judgment_result_json> <output_json> [min_score]")
            sys.exit(1)

        input_file = Path(sys.argv[2])
        judgment_result_file = Path(sys.argv[3])
        output_file = Path(sys.argv[4])
        min_score = int(sys.argv[5]) if len(sys.argv) > 5 else 1

        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        print(f"ğŸ“– Reading input file: {input_file}")

        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            print(f"âŒ Error: File not found: {input_file}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"âŒ Error: Invalid JSON format: {e}")
            sys.exit(1)

        tweets = data.get('top_tweets', [])

        if not tweets:
            print("âš ï¸  Warning: No tweets found in the data")
            sys.exit(1)

        print(f"âœ… Loaded {len(tweets)} tweets")

        # LLMåˆ¤å®šçµæœèª­ã¿è¾¼ã¿
        print(f"\nğŸ“– Reading LLM judgment results: {judgment_result_file}")

        try:
            with open(judgment_result_file, 'r', encoding='utf-8') as f:
                judgment_results = json.load(f)
        except FileNotFoundError:
            print(f"âŒ Error: File not found: {judgment_result_file}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"âŒ Error: Invalid JSON format: {e}")
            sys.exit(1)

        if not isinstance(judgment_results, list):
            print(f"âŒ Error: Judgment results must be a JSON array")
            sys.exit(1)

        print(f"âœ… Loaded {len(judgment_results)} judgment results")

        # LLMåˆ¤å®šçµæœã‚’é©ç”¨ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        ai_tweets = apply_llm_judgment(tweets, judgment_results, min_score=min_score)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        metadata = data.get('metadata', {})
        metadata['ai_filtered_at'] = datetime.now().isoformat()
        metadata['ai_filter_min_score'] = min_score
        metadata['ai_filter_passed'] = len(ai_tweets)
        metadata['ai_filter_rejected'] = len(tweets) - len(ai_tweets)
        metadata['ai_filter_pass_rate'] = len(ai_tweets) / len(tweets) if tweets else 0

        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        output_data = {
            "metadata": metadata,
            "top_tweets": ai_tweets
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        print(f"\nğŸ’¾ Writing output to: {output_file}")
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print("âœ… Output file created successfully")

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*60)
        print("âœ… AI filtering completed")
        print("="*60)
        print(f"  - Input tweets: {len(tweets)}")
        print(f"  - AI-related tweets (score â‰¥ {min_score}): {len(ai_tweets)} ({len(ai_tweets)/len(tweets)*100:.1f}%)")
        print(f"  - Rejected tweets (score < {min_score}): {len(tweets) - len(ai_tweets)} ({(len(tweets) - len(ai_tweets))/len(tweets)*100:.1f}%)")
        print(f"  - Output file: {output_file}")
        print("="*60)

    else:
        print(f"âŒ Error: Unknown mode '{mode}'")
        print("Valid modes: prepare, apply")
        sys.exit(1)


if __name__ == "__main__":
    main()
