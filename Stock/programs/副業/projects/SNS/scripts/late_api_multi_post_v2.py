#!/usr/bin/env python3
"""
Late API ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ v2ï¼ˆOption Cå¯¾å¿œç‰ˆï¼‰

æ©Ÿèƒ½:
- LinkedIn 1æ¡ˆï¼ˆ8:00 JSTï¼‰
- X 3æŠ•ç¨¿: æ´¾ç”Ÿ(7:30) + ã‚¹ãƒ¬ãƒƒãƒ‰1(12:00) + ã‚¹ãƒ¬ãƒƒãƒ‰2(20:00)
- Threads 2æŠ•ç¨¿: æ´¾ç”Ÿ(7:30) + æ–°è¦(20:00)

åˆè¨ˆ6æŠ•ç¨¿ã‚’å€‹åˆ¥ã«POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆ

Usage:
    python3 scripts/late_api_multi_post_v2.py
"""

import os
import sys
import json
import re
from pathlib import Path
from datetime import datetime, timedelta
import requests
from datetime import timezone
from typing import Optional, List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root / "scripts"))

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
JST = timezone(timedelta(hours=9))

# ========================================
# æŠ•ç¨¿ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šï¼ˆOption Cï¼‰
# ========================================
POSTING_SCHEDULE = {
    'linkedin': [
        {'time': '08:00', 'type': 'main', 'topic': 'top1', 'date_offset': 'auto'},
        {'time': '08:00', 'type': 'main', 'topic': 'top2', 'date_offset': 'auto'},
        {'time': '08:00', 'type': 'main', 'topic': 'top3', 'date_offset': 'auto'}
    ],
    'twitter': [
        {'time': '07:30', 'type': 'thread', 'topic': 'top1'},
        {'time': '12:00', 'type': 'thread', 'topic': 'top2'},
        {'time': '20:00', 'type': 'thread', 'topic': 'top3'}
    ],
    'threads': [
        {'time': '07:30', 'type': 'new', 'topic': 'top1'},
        {'time': '12:00', 'type': 'new', 'topic': 'top2'},
        {'time': '20:00', 'type': 'new', 'topic': 'top3'}
    ]
}


def load_env_vars() -> dict:
    """
    .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
    """
    env_file = project_root / ".env"
    env_vars = {}

    if env_file.exists():
        with open(env_file, "r") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¡ãƒ³ãƒˆé™¤å»
                    if "#" in value:
                        in_quote = False
                        quote_char = None
                        clean_value = []
                        for ch in value:
                            if ch in ['"', "'"]:
                                if not in_quote:
                                    in_quote = True
                                    quote_char = ch
                                elif ch == quote_char:
                                    in_quote = False
                                    quote_char = None
                            elif ch == "#" and not in_quote:
                                break
                            clean_value.append(ch)
                        value = "".join(clean_value)
                    value = value.strip().strip('"').strip("'")
                    env_vars[key.strip()] = value
    return env_vars


def remove_markdown(text: str) -> str:
    """Markdownè£…é£¾ã‚’é™¤å»"""
    text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
    text = re.sub(r'^\- ', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\d+\. ', '', text, flags=re.MULTILINE)
    return text


# ========================================
# ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–¢æ•°
# ========================================

def extract_linkedin_content(markdown: str, variant_number: int = 2) -> Optional[dict]:
    """
    LinkedInæŠ•ç¨¿ï¼ˆæ¡ˆNï¼‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º

    **v2å¯¾å¿œ**: variant_numberã‚’1-3ã§æŒ‡å®šå¯èƒ½
    v2.1: ã€Œæœ€åˆã®ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆfirstCommentï¼‰ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚æŠ½å‡º

    Args:
        markdown: Phase 3ç”Ÿæˆã•ã‚ŒãŸMarkdownãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
        variant_number: ãƒãƒªã‚¢ãƒ³ãƒˆç•ªå·ï¼ˆ1-3ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ¡ˆ2ãŒæœ€æ¨å¥¨ï¼‰

    Returns:
        dict: {"title": str, "body": str, "full_content": str, "first_comment": str}
    """
    # æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: ## LinkedInæ¡ˆ1ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³Xã€Œåç§°ã€ã€...ï¼‰
    pattern = rf'## LinkedInæ¡ˆ{variant_number}ï¼ˆ.*?\ï¼‰\n\n(.*?)(?=\n## |\Z)'
    match = re.search(pattern, markdown, re.DOTALL)

    if not match:
        # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        pattern = rf'## æ¡ˆ{variant_number}:.*?\n\n### ã‚¿ã‚¤ãƒˆãƒ«\n\*\*(.*?)\*\*\n\n### æœ¬æ–‡.*?\n\n(.*?)(?=\n---\n|\Z)'
        match = re.search(pattern, markdown, re.DOTALL)

        if not match:
            return None

        title = remove_markdown(match.group(1).strip())
        body = remove_markdown(match.group(2).strip())

        return {
            "title": title,
            "body": body,
            "full_content": f"{title}\n\n{body}",
            "first_comment": None  # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã¯firstCommentæœªå¯¾å¿œ
        }

    # æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆ
    full_section = match.group(1).strip()

    # v2å½¢å¼ã§ã¯ã€Œ#### æœ€åˆã®ã‚³ãƒ¡ãƒ³ãƒˆã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„ãŸã‚ã€æœ¬æ–‡å…¨ä½“ã‚’æŠ½å‡º
    body = remove_markdown(full_section)

    # ã‚¿ã‚¤ãƒˆãƒ«ã¯æœ¬æ–‡ã®æœ€åˆã®è¡Œã‚’ä½¿ç”¨
    lines = body.split('\n', 1)
    title = lines[0] if lines else ""

    first_comment = None  # v2å½¢å¼ã§ã¯firstCommentãªã—

    return {
        "title": title,
        "body": body,
        "full_content": body,
        "first_comment": first_comment
    }


def extract_x_thread_content(markdown: str, thread_number: int) -> Optional[List[str]]:
    """
    Xã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆ5-7ãƒ„ã‚¤ãƒ¼ãƒˆæ·±æ˜ã‚Šå‹ï¼‰

    **v2å¯¾å¿œ**: thread_number ã‚’ 1-3 ã«æ‹¡å¼µï¼ˆå¾“æ¥ã¯1-2ã®ã¿ï¼‰

    Args:
        markdown: Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
        thread_number: ã‚¹ãƒ¬ãƒƒãƒ‰ç•ªå·ï¼ˆ1=Top1, 2=Top2, 3=Top3ï¼‰

    Returns:
        List[str]: å„ãƒ„ã‚¤ãƒ¼ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆ5-7ä»¶ï¼‰
    """
    # v2ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: ## Xã‚¹ãƒ¬ãƒƒãƒ‰1ï¼ˆTop 1ãƒˆãƒ”ãƒƒã‚¯: XXXã€æ·±æ˜ã‚Šå‹ï¼‰
    pattern = rf'## Xã‚¹ãƒ¬ãƒƒãƒ‰{thread_number}ï¼ˆ.*?\ï¼‰\n\n(.*?)(?=\n## |\Z)'
    match = re.search(pattern, markdown, re.DOTALL)

    if not match:
        return None

    content = match.group(1).strip()

    # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åˆ†å‰²ï¼ˆ**N/M**, (N/M), ### ãƒ„ã‚¤ãƒ¼ãƒˆN å½¢å¼ï¼‰
    tweets = []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: **N/M** å½¢å¼ï¼ˆé«˜é‡å¼ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
    tweet_sections = re.split(r'\n\*\*\d+/\d+\*\*\n', content)
    if len(tweet_sections) > 1:
        for section in tweet_sections[1:]:
            # ---ã§åŒºåˆ‡ã‚‰ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†é›¢
            parts = section.split('\n---\n')
            if parts:
                tweet_text = remove_markdown(parts[0].strip())
                if tweet_text:
                    tweets.append(tweet_text[:280])

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ### ãƒ„ã‚¤ãƒ¼ãƒˆN å½¢å¼
    if not tweets:
        tweet_sections = re.split(r'### ãƒ„ã‚¤ãƒ¼ãƒˆ\d+', content)
        if len(tweet_sections) > 1:
            for section in tweet_sections[1:]:
                tweet_text = remove_markdown(section.strip())
                if tweet_text:
                    tweets.append(tweet_text[:280])

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: (N/M) å½¢å¼
    if not tweets:
        tweet_parts = re.split(r'\(\d+/\d+\)', content)
        for i, part in enumerate(tweet_parts):
            if part.strip():
                tweet_text = remove_markdown(part.strip())
                if tweet_text:
                    prefix = f"({i}/{len(tweet_parts)-1}) " if i > 0 else ""
                    tweets.append(f"{prefix}{tweet_text}"[:280])

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’7ãƒ„ã‚¤ãƒ¼ãƒˆã«è‡ªå‹•åˆ†å‰²
    if not tweets and content:
        content_clean = remove_markdown(content)
        # æ®µè½ã§åˆ†å‰²
        paragraphs = content_clean.split("\n\n")
        for p in paragraphs:
            if p.strip():
                tweets.append(p.strip()[:280])
        # æœ€å¤§7ãƒ„ã‚¤ãƒ¼ãƒˆã«åˆ¶é™
        tweets = tweets[:7]

    return tweets if tweets else None


def extract_threads_new_content(markdown: str) -> Optional[dict]:
    """
    Threadsæ–°è¦æŠ•ç¨¿ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆTop 2ãƒˆãƒ”ãƒƒã‚¯ã€LinkedInä¼¼è¡¨ç¾ï¼‰

    ã‚»ã‚¯ã‚·ãƒ§ãƒ³: ## Threadsæ–°è¦æŠ•ç¨¿ï¼ˆTop 2ãƒˆãƒ”ãƒƒã‚¯ï¼‰

    ãƒ¡ã‚¿æƒ…å ±ï¼ˆ**ãƒˆãƒ”ãƒƒã‚¯**:, ---, **æ–‡å­—æ•°**:ï¼‰ã‚’é™¤å¤–ã—ã¦æœ¬æ–‡ã®ã¿ã‚’æŠ½å‡º
    """
    # **ãƒˆãƒ”ãƒƒã‚¯**: ... ã¨ --- ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æœ¬æ–‡ã®ã¿æŠ½å‡º
    pattern = r'## Threadsæ–°è¦æŠ•ç¨¿ï¼ˆTop 2ãƒˆãƒ”ãƒƒã‚¯ï¼‰.*?\n\n\*\*ãƒˆãƒ”ãƒƒã‚¯\*\*:.*?\n\n---\n\n(.*?)(?=\n---\n|\n## |\Z)'
    match = re.search(pattern, markdown, re.DOTALL)

    if not match:
        return None

    content = remove_markdown(match.group(1).strip())
    return {"content": content[:500], "type": "new"}


def extract_threads_post_with_char_control(markdown: str, post_number: int) -> Optional[dict]:
    """
    ThreadsæŠ•ç¨¿ã‚’æŠ½å‡ºï¼ˆæ–‡å­—æ•°åˆ¶å¾¡å¯¾å¿œç‰ˆï¼‰

    â‰¤500æ–‡å­—: å˜ä¸€æŠ•ç¨¿
    >500æ–‡å­—: 2-3æŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰

    Args:
        post_number: æŠ•ç¨¿ç•ªå·ï¼ˆ1-3ï¼‰

    Returns:
        dict: {"type": "single", "content": str} or
              {"type": "thread", "posts": [...], "total_posts": int}
    """
    # v2ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: ## ThreadsæŠ•ç¨¿1ï¼ˆTop 1ãƒˆãƒ”ãƒƒã‚¯: XXXï¼‰
    pattern = rf'## ThreadsæŠ•ç¨¿{post_number}ï¼ˆ.*?\ï¼‰\n\n(.*?)(?=\n## |\Z)'
    match = re.search(pattern, markdown, re.DOTALL)

    if not match:
        return None

    content = remove_markdown(match.group(1).strip())
    char_count = len(content)

    if char_count <= 500:
        return {"type": "single", "content": content, "char_count": char_count}
    else:
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åˆ†å‰²: æ®µè½å˜ä½ã§500æ–‡å­—ä»¥å†…ã«åˆ†å‰²
        paragraphs = content.split('\n\n')
        posts = []
        current_post = ""

        for para in paragraphs:
            if len(current_post) + len(para) + 2 <= 500:
                current_post += para + "\n\n"
            else:
                if current_post:
                    posts.append({"content": current_post.strip(), "char_count": len(current_post.strip())})
                current_post = para + "\n\n"

        if current_post:
            posts.append({"content": current_post.strip(), "char_count": len(current_post.strip())})

        return {"type": "thread", "posts": posts, "total_posts": len(posts)}


# ========================================
# Late APIæŠ•ç¨¿é–¢æ•°
# ========================================

def post_to_late_api(
    content: str,
    platform: str,
    account_id: str,
    scheduled_datetime: datetime,
    api_key: str,
    thread_items: Optional[List[str]] = None,
    first_comment: Optional[str] = None
) -> dict:
    """
    Late APIã«1ä»¶ã®æŠ•ç¨¿ã‚’é€ä¿¡

    v2.1: LinkedIn firstCommentå¯¾å¿œè¿½åŠ 

    Args:
        content: æŠ•ç¨¿æœ¬æ–‡ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰æ™‚ã¯æœ€åˆã®æŠ•ç¨¿ï¼‰
        platform: ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆlinkedin, twitter, threadsï¼‰
        account_id: ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID
        scheduled_datetime: äºˆç´„æ—¥æ™‚ï¼ˆJSTï¼‰
        api_key: Late API ã‚­ãƒ¼
        thread_items: Xã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿æ™‚ã®å„ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
        first_comment: LinkedIn firstCommentï¼ˆLinkedInã®ã¿å¯¾å¿œï¼‰

    Returns:
        dict: Late APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    base_url = "https://getlate.dev/api/v1"

    if scheduled_datetime.tzinfo is None:
        scheduled_datetime = scheduled_datetime.replace(tzinfo=JST)

    scheduled_datetime_str = scheduled_datetime.strftime("%Y-%m-%dT%H:%M:%S%z")
    if len(scheduled_datetime_str) >= 5:
        scheduled_datetime_str = scheduled_datetime_str[:-2] + ':' + scheduled_datetime_str[-2:]

    # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ è¨­å®š
    platform_config = {
        "platform": platform,
        "accountId": account_id
    }

    # LinkedIn firstCommentå¯¾å¿œ
    if first_comment and platform == "linkedin":
        platform_config["platformSpecificData"] = {
            "firstComment": first_comment
        }

    # Xã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ã®å ´åˆ
    if thread_items and platform == "twitter":
        platform_config["platformSpecificData"] = {
            "threadItems": [{"content": tweet} for tweet in thread_items]
        }

    payload = {
        "content": content,
        "platforms": [platform_config],
        "scheduledFor": scheduled_datetime_str,
        "timezone": "Asia/Tokyo"
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    response = requests.post(
        f"{base_url}/posts",
        headers=headers,
        json=payload,
        timeout=30
    )

    if response.status_code not in [200, 201]:
        raise Exception(f"Late API Error: {response.status_code} - {response.text}")

    return response.json()


def get_existing_scheduled_posts(api_key: str) -> dict:
    """
    æ—¢å­˜ã®äºˆç´„æŠ•ç¨¿ã‚’å–å¾—ã—ã€æ™‚é–“å¸¯åˆ¥ã«åˆ†é¡

    Returns:
        dict: {
            'posts': [...],
            'reserved_by_hour': {7: set(), 8: set(), 12: set(), 20: set()}
        }
    """
    base_url = "https://getlate.dev/api/v1"

    try:
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        response = requests.get(f"{base_url}/posts", headers=headers, params={"status": "scheduled"}, timeout=30)
        response.raise_for_status()
        scheduled_posts = response.json()
    except Exception as e:
        print(f"âš ï¸  æ—¢å­˜äºˆç´„ã®å–å¾—ã«å¤±æ•—: {e}")
        return {'posts': [], 'reserved_by_hour': {7: set(), 8: set(), 12: set(), 20: set()}}

    reserved_by_hour = {7: set(), 8: set(), 12: set(), 20: set()}

    for post in scheduled_posts.get("posts", []):
        scheduled_for = post.get("scheduledFor")
        if scheduled_for:
            try:
                dt = datetime.fromisoformat(scheduled_for.replace('Z', '+00:00'))
                dt_jst = dt.astimezone(JST)
                hour = dt_jst.hour
                if hour in reserved_by_hour:
                    reserved_by_hour[hour].add(dt_jst.date())
            except Exception:
                pass

    return {
        'posts': scheduled_posts.get('posts', []),
        'reserved_by_hour': reserved_by_hour
    }


def find_available_date(reserved_by_hour: dict, target_hours: List[int]) -> datetime:
    """
    æŒ‡å®šã—ãŸå…¨æ™‚é–“å¸¯ã§ç©ºã„ã¦ã„ã‚‹æœ€ã‚‚è¿‘ã„æ—¥ä»˜ã‚’æ¤œç´¢

    Args:
        reserved_by_hour: æ™‚é–“å¸¯åˆ¥ã®äºˆç´„æ¸ˆã¿æ—¥ä»˜
        target_hours: ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ™‚é–“å¸¯ãƒªã‚¹ãƒˆ [7, 8, 12, 20]

    Returns:
        datetime.date: åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜
    """
    current_date = datetime.now(JST).date() + timedelta(days=1)
    max_search_days = 30

    for _ in range(max_search_days):
        is_available = True
        for hour in target_hours:
            if current_date in reserved_by_hour.get(hour, set()):
                is_available = False
                break

        if is_available:
            return current_date

        current_date += timedelta(days=1)

    # 30æ—¥å¾Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç¿Œæ—¥ã‚’è¿”ã™
    return datetime.now(JST).date() + timedelta(days=1)


def find_available_dates_for_linkedin(
    reserved_by_hour: dict,
    target_hour: int = 8,
    days_needed: int = 3,
    max_search_days: int = 14
) -> List[datetime.date]:
    """
    LinkedInç”¨ã«è¤‡æ•°ã®ç©ºãæ—¥ã‚’æ¤œç´¢

    Args:
        reserved_by_hour: æ™‚é–“å¸¯åˆ¥ã®äºˆç´„æ¸ˆã¿æ—¥ä»˜
        target_hour: æŠ•ç¨¿æ™‚åˆ»ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ8æ™‚ï¼‰
        days_needed: å¿…è¦ãªæ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3æ—¥ï¼‰
        max_search_days: æœ€å¤§æ¤œç´¢æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ14æ—¥ï¼‰

    Returns:
        List[datetime.date]: åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¤§3æ—¥ï¼‰

    Raises:
        Exception: å¿…è¦ãªæ—¥æ•°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    """
    current_date = datetime.now(JST).date() + timedelta(days=1)
    available_dates = []

    for day_offset in range(max_search_days):
        check_date = current_date + timedelta(days=day_offset)

        if check_date not in reserved_by_hour.get(target_hour, set()):
            available_dates.append(check_date)

        if len(available_dates) == days_needed:
            break

    if len(available_dates) < days_needed:
        raise Exception(
            f"LinkedInç©ºãæ—¥ãŒ{len(available_dates)}æ—¥ã—ã‹è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            f"14æ—¥ä»¥å†…ã«{days_needed}æ—¥ã®ç©ºããŒå¿…è¦ã§ã™ã€‚"
        )

    return available_dates


# ========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("Late API ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æŠ•ç¨¿ v2ï¼ˆOption Cå¯¾å¿œç‰ˆï¼‰")
    print("=" * 60)
    print()
    print("æŠ•ç¨¿è¨ˆç”»:")
    print("  - LinkedIn: 1æ¡ˆï¼ˆ8:00ï¼‰")
    print("  - X: 3æŠ•ç¨¿ï¼ˆæ´¾ç”Ÿ7:30 + ã‚¹ãƒ¬ãƒƒãƒ‰12:00 + ã‚¹ãƒ¬ãƒƒãƒ‰20:00ï¼‰")
    print("  - Threads: 2æŠ•ç¨¿ï¼ˆæ´¾ç”Ÿ7:30 + æ–°è¦20:00ï¼‰")
    print("  åˆè¨ˆ: 6æŠ•ç¨¿")
    print()

    try:
        # 1. ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
        env_vars = load_env_vars()
        api_key = env_vars.get("LATE_API_KEY")
        linkedin_account_id = env_vars.get("LATE_LINKEDIN_ACCOUNT_ID")
        twitter_account_id = env_vars.get("LATE_TWITTER_ACCOUNT_ID")
        threads_account_id = env_vars.get("LATE_THREADS_ACCOUNT_ID")

        if not api_key:
            raise ValueError("LATE_API_KEY not found in .env file")

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆIDç¢ºèª
        available_platforms = []
        if linkedin_account_id:
            available_platforms.append("linkedin")
        if twitter_account_id:
            available_platforms.append("twitter")
        if threads_account_id:
            available_platforms.append("threads")

        print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {', '.join(available_platforms)}")

        if not available_platforms:
            raise ValueError("No platform account IDs found in .env file")

        # 2. Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        data_dir = project_root / "data"
        markdown_files = list(data_dir.glob("posts_generated_takano_*.md"))

        if not markdown_files:
            print("âŒ posts_generated_takano_*.md ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)

        latest_file = max(markdown_files, key=lambda f: f.stat().st_mtime)
        print(f"ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file.name}")
        markdown_content = latest_file.read_text(encoding="utf-8")

        # 3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
        print()
        print("ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºä¸­...")

        contents = {}

        # LinkedInï¼ˆæ¡ˆ1-3ã‚’ä½¿ç”¨ï¼‰
        for i in range(1, 4):
            linkedin_content = extract_linkedin_content(markdown_content, i)
            if linkedin_content:
                contents[f'linkedin{i}'] = linkedin_content
                print(f"  âœ… LinkedInæ¡ˆ{i}: {len(linkedin_content['full_content'])}æ–‡å­—")
            else:
                print(f"  âš ï¸  LinkedInæ¡ˆ{i}: æŠ½å‡ºå¤±æ•—")

        # Xã‚¹ãƒ¬ãƒƒãƒ‰1-3ï¼ˆTop 1-3ãƒˆãƒ”ãƒƒã‚¯ï¼‰
        for i in range(1, 4):
            x_thread = extract_x_thread_content(markdown_content, i)
            if x_thread:
                contents[f'x_thread{i}'] = x_thread
                print(f"  âœ… Xã‚¹ãƒ¬ãƒƒãƒ‰{i}: {len(x_thread)}ãƒ„ã‚¤ãƒ¼ãƒˆ")
            else:
                print(f"  âš ï¸  Xã‚¹ãƒ¬ãƒƒãƒ‰{i}: æŠ½å‡ºå¤±æ•—")

        # ThreadsæŠ•ç¨¿1-3ï¼ˆTop 1-3ãƒˆãƒ”ãƒƒã‚¯ã€æ–‡å­—æ•°åˆ¶å¾¡ï¼‰
        for i in range(1, 4):
            threads_post = extract_threads_post_with_char_control(markdown_content, i)
            if threads_post:
                contents[f'threads_post{i}'] = threads_post
                type_label = "å˜ä¸€" if threads_post['type'] == 'single' else f"ã‚¹ãƒ¬ãƒƒãƒ‰{threads_post['total_posts']}æŠ•ç¨¿"
                print(f"  âœ… ThreadsæŠ•ç¨¿{i}: {type_label}")
            else:
                print(f"  âš ï¸  ThreadsæŠ•ç¨¿{i}: æŠ½å‡ºå¤±æ•—")

        # 4. æ—¢å­˜äºˆç´„å–å¾—ã¨æ—¥ä»˜æ±ºå®š
        print()
        print("ğŸ” æ—¢å­˜äºˆç´„æŠ•ç¨¿ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        existing = get_existing_scheduled_posts(api_key)
        print(f"   æ—¢å­˜äºˆç´„æŠ•ç¨¿: {len(existing['posts'])}ä»¶")

        # LinkedInç”¨ã®ç©ºãæ—¥ã‚’3æ—¥æ¤œå‡º
        try:
            linkedin_dates = find_available_dates_for_linkedin(
                reserved_by_hour=existing['reserved_by_hour'],
                target_hour=8,
                days_needed=3,
                max_search_days=7
            )
            print(f"âœ… LinkedInç©ºãæ—¥: {len(linkedin_dates)}æ—¥æ¤œå‡º")
            for i, date in enumerate(linkedin_dates, 1):
                print(f"   {i}æ—¥ç›®: {date}")
        except Exception as e:
            print(f"âš ï¸  7æ—¥ä»¥å†…ã«ç©ºãæ—¥ä¸è¶³ã€‚14æ—¥ã«å»¶é•·ã—ã¦å†è©¦è¡Œ...")
            linkedin_dates = find_available_dates_for_linkedin(
                reserved_by_hour=existing['reserved_by_hour'],
                target_hour=8,
                days_needed=3,
                max_search_days=14
            )
            print(f"âœ… LinkedInç©ºãæ—¥: {len(linkedin_dates)}æ—¥æ¤œå‡ºï¼ˆ14æ—¥ã‚¹ã‚­ãƒ£ãƒ³ï¼‰")
            for i, date in enumerate(linkedin_dates, 1):
                print(f"   {i}æ—¥ç›®: {date}")

        # X/Threadsç”¨ã®æŠ•ç¨¿æ—¥ã‚’æ¤œç´¢ï¼ˆ7:30, 12:00, 20:00ãŒå…¨ã¦ç©ºã„ã¦ã„ã‚‹æ—¥ï¼‰
        target_date = find_available_date(existing['reserved_by_hour'], [7, 12, 20])
        print(f"âœ… X/ThreadsæŠ•ç¨¿æ—¥: {target_date}")

        # 5. æŠ•ç¨¿è¨ˆç”»ã‚’ä½œæˆ
        posting_plan = []

        # LinkedInï¼ˆ8:00ã€3æŠ•ç¨¿ã‚’3æ—¥åˆ†æ•£ï¼‰
        if linkedin_account_id:
            for i in range(1, 4):
                linkedin_key = f'linkedin{i}'
                if linkedin_key in contents:
                    posting_plan.append({
                        'platform': 'linkedin',
                        'type': 'main',
                        'time': '08:00',
                        'date': linkedin_dates[i-1],  # ç©ºãæ—¥ã‚’å€‹åˆ¥æŒ‡å®š
                        'content': contents[linkedin_key]['full_content'],
                        'account_id': linkedin_account_id,
                        'title': f'LinkedInæ¡ˆ{i}ï¼ˆTop {i}ï¼‰',
                        'thread_items': None,
                        'first_comment': contents[linkedin_key].get('first_comment')
                    })

        # Xã‚¹ãƒ¬ãƒƒãƒ‰1-3ï¼ˆ7:30, 12:00, 20:00ï¼‰
        if twitter_account_id:
            for i in range(1, 4):
                x_thread_key = f'x_thread{i}'
                if x_thread_key in contents:
                    posting_plan.append({
                        'platform': 'twitter',
                        'type': 'thread',
                        'time': ['07:30', '12:00', '20:00'][i-1],
                        'content': contents[x_thread_key][0] if contents[x_thread_key] else '',
                        'account_id': twitter_account_id,
                        'title': f'Xã‚¹ãƒ¬ãƒƒãƒ‰{i}ï¼ˆTop {i}ã€{len(contents[x_thread_key])}ãƒ„ã‚¤ãƒ¼ãƒˆï¼‰',
                        'thread_items': contents[x_thread_key]
                    })

        # ThreadsæŠ•ç¨¿1-3ï¼ˆ7:30, 12:00, 20:00ï¼‰
        if threads_account_id:
            for i in range(1, 4):
                threads_key = f'threads_post{i}'
                if threads_key in contents:
                    threads_data = contents[threads_key]
                    if threads_data['type'] == 'single':
                        # å˜ä¸€æŠ•ç¨¿
                        posting_plan.append({
                            'platform': 'threads',
                            'type': 'new',
                            'time': ['07:30', '12:00', '20:00'][i-1],
                            'content': threads_data['content'],
                            'account_id': threads_account_id,
                            'title': f'ThreadsæŠ•ç¨¿{i}ï¼ˆTop {i}ï¼‰',
                            'thread_items': None
                        })
                    else:
                        # ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿
                        posting_plan.append({
                            'platform': 'threads',
                            'type': 'new',
                            'time': ['07:30', '12:00', '20:00'][i-1],
                            'content': threads_data['posts'][0]['content'],
                            'account_id': threads_account_id,
                            'title': f'ThreadsæŠ•ç¨¿{i}ï¼ˆTop {i}ã€ã‚¹ãƒ¬ãƒƒãƒ‰{threads_data["total_posts"]}æŠ•ç¨¿ï¼‰',
                            'thread_items': [post['content'] for post in threads_data['posts']]
                        })

        # 6. æŠ•ç¨¿è¨ˆç”»ã‚’è¡¨ç¤º
        print()
        print("=" * 60)
        print("æŠ•ç¨¿è¨ˆç”»ï¼ˆç«¶åˆå›é¿æ¸ˆã¿ï¼‰")
        print("=" * 60)

        for plan in posting_plan:
            platform_emoji = {'linkedin': 'ğŸ’¼', 'twitter': 'ğŸ¦', 'threads': 'ğŸ§µ'}.get(plan['platform'], 'ğŸ“±')
            post_date = plan.get('date', target_date)  # LinkedInã¯å€‹åˆ¥æ—¥ä»˜
            print(f"{platform_emoji} {post_date} {plan['time']} JST - {plan['platform'].upper()}")
            print(f"   ã‚¿ã‚¤ãƒ—: {plan['type']}")
            print(f"   å†…å®¹: {plan['title']}...")
            if plan['thread_items']:
                print(f"   ã‚¹ãƒ¬ãƒƒãƒ‰: {len(plan['thread_items'])}ãƒ„ã‚¤ãƒ¼ãƒˆ")
            print()

        print(f"åˆè¨ˆ: {len(posting_plan)}æŠ•ç¨¿")
        print()

        # 7. ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªï¼ˆç’°å¢ƒå¤‰æ•°ã§è‡ªå‹•æ‰¿èªå¯èƒ½ï¼‰
        print("=" * 60)

        auto_confirm = env_vars.get("AUTO_CONFIRM_POSTING", "").lower() in ["true", "1", "yes"]

        if auto_confirm:
            print("âœ… AUTO_CONFIRM_POSTING=true ã«ã‚ˆã‚Šè‡ªå‹•å®Ÿè¡Œã—ã¾ã™")
        else:
            confirm = input("ä¸Šè¨˜ã®è¨ˆç”»ã§æŠ•ç¨¿ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ")
            if confirm.lower() != 'y':
                print("âŒ å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
                sys.exit(0)

        # 8. æŠ•ç¨¿å®Ÿè¡Œ
        print()
        print("=" * 60)
        print("Late APIæŠ•ç¨¿å®Ÿè¡Œä¸­...")
        print("=" * 60)
        print()

        results = []

        for plan in posting_plan:
            # æ™‚åˆ»ã‚’ãƒ‘ãƒ¼ã‚¹
            hour, minute = map(int, plan['time'].split(':'))
            # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥æ—¥ä»˜ï¼ˆLinkedInã¯å€‹åˆ¥æ—¥ä»˜ï¼‰
            post_date = plan.get('date', target_date)
            scheduled_datetime = datetime.combine(
                post_date,
                datetime.min.time()
            ).replace(hour=hour, minute=minute, tzinfo=JST)

            platform_emoji = {'linkedin': 'ğŸ’¼', 'twitter': 'ğŸ¦', 'threads': 'ğŸ§µ'}.get(plan['platform'], 'ğŸ“±')
            print(f"{platform_emoji} {plan['platform'].upper()} ({plan['time']}, {post_date}) ã‚’æŠ•ç¨¿ä¸­...")
            print(f"   ã‚¿ã‚¤ãƒ—: {plan['type']}")

            # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤
            retry_delays = [5, 15, 30]  # ç§’
            success = False

            for retry_attempt, delay in enumerate(retry_delays + [None], 1):
                try:
                    result = post_to_late_api(
                        content=plan['content'],
                        platform=plan['platform'],
                        account_id=plan['account_id'],
                        scheduled_datetime=scheduled_datetime,
                        api_key=api_key,
                        thread_items=plan['thread_items'],
                        first_comment=plan.get('first_comment')
                    )

                    post_id = result.get("post", {}).get("_id") or result.get("id")
                    print(f"   âœ… æˆåŠŸ! Post ID: {post_id}")
                    print()

                    results.append({
                        "platform": plan['platform'],
                        "type": plan['type'],
                        "status": "success",
                        "post_id": post_id,
                        "scheduled_for": scheduled_datetime.isoformat(),
                        "title": plan['title']
                    })
                    success = True
                    break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—çµ‚äº†

                except Exception as e:
                    if delay is not None:
                        print(f"   âš ï¸  ãƒªãƒˆãƒ©ã‚¤ {retry_attempt}/{len(retry_delays)}ï¼ˆ{delay}ç§’å¾Œï¼‰...")
                        import time
                        time.sleep(delay)
                    else:
                        # 3å›å¤±æ•—â†’Markdownãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        print(f"   âŒ æœ€çµ‚å¤±æ•—: {e}")
                        data_dir = Path(__file__).parent.parent / "data"
                        fallback_file = data_dir / f"manual_posts/{plan['platform']}_{plan['type']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                        fallback_file.parent.mkdir(parents=True, exist_ok=True)
                        with open(fallback_file, 'w', encoding='utf-8') as f:
                            f.write(f"# æ‰‹å‹•æŠ•ç¨¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
                            f.write(f"**ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: {plan['platform']}\n")
                            f.write(f"**äºˆç´„æ—¥æ™‚**: {scheduled_datetime.isoformat()}\n\n")
                            f.write(f"## æŠ•ç¨¿å†…å®¹\n\n{plan['content']}\n")
                        print(f"   ğŸ“„ æ‰‹å‹•æŠ•ç¨¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {fallback_file.name}")
                        print()

                        results.append({
                            "platform": plan['platform'],
                            "type": plan['type'],
                            "status": "error",
                            "error_message": str(e),
                            "scheduled_for": scheduled_datetime.isoformat(),
                            "fallback_file": str(fallback_file)
                        })

        # 9. çµæœä¿å­˜
        result_file = data_dir / f"late_api_multiplatform_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(result_file, "w", encoding="utf-8") as f:
            json.dump({
                "executed_at": datetime.now(JST).isoformat(),
                "target_date": str(target_date),
                "posting_schedule": POSTING_SCHEDULE,
                "results": results
            }, f, indent=2, ensure_ascii=False)

        # 10. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("=" * 60)
        print("å®Ÿè¡Œå®Œäº†")
        print("=" * 60)
        print(f"ğŸ’¾ çµæœä¿å­˜: {result_file.name}")
        print()

        success_count = sum(1 for r in results if r["status"] == "success")
        failed_count = len(results) - success_count

        print(f"âœ… æˆåŠŸ: {success_count}/{len(posting_plan)}æŠ•ç¨¿")
        print(f"âŒ å¤±æ•—: {failed_count}/{len(posting_plan)}æŠ•ç¨¿")

        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥ã‚µãƒãƒªãƒ¼
        print()
        print("ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥:")
        for platform in ['linkedin', 'twitter', 'threads']:
            platform_results = [r for r in results if r['platform'] == platform]
            if platform_results:
                success = sum(1 for r in platform_results if r['status'] == 'success')
                emoji = {'linkedin': 'ğŸ’¼', 'twitter': 'ğŸ¦', 'threads': 'ğŸ§µ'}.get(platform, 'ğŸ“±')
                print(f"  {emoji} {platform.upper()}: {success}/{len(platform_results)}")

        if success_count == len(posting_plan):
            print()
            print("ğŸ‰ å…¨æŠ•ç¨¿ãŒæˆåŠŸã—ã¾ã—ãŸï¼")
            print("Late APIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç¢ºèªã—ã¦ãã ã•ã„:")
            print("https://getlate.dev/dashboard")
        elif success_count > 0:
            print()
            print("âš ï¸  ä¸€éƒ¨ã®æŠ•ç¨¿ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            for r in results:
                if r["status"] == "error":
                    print(f"   - {r['platform']} ({r['type']}): {r.get('error_message', 'Unknown error')}")
        else:
            print()
            print("âŒ å…¨ã¦ã®æŠ•ç¨¿ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            print("ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
