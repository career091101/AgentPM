#!/usr/bin/env python3
"""
Late API æ—¥æ¬¡ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¯æ—¥AM 9:00ã«å‰æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ã—ã€SQLiteã«ä¿å­˜
"""

import requests
import json
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
import os
from typing import Dict, List


# ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent.parent
CONFIG_PATH = BASE_DIR / "config" / "late_api_config.json"
DB_PATH = BASE_DIR / "data" / "analytics.db"
BACKUP_DIR = BASE_DIR / "data" / "analytics_backup"


def load_config() -> dict:
    """Late APIè¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰"""
    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
        config = json.load(f)

    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰API Keyã‚’å–å¾—
    api_key = os.environ.get("LATE_API_KEY")
    if api_key:
        config["api_key"] = api_key

    return config


def get_headers(api_key: str) -> dict:
    """APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼"""
    return {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }


def init_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # analytics ãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS analytics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id TEXT UNIQUE NOT NULL,
            platform TEXT NOT NULL,
            published_at TEXT NOT NULL,
            impressions INTEGER DEFAULT 0,
            reach INTEGER DEFAULT 0,
            likes INTEGER DEFAULT 0,
            comments INTEGER DEFAULT 0,
            shares INTEGER DEFAULT 0,
            clicks INTEGER DEFAULT 0,
            views INTEGER DEFAULT 0,
            engagement_rate REAL DEFAULT 0.0,
            collected_at TEXT NOT NULL,
            raw_data TEXT
        )
    """)

    # daily_summary ãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS daily_summary (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT UNIQUE NOT NULL,
            platform TEXT NOT NULL,
            total_posts INTEGER DEFAULT 0,
            total_impressions INTEGER DEFAULT 0,
            total_engagement INTEGER DEFAULT 0,
            avg_engagement_rate REAL DEFAULT 0.0,
            top_post_id TEXT,
            top_post_impressions INTEGER DEFAULT 0,
            collected_at TEXT NOT NULL
        )
    """)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_platform ON analytics(platform)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_published_at ON analytics(published_at)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_impressions ON analytics(impressions DESC)")

    conn.commit()
    conn.close()

    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")


def get_analytics(
    config: dict,
    from_date: str,
    to_date: str,
    platform: str = None,
    limit: int = 1000
) -> List[Dict]:
    """Late APIçµŒç”±ã§ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    api_key = config["api_key"]
    base_url = config["base_url"]

    params = {
        "fromDate": from_date,
        "toDate": to_date,
        "limit": limit,
        "sortBy": "date"
    }

    if platform:
        params["platform"] = platform

    try:
        response = requests.get(
            f"{base_url}/analytics",
            headers=get_headers(api_key),
            params=params,
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ã‚ˆã£ã¦åˆ†å²
            if isinstance(data, dict) and "data" in data:
                return data["data"]
            elif isinstance(data, list):
                return data
            else:
                return [data]
        else:
            print(f"âŒ API ã‚¨ãƒ©ãƒ¼: {response.status_code}")
            print(f"Response: {response.text}")
            return []

    except Exception as e:
        print(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def save_to_database(analytics_data: List[Dict]):
    """ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteã«ä¿å­˜"""
    if not analytics_data:
        print("âš ï¸  ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    collected_at = datetime.now().isoformat()

    saved_count = 0
    updated_count = 0

    for item in analytics_data:
        try:
            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¯å®Ÿéš›ã®APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            post_id = item.get("postId") or item.get("id")
            platform = item.get("platform", "unknown")
            published_at = item.get("publishedAt") or item.get("createdAt")
            impressions = item.get("impressions", 0)
            reach = item.get("reach", 0)
            likes = item.get("likes", 0)
            comments = item.get("comments", 0)
            shares = item.get("shares", 0)
            clicks = item.get("clicks", 0)
            views = item.get("views", 0)
            engagement_rate = item.get("engagementRate", 0.0)

            # INSERT OR REPLACEï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯æ›´æ–°ï¼‰
            cursor.execute("""
                INSERT OR REPLACE INTO analytics (
                    post_id, platform, published_at, impressions, reach,
                    likes, comments, shares, clicks, views, engagement_rate,
                    collected_at, raw_data
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                post_id, platform, published_at, impressions, reach,
                likes, comments, shares, clicks, views, engagement_rate,
                collected_at, json.dumps(item)
            ))

            if cursor.rowcount > 0:
                if cursor.lastrowid > 0:
                    saved_count += 1
                else:
                    updated_count += 1

        except Exception as e:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"å•é¡Œã®ãƒ‡ãƒ¼ã‚¿: {item}")

    conn.commit()
    conn.close()

    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†: {saved_count}ä»¶æ–°è¦, {updated_count}ä»¶æ›´æ–°")


def save_daily_summary(date: str, platform: str):
    """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ãƒ»ä¿å­˜"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # æ—¥æ¬¡é›†è¨ˆ
    cursor.execute("""
        SELECT
            COUNT(*) as total_posts,
            SUM(impressions) as total_impressions,
            SUM(likes + comments + shares) as total_engagement,
            AVG(engagement_rate) as avg_engagement_rate,
            MAX(impressions) as max_impressions
        FROM analytics
        WHERE DATE(published_at) = ? AND platform = ?
    """, (date, platform))

    result = cursor.fetchone()

    if result and result[0] > 0:
        # ãƒˆãƒƒãƒ—æŠ•ç¨¿å–å¾—
        cursor.execute("""
            SELECT post_id, impressions
            FROM analytics
            WHERE DATE(published_at) = ? AND platform = ?
            ORDER BY impressions DESC
            LIMIT 1
        """, (date, platform))

        top_post = cursor.fetchone()
        top_post_id = top_post[0] if top_post else None
        top_post_impressions = top_post[1] if top_post else 0

        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        cursor.execute("""
            INSERT OR REPLACE INTO daily_summary (
                date, platform, total_posts, total_impressions, total_engagement,
                avg_engagement_rate, top_post_id, top_post_impressions, collected_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            date, platform, result[0], result[1] or 0, result[2] or 0,
            result[3] or 0.0, top_post_id, top_post_impressions,
            datetime.now().isoformat()
        ))

    conn.commit()
    conn.close()


def export_csv_backup(analytics_data: List[Dict], date: str):
    """CSVãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‡ºåŠ›"""
    import csv

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    BACKUP_DIR.mkdir(exist_ok=True)

    csv_path = BACKUP_DIR / f"analytics_{date}.csv"

    if not analytics_data:
        print("âš ï¸  CSVã«å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åå–å¾—
    fieldnames = list(analytics_data[0].keys())

    with open(csv_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(analytics_data)

    print(f"âœ… CSVãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜: {csv_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 80)
    print("ğŸš€ Late API æ—¥æ¬¡ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹åé›†é–‹å§‹")
    print("=" * 80)
    print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    init_database()

    # å‰æ—¥ã®æ—¥ä»˜
    yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    print(f"ğŸ“… åé›†å¯¾è±¡æ—¥: {yesterday}\n")

    # å…¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
    platforms = ["facebook", "linkedin", "twitter", "threads"]
    all_data = []

    for platform in platforms:
        print(f"ğŸ“Š {platform.upper()} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")

        data = get_analytics(
            config=config,
            from_date=yesterday,
            to_date=yesterday,
            platform=platform,
            limit=1000
        )

        if data:
            print(f"   å–å¾—ä»¶æ•°: {len(data)}ä»¶")
            all_data.extend(data)

            # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥ã‚µãƒãƒªãƒ¼ä¿å­˜
            save_daily_summary(yesterday, platform)
        else:
            print(f"   âš ï¸  ãƒ‡ãƒ¼ã‚¿ãªã—")

    print()

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
    if all_data:
        save_to_database(all_data)

        # CSVãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        export_csv_backup(all_data, yesterday)

        print(f"\nğŸ“ˆ ã‚µãƒãƒªãƒ¼:")
        print(f"   ç·å–å¾—ä»¶æ•°: {len(all_data)}ä»¶")
        print(f"   å¯¾è±¡æ—¥: {yesterday}")
        print(f"   ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ•°: {len(platforms)}")
    else:
        print("âš ï¸  å–å¾—ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã—ãŸ")

    print("\n" + "=" * 80)
    print("âœ… æ—¥æ¬¡ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹åé›†å®Œäº†")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
