#!/usr/bin/env python3
"""
å…¨ãƒãƒƒãƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆï¼ˆé‡è¤‡æŽ’é™¤æ©Ÿèƒ½ä»˜ãï¼‰

ã€æ”¹å–„ç‚¹ã€‘
1. ãƒžãƒ¼ã‚¸æ™‚ã«åŒ»é™¢åã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
2. æœ€åˆã«å‡ºç¾ã—ãŸåŒ»é™¢ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
3. é‡è¤‡çµ±è¨ˆã‚’å‡ºåŠ›
4. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®çµ±åˆCSVä½œæˆ

ã€åŠ¹æžœã€‘
- é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æŽ’é™¤
- çµ±åˆCSVã®å“è³ªå‘ä¸Š
- å¾Œç¶šå‡¦ç†ã®åŠ¹çŽ‡åŒ–
"""

import csv
import glob
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set

def merge_all_batches_with_dedup():
    """å…¨ãƒãƒƒãƒCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€é‡è¤‡æŽ’é™¤ã—ã¦çµ±åˆ"""

    # å…¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    csv_files = []
    csv_files.extend(glob.glob('batch_002_leads_final.csv'))
    csv_files.extend(glob.glob('batch_*_leads_llm_*.csv'))

    print(f"ðŸ“‚ {len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¾ã™")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
    seen_clinics: Set[str] = set()
    unique_rows: List[Dict] = []
    duplicate_count = 0
    total_count = 0

    fieldnames = None

    for csv_file in sorted(csv_files):
        print(f"   èª­ã¿è¾¼ã¿ä¸­: {csv_file}")

        with open(csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)

            if fieldnames is None:
                fieldnames = reader.fieldnames

            for row in reader:
                total_count += 1
                clinic_name = row.get('åŒ»é™¢å', '')

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if clinic_name in seen_clinics:
                    duplicate_count += 1
                    print(f"      âš ï¸  é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {clinic_name}")
                    continue

                # æ–°è¦åŒ»é™¢ã¨ã—ã¦è¨˜éŒ²
                seen_clinics.add(clinic_name)
                unique_rows.append(row)

    print(f"\nðŸ“Š çµ±åˆçµæžœ:")
    print(f"   ç·èª­ã¿è¾¼ã¿ä»¶æ•°: {total_count}ä»¶")
    print(f"   é‡è¤‡å‰Šé™¤: {duplicate_count}ä»¶")
    print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢æ•°: {len(unique_rows)}ä»¶")
    print(f"   é‡è¤‡çŽ‡: {duplicate_count / total_count * 100:.1f}%")

    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    unique_rows_sorted = sorted(unique_rows, key=lambda x: int(x['ã‚¹ã‚³ã‚¢']), reverse=True)

    # æœ€çµ‚CSVå‡ºåŠ›ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"tanabe_dental_leads_all_batches_UNIQUE_{timestamp}.csv"

    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(unique_rows_sorted)

    print(f"\nâœ… æœ€çµ‚å–¶æ¥­ãƒªã‚¹ãƒˆä½œæˆå®Œäº†: {output_file}")

    # çµ±è¨ˆæƒ…å ±
    if unique_rows_sorted:
        avg_score = sum(int(r['ã‚¹ã‚³ã‚¢']) for r in unique_rows_sorted) / len(unique_rows_sorted)

        print(f"\n--- çµ±è¨ˆæƒ…å ± ---")
        print(f"ç·ä»¶æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {len(unique_rows_sorted)}ä»¶")
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.1f}ç‚¹")
        print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {unique_rows_sorted[0]['ã‚¹ã‚³ã‚¢']}ç‚¹ - {unique_rows_sorted[0]['åŒ»é™¢å']}")
        print(f"æœ€ä½Žã‚¹ã‚³ã‚¢: {unique_rows_sorted[-1]['ã‚¹ã‚³ã‚¢']}ç‚¹ - {unique_rows_sorted[-1]['åŒ»é™¢å']}")

        # ã‚¹ã‚³ã‚¢å¸¯åˆ¥é›†è¨ˆ
        high_score = sum(1 for r in unique_rows_sorted if int(r['ã‚¹ã‚³ã‚¢']) >= 70)
        mid_score = sum(1 for r in unique_rows_sorted if 50 <= int(r['ã‚¹ã‚³ã‚¢']) < 70)
        low_score = sum(1 for r in unique_rows_sorted if int(r['ã‚¹ã‚³ã‚¢']) < 50)

        print(f"\nã‚¹ã‚³ã‚¢å¸¯åˆ¥:")
        print(f"  70ç‚¹ä»¥ä¸Šï¼ˆå„ªå…ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒæŽ¨å¥¨ï¼‰: {high_score}ä»¶")
        print(f"  50-69ç‚¹ï¼ˆä¸­å„ªå…ˆåº¦ï¼‰: {mid_score}ä»¶")
        print(f"  50ç‚¹æœªæº€ï¼ˆä½Žå„ªå…ˆåº¦ï¼‰: {low_score}ä»¶")

    return output_file

if __name__ == '__main__':
    merge_all_batches_with_dedup()
