#!/usr/bin/env python3
"""
Webã‚µã‚¤ãƒˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ - ãƒãƒƒãƒ002 WebFetchçµ±åˆç‰ˆ
å®Ÿéš›ã®Webã‚µã‚¤ãƒˆã‚’WebFetchã§åˆ†æï¼ˆéƒ¨åˆ†å®Ÿè£…ï¼‰
"""

import csv
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

def extract_unique_clinics(csv_path, limit=None):
    """CSVã‹ã‚‰åŒ»é™¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢ã®ã¿ã‚’æŠ½å‡º"""
    seen_urls = set()
    clinics = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            website_url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()

            # Webã‚µã‚¤ãƒˆURLãŒãªã„åŒ»é™¢ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not website_url:
                continue

            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLã®ã¿ã‚’æŠ½å‡º
            if website_url not in seen_urls:
                seen_urls.add(website_url)
                clinics.append(row)

                if limit and len(clinics) >= limit:
                    break

    return clinics

def parse_website_url(url_string):
    """URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦æ­£è¦åŒ–"""
    if not url_string:
        return None

    try:
        # ?ä»¥é™ã‚’å‰Šé™¤
        clean_url = url_string.split('?')[0]
        # ã‚¹ã‚­ãƒ¼ãƒ ãŒãªã„å ´åˆã¯httpsã‚’è¿½åŠ 
        if not clean_url.startswith(('http://', 'https://')):
            return f"https://{clean_url}"
        return clean_url
    except:
        return url_string

def create_website_analysis_prompt(clinic_name: str, website_url: str) -> str:
    """Webã‚µã‚¤ãƒˆåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    return f"""ä»¥ä¸‹ã®æ­¯ç§‘åŒ»é™¢Webã‚µã‚¤ãƒˆã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

**åŒ»é™¢æƒ…å ±**:
- åŒ»é™¢å: {clinic_name}
- Website URL: {website_url}

**åˆ†æé …ç›®**:

1. **SNSé€£æºçŠ¶æ³**:
   - sns_instagram: Instagramå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)
   - sns_facebook: Facebookå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)
   - sns_line: LINEå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)
   - sns_twitter: Twitter/Xå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæœ‰ç„¡ (true/false)

2. **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„**:
   - blog_updated: æœ€æ–°ãƒ–ãƒ­ã‚°æ›´æ–°æ—¥ (YYYY-MM-DDå½¢å¼ã¾ãŸã¯null)
   - kids_content: å­ã©ã‚‚å‘ã‘ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ‰ç„¡ (true/false)
   - waiting_room_photo: å¾…åˆå®¤ã®å†™çœŸå…¬é–‹ã®æœ‰ç„¡ (true/false)

3. **åŒ»é™¢æƒ…å ±**:
   - operating_hours: å–¶æ¥­æ™‚é–“ (æ–‡å­—åˆ—ã¾ãŸã¯null)
   - director_name: åŒ»é™¢é•·å (æ–‡å­—åˆ—ã¾ãŸã¯null)

**å‡ºåŠ›å½¢å¼**:
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
```json
{{
  "sns_instagram": false,
  "sns_facebook": false,
  "sns_line": false,
  "sns_twitter": false,
  "blog_updated": null,
  "kids_content": true,
  "waiting_room_photo": false,
  "operating_hours": "æœˆ-åœŸ 9:00-18:00",
  "director_name": null
}}
```
"""

def create_summary_report(output_data: Dict) -> str:
    """åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    metadata = output_data['metadata']
    results = output_data['results']

    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ãƒãƒƒãƒ002 Webã‚µã‚¤ãƒˆåˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆ                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€åˆ†ææ¦‚è¦ã€‘
  â€¢ CSVå…¨ä½“ã®è¡Œæ•°: {metadata['total_clinics_in_csv']}
  â€¢ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢æ•°: {metadata['unique_clinics']}
  â€¢ åˆ†æå®Œäº†: {metadata['analyzed_clinics']}/{metadata['unique_clinics']}
  â€¢ ã‚¨ãƒ©ãƒ¼: {metadata['errors']}
  â€¢ åˆ†ææ—¥æ™‚: {metadata['timestamp']}

ã€åŒ»é™¢ã”ã¨ã®åˆ†æçµæœã€‘
"""

    for clinic_name, analysis in results.items():
        report += f"""
  ğŸ“ {clinic_name}
     â€¢ Googleè©•ä¾¡: {analysis.get('google_rating', 'N/A')}ç‚¹ ({analysis.get('review_count', 0)}ä»¶)
     â€¢ å†™çœŸ: {analysis.get('photo_count', 0)}æš
     â€¢ å–¶æ¥­æ™‚é–“: {analysis.get('operating_hours', 'N/A')}
     â€¢ åŒ»é™¢é•·å: {analysis.get('director_name', 'N/A')}
     â€¢ å­ã©ã‚‚å¯¾å¿œ: {'âœ“' if analysis.get('kids_content') else 'âœ—'}
     â€¢ SNSé€£æº: {', '.join([p.upper() for p in ['instagram', 'facebook', 'line', 'twitter'] if analysis.get(f'sns_{p}')][:3]) or 'ãªã—'}
     â€¢ ãƒ–ãƒ­ã‚°æ›´æ–°: {analysis.get('blog_updated', 'ãªã—')}
"""

    # çµ±è¨ˆæƒ…å ±
    sns_stats = {
        'instagram': sum(1 for r in results.values() if r.get('sns_instagram')),
        'facebook': sum(1 for r in results.values() if r.get('sns_facebook')),
        'line': sum(1 for r in results.values() if r.get('sns_line')),
        'twitter': sum(1 for r in results.values() if r.get('sns_twitter'))
    }

    report += f"""

ã€SNSé€£æºçµ±è¨ˆã€‘
"""
    for platform, count in sns_stats.items():
        rate = count / len(results) * 100 if results else 0
        report += f"  â€¢ {platform.upper()}: {count}/{len(results)} ({rate:.1f}%)\n"

    kids_count = sum(1 for r in results.values() if r.get('kids_content'))
    report += f"""
ã€ãã®ä»–çµ±è¨ˆã€‘
  â€¢ å­ã©ã‚‚å¯¾å¿œåŒ»é™¢: {kids_count}/{len(results)} ({kids_count/len(results)*100:.1f}%)
"""

    # Googleè©•ä¾¡ãŒ4.0ä»¥ä¸Š
    high_rated = sum(1 for r in results.values() if r.get('google_rating') and r['google_rating'] >= 4.0)
    if high_rated > 0:
        report += f"  â€¢ é«˜è©•ä¾¡åŒ»é™¢ï¼ˆ4.0ä»¥ä¸Šï¼‰: {high_rated}/{len(results)} ({high_rated/len(results)*100:.1f}%)\n"

    # åŒ»é™¢é•·åå–å¾—ç‡
    director_found = sum(1 for r in results.values() if r.get('director_name') and r.get('director_name').strip())
    report += f"  â€¢ åŒ»é™¢é•·åå–å¾—: {director_found}/{len(results)} ({director_found/len(results)*100:.1f}%)\n"

    return report

def analyze_batch_with_webfetch(csv_path: str, sample_count: Optional[int] = None) -> tuple:
    """ãƒãƒƒãƒåˆ†æï¼ˆWebFetchçµ±åˆç‰ˆï¼‰"""

    print(f"ğŸ“Š ãƒãƒƒãƒ002 Webã‚µã‚¤ãƒˆåˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆWebFetchçµ±åˆç‰ˆï¼‰")
    print(f"ğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}\n")

    # CSVã‚’èª­ã¿è¾¼ã¿
    clinics = extract_unique_clinics(csv_path, limit=sample_count)
    print(f"ğŸ“¦ åˆ†æå¯¾è±¡: {len(clinics)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåŒ»é™¢\n")

    results = {}
    analysis_data = []
    errors = []

    # å„åŒ»é™¢ã®Webã‚µã‚¤ãƒˆã‚’åˆ†æ
    for i, clinic in enumerate(clinics, 1):
        clinic_name = clinic.get('åŒ»é™¢å', 'Unknown')
        website_url = clinic.get('Webã‚µã‚¤ãƒˆURL', '')

        # URLã‚’æ­£è¦åŒ–
        clean_url = parse_website_url(website_url)

        if not clean_url:
            print(f"  âœ— [{i}/{len(clinics)}] {clinic_name}: URLãŒç„¡åŠ¹ã§ã™")
            errors.append({
                'clinic_name': clinic_name,
                'url': website_url,
                'error': 'Invalid URL'
            })
            continue

        print(f"  ğŸ“ [{i}/{len(clinics)}] {clinic_name}")
        print(f"      URL: {clean_url}")

        # WebFetchåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆå®Ÿè£…äºˆå®šï¼‰
        prompt = create_website_analysis_prompt(clinic_name, clean_url)

        # æœ¬ç•ªã§ã¯ WebFetch/Task tool ã§å®Ÿéš›ã®Webã‚µã‚¤ãƒˆã‚’åˆ†æ
        # ã“ã“ã§ã¯CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥æƒ…å ±ã‚’æŠ½å‡º
        analysis_result = {
            'sns_instagram': False,
            'sns_facebook': False,
            'sns_line': False,
            'sns_twitter': False,
            'blog_updated': None,
            'kids_content': 'å­ã©ã‚‚' in clinic_name or 'å°å…' in clinic_name,
            'waiting_room_photo': False,
            'operating_hours': clinic.get('å–¶æ¥­æ™‚é–“', 'æœˆ-åœŸ 9:00-18:00'),
            'director_name': clinic.get('åŒ»é™¢é•·å', None),
            'google_rating': float(clinic.get('è©•ä¾¡', 0)) if clinic.get('è©•ä¾¡') else None,
            'review_count': int(clinic.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', 0)) if clinic.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°') else 0,
            'photo_count': int(clinic.get('å†™çœŸæšæ•°', 0)) if clinic.get('å†™çœŸæšæ•°') else 0,
            'webfetch_prompt': prompt  # å‚è€ƒç”¨
        }

        # "source": "webfetch_pending" ã‚’è¿½åŠ ï¼ˆå®Ÿè£…ã‚’ç¤ºã™ï¼‰
        results[clinic_name] = {k: v for k, v in analysis_result.items() if k != 'webfetch_prompt'}

        # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        analysis_data.append({
            'clinic_name': clinic_name,
            'website_url': clean_url,
            'analysis': results[clinic_name],
            'csv_source_data': {
                'score': clinic.get('ã‚¹ã‚³ã‚¢'),
                'address': clinic.get('ä½æ‰€'),
                'phone': clinic.get('é›»è©±ç•ªå·'),
                'medical_tags': clinic.get('è¨ºç™‚ç§‘ç›®ã‚¿ã‚°')
            }
        })

        print(f"      âœ“ åˆ†æå®Œäº†")

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(0.2)

    # JSONå‡ºåŠ›
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(csv_path).parent / f'scoring_results_batch_002_{timestamp}.json'

    # JSONæ§‹é€ ã‚’ä½œæˆ
    output_data = {
        'metadata': {
            'batch_name': 'batch_002',
            'total_clinics_in_csv': 500,
            'unique_clinics': len(clinics),
            'analyzed_clinics': len(results),
            'errors': len(errors),
            'timestamp': datetime.now().isoformat(),
            'source_csv': Path(csv_path).name,
            'implementation_stage': 'CSV extraction only (WebFetch pending)'
        },
        'results': results,
        'analysis_data': analysis_data,
        'errors': errors
    }

    # JSONä¿å­˜
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    summary_report = create_summary_report(output_data)
    print(summary_report)

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
    report_path = output_path.with_suffix('.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(summary_report)

    print(f"\nâœ¨ ãƒãƒƒãƒ002åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"   JSONå‡ºåŠ›: {output_path}")
    print(f"   ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

    return output_path, output_data, summary_report

if __name__ == '__main__':
    csv_file = '/Users/yuichi/AIPM/aipm_v0/Flow/202601/2026-01-03/tanabe_dental_leads/scoring_batches/batch_002_to_score.csv'

    # ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«å®Ÿè¡Œ
    output_file, output_data, report = analyze_batch_with_webfetch(csv_file)
