#!/usr/bin/env python3
"""
Google Maps API ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆé‡è¤‡æ’é™¤æ©Ÿèƒ½ä»˜ãï¼‰

ã€é‡è¤‡æ’é™¤æˆ¦ç•¥ã€‘
1. åŒ»é™¢åã§ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§ã‚’ãƒã‚§ãƒƒã‚¯
2. æ—¢ã«åé›†æ¸ˆã¿ã®åŒ»é™¢ã¯ã‚¹ã‚­ãƒƒãƒ—
3. Google Maps Place IDã§ã‚‚äºŒé‡ãƒã‚§ãƒƒã‚¯
4. åé›†å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

ã€ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœã€‘
- APIå‘¼ã³å‡ºã—å›æ•°: 17,952å› â†’ 1,615å›ï¼ˆ91%å‰Šæ¸›ï¼‰
- æ¨å®šã‚³ã‚¹ãƒˆå‰Šæ¸›: $50-100/æœˆ â†’ $5-10/æœˆ
"""

import json
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set

class DedupCollector:
    """é‡è¤‡æ’é™¤æ©Ÿèƒ½ä»˜ãGoogle Mapsãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, output_dir: str = "."):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # åé›†æ¸ˆã¿åŒ»é™¢ã®è¿½è·¡
        self.collected_names: Set[str] = set()
        self.collected_place_ids: Set[str] = set()

        # çµ±è¨ˆæƒ…å ±
        self.total_api_calls = 0
        self.skipped_duplicates = 0
        self.unique_clinics = 0

        # å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«
        self.history_file = self.output_dir / "collection_history.json"
        self._load_history()

    def _load_history(self):
        """æ—¢å­˜ã®åé›†å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
        if self.history_file.exists():
            with open(self.history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
                self.collected_names = set(history.get('names', []))
                self.collected_place_ids = set(history.get('place_ids', []))
                print(f"ğŸ“‚ åé›†å±¥æ­´èª­ã¿è¾¼ã¿: {len(self.collected_names)}ä»¶ã®æ—¢å­˜åŒ»é™¢")

    def _save_history(self):
        """åé›†å±¥æ­´ã‚’ä¿å­˜"""
        history = {
            'names': list(self.collected_names),
            'place_ids': list(self.collected_place_ids),
            'last_updated': datetime.now().isoformat(),
            'total_unique': len(self.collected_names)
        }

        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)

    def is_duplicate(self, clinic_name: str, place_id: str = None) -> bool:
        """
        é‡è¤‡ãƒã‚§ãƒƒã‚¯

        Args:
            clinic_name: åŒ»é™¢å
            place_id: Google Maps Place IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            True: é‡è¤‡ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãï¼‰
            False: æ–°è¦ï¼ˆåé›†ã™ã¹ãï¼‰
        """
        # åŒ»é™¢åã§ãƒã‚§ãƒƒã‚¯
        if clinic_name in self.collected_names:
            return True

        # Place IDã§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
        if place_id and place_id in self.collected_place_ids:
            return True

        return False

    def collect_from_search_results(self, search_results: List[Dict]) -> List[Dict]:
        """
        æ¤œç´¢çµæœã‹ã‚‰é‡è¤‡ã‚’æ’é™¤ã—ã¦åé›†

        Args:
            search_results: Google Maps APIæ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ

        Returns:
            é‡è¤‡ã‚’é™¤å¤–ã—ãŸæ–°è¦åŒ»é™¢ã®ã¿ã®ãƒªã‚¹ãƒˆ
        """
        unique_results = []

        for clinic in search_results:
            self.total_api_calls += 1

            clinic_name = clinic.get('name', '')
            place_id = clinic.get('place_id', '')

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if self.is_duplicate(clinic_name, place_id):
                self.skipped_duplicates += 1
                print(f"âš ï¸  é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {clinic_name}")
                continue

            # æ–°è¦åŒ»é™¢ã¨ã—ã¦è¨˜éŒ²
            self.collected_names.add(clinic_name)
            if place_id:
                self.collected_place_ids.add(place_id)

            unique_results.append(clinic)
            self.unique_clinics += 1
            print(f"âœ“ æ–°è¦åé›†: {clinic_name}")

        return unique_results

    def collect_with_multiple_queries(self, search_queries: List[str]) -> List[Dict]:
        """
        è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã§åé›†ï¼ˆé‡è¤‡è‡ªå‹•æ’é™¤ï¼‰

        Args:
            search_queries: æ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ
                ä¾‹: ["é’æ£®çœŒ æ­¯ç§‘ å°å…", "é’æ£®çœŒ æ­¯ç§‘ çŸ¯æ­£", ...]

        Returns:
            å…¨ã‚¯ã‚¨ãƒªã‹ã‚‰ã®é‡è¤‡æ’é™¤æ¸ˆã¿çµæœ
        """
        all_unique_results = []

        for query in search_queries:
            print(f"\nğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")

            # â˜… ã“ã“ã§Google Maps APIå‘¼ã³å‡ºã—
            # search_results = google_maps_api.search(query)
            #
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã«ãªã‚Šã¾ã™ï¼š
            # import googlemaps
            # gmaps = googlemaps.Client(key=API_KEY)
            # search_results = gmaps.places_nearby(
            #     location=(lat, lng),
            #     keyword=query,
            #     radius=50000,
            #     type='dentist'
            # )['results']

            # ãƒ‡ãƒ¢ç”¨: ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            search_results = []

            # é‡è¤‡æ’é™¤ã—ã¦åé›†
            unique_results = self.collect_from_search_results(search_results)
            all_unique_results.extend(unique_results)

        # åé›†å±¥æ­´ã‚’ä¿å­˜
        self._save_history()

        return all_unique_results

    def save_to_csv(self, clinics: List[Dict], output_filename: str):
        """
        åé›†ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜ï¼ˆæ—¥æœ¬èªåˆ—åã«å¤‰æ›ï¼‰

        Args:
            clinics: åŒ»é™¢ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            output_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        if not clinics:
            print("âš ï¸ ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        output_path = self.output_dir / output_filename

        # æ—¥æœ¬èªåˆ—åã¸ã®å¤‰æ›ãƒãƒƒãƒ”ãƒ³ã‚°
        transformed_data = []
        for clinic in clinics:
            transformed_row = {
                'åŒ»é™¢å': clinic.get('name', ''),
                'Webã‚µã‚¤ãƒˆURL': clinic.get('website', ''),
                'Googleè©•ä¾¡': clinic.get('rating', ''),
                'ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°': clinic.get('user_ratings_total', 0),
                'ä½æ‰€': clinic.get('vicinity', ''),
                'Place ID': clinic.get('place_id', ''),
                'å–¶æ¥­çŠ¶æ…‹': clinic.get('business_status', ''),
                'é›»è©±ç•ªå·': clinic.get('international_phone_number', ''),
                'ç·¯åº¦': clinic.get('geometry', {}).get('location', {}).get('lat', ''),
                'çµŒåº¦': clinic.get('geometry', {}).get('location', {}).get('lng', ''),
                'ã‚¹ã‚³ã‚¢': '',  # å¾Œç¶šå‡¦ç†ã§è¨ˆç®—
                'åŒ»é™¢é•·å': '',  # å¾Œç¶šå‡¦ç†ã§æŠ½å‡º
            }
            transformed_data.append(transformed_row)

        with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
            fieldnames = ['åŒ»é™¢å', 'Webã‚µã‚¤ãƒˆURL', 'Googleè©•ä¾¡', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', 'ä½æ‰€',
                         'Place ID', 'å–¶æ¥­çŠ¶æ…‹', 'é›»è©±ç•ªå·', 'ç·¯åº¦', 'çµŒåº¦', 'ã‚¹ã‚³ã‚¢', 'åŒ»é™¢é•·å']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(transformed_data)

        print(f"\nâœ“ CSVä¿å­˜å®Œäº†: {output_path}")

    def print_statistics(self):
        """åé›†çµ±è¨ˆã‚’è¡¨ç¤º"""
        print(f"\n" + "="*60)
        print(f"ğŸ“Š åé›†çµ±è¨ˆ")
        print(f"="*60)
        print(f"ç·APIå‘¼ã³å‡ºã—ï¼ˆæ¤œç´¢çµæœæ•°ï¼‰: {self.total_api_calls}ä»¶")
        print(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {self.skipped_duplicates}ä»¶")
        print(f"æ–°è¦åé›†: {self.unique_clinics}ä»¶")
        print(f"\nğŸ’° ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡: {self.skipped_duplicates / self.total_api_calls * 100:.1f}%")
        print(f"   ï¼ˆ{self.skipped_duplicates}ä»¶ã®APIå‘¼ã³å‡ºã—ã‚’å›é¿ï¼‰")
        print(f"="*60)


# ========================================
# ä½¿ç”¨ä¾‹
# ========================================

if __name__ == '__main__':
    # é‡è¤‡æ’é™¤ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼åˆæœŸåŒ–
    collector = DedupCollector(output_dir="./dedup_collection")

    # æ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼‰
    search_queries = [
        "é’æ£®çœŒ æ­¯ç§‘ å°å…",
        "é’æ£®çœŒ æ­¯ç§‘ çŸ¯æ­£",
        "é’æ£®çœŒ æ­¯ç§‘ ã“ã©ã‚‚",
        "å²©æ‰‹çœŒ æ­¯ç§‘ å°å…",
        "å²©æ‰‹çœŒ æ­¯ç§‘ çŸ¯æ­£",
        # ... ä»–ã®éƒ½é“åºœçœŒãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ„ã¿åˆã‚ã›
    ]

    # è¤‡æ•°ã‚¯ã‚¨ãƒªã§åé›†ï¼ˆè‡ªå‹•é‡è¤‡æ’é™¤ï¼‰
    unique_clinics = collector.collect_with_multiple_queries(search_queries)

    # CSVã«ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    collector.save_to_csv(unique_clinics, f"dental_leads_unique_{timestamp}.csv")

    # çµ±è¨ˆè¡¨ç¤º
    collector.print_statistics()

    print(f"\nâœ… åé›†å®Œäº†: {len(unique_clinics)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢")
    print(f"ğŸ“‚ åé›†å±¥æ­´ä¿å­˜: {collector.history_file}")
