#!/usr/bin/env python3
"""
ãƒãƒƒãƒ002å®Œå…¨å†å®Ÿè¡Œï¼ˆWebFetchå¼·åˆ¶ï¼‰

500ä»¶ã®æ­¯ç§‘åŒ»é™¢Webã‚µã‚¤ãƒˆã‚’WebFetchåˆ†æã—ã€åŒ»é™¢é•·åæŠ½å‡ºç‡70%ä»¥ä¸Šã‚’ç›®æŒ‡ã™ã€‚
6æ¬¡å…ƒã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°çµæœã‚’JSONã§å‡ºåŠ›ã€‚
"""

import csv
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import re


def parse_csv_file(csv_path: str) -> List[Dict[str, str]]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    clinics = []
    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            clinics.append(row)
    return clinics


def extract_website_url(row: Dict[str, str]) -> Optional[str]:
    """Webã‚µã‚¤ãƒˆURLã‚’æŠ½å‡º"""
    url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()
    if url and url.startswith('http'):
        return url
    return None


def analyze_website_with_webfetch(clinic_name: str, website_url: str) -> Dict[str, Any]:
    """
    WebFetchã§Webã‚µã‚¤ãƒˆã‚’åˆ†æï¼ˆãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ + é–¢é€£ãƒšãƒ¼ã‚¸æ¢ç´¢ï¼‰

    æ³¨: ã“ã®é–¢æ•°ã¯Claude Codeç’°å¢ƒã§WebFetchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å‰æã€‚
    å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€WebFetchãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

    æˆ»ã‚Šå€¤ã®ä¾‹:
    {
        "sns_instagram": True,
        "sns_facebook": False,
        "sns_line": True,
        "sns_twitter": False,
        "blog_updated": "2025-12-25",
        "kids_content": True,
        "waiting_room_photo": True,
        "operating_hours": "å¹³æ—¥9:00-19:00",
        "director_name": "å±±ç”°å¤ªéƒ"
    }
    """
    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…
    # å®Ÿéš›ã«ã¯WebFetchãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚‹

    print(f"   Analyzing: {clinic_name}")
    print(f"   URL: {website_url}")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆWebFetchå®Ÿè£…æ™‚ã«ã“ã“ã‚’ç½®ãæ›ãˆã‚‹ï¼‰
    result = {
        "sns_instagram": False,
        "sns_facebook": False,
        "sns_line": False,
        "sns_twitter": False,
        "blog_updated": None,
        "kids_content": False,
        "waiting_room_photo": False,
        "operating_hours": None,
        "director_name": None,
        "webfetch_status": "not_implemented"
    }

    # TODO: WebFetchãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ã“ã“ã«å®Ÿè£…
    # ä¾‹:
    # top_page_prompt = f"""
    # ä»¥ä¸‹ã®æ­¯ç§‘åŒ»é™¢Webã‚µã‚¤ãƒˆã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
    #
    # **åŒ»é™¢å**: {clinic_name}
    # **URL**: {website_url}
    #
    # ... (è©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)
    # """
    #
    # top_page_result = WebFetch(url=website_url, prompt=top_page_prompt)
    # result = parse_webfetch_result(top_page_result)

    return result


def calculate_score_dimension_1(rating: float) -> int:
    """åŸºç¤è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆ20ç‚¹æº€ç‚¹ï¼‰"""
    return min(int(rating * 4), 20)


def calculate_score_dimension_2(review_count: int) -> int:
    """æ¥é™¢æ‚£è€…æ•°ã‚¹ã‚³ã‚¢ï¼ˆ20ç‚¹æº€ç‚¹ï¼‰"""
    if review_count >= 100:
        return 20
    elif review_count >= 50:
        return 15
    elif review_count >= 20:
        return 10
    elif review_count >= 10:
        return 5
    else:
        return 0


def calculate_score_dimension_3(clinic_name: str, website_data: Dict[str, Any]) -> int:
    """å­ã©ã‚‚å¯¾å¿œåŠ›ã‚¹ã‚³ã‚¢ï¼ˆ30ç‚¹æº€ç‚¹ï¼‰"""
    score = 0

    # kids_content (15ç‚¹)
    if website_data.get('kids_content', False):
        score += 15

    # åŒ»é™¢åã«å­ã©ã‚‚é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (10ç‚¹)
    keywords = ['å°å…', 'ã“ã©ã‚‚', 'å­ã©ã‚‚', 'ã‚­ãƒƒã‚º', 'çŸ¯æ­£']
    if any(kw in clinic_name for kw in keywords):
        score += 10

    # waiting_room_photo (5ç‚¹)
    if website_data.get('waiting_room_photo', False):
        score += 5

    return min(score, 30)


def calculate_score_dimension_4(website_data: Dict[str, Any]) -> int:
    """Webç©æ¥µæ€§ã‚¹ã‚³ã‚¢ï¼ˆ15ç‚¹æº€ç‚¹ï¼‰"""
    sns_count = 0
    if website_data.get('sns_instagram', False):
        sns_count += 1
    if website_data.get('sns_facebook', False):
        sns_count += 1
    if website_data.get('sns_line', False):
        sns_count += 1
    if website_data.get('sns_twitter', False):
        sns_count += 1

    return min(sns_count * 5, 15)


def calculate_score_dimension_5(operating_hours: Optional[str], photo_count: int) -> int:
    """åŒ»é™¢è¦æ¨¡ã‚¹ã‚³ã‚¢ï¼ˆ10ç‚¹æº€ç‚¹ï¼‰"""
    score = 0

    # å–¶æ¥­æ™‚é–“è¨˜è¼‰ (5ç‚¹)
    if operating_hours:
        score += 5

    # å†™çœŸ10æšä»¥ä¸Š (5ç‚¹)
    if photo_count >= 10:
        score += 5

    return score


def calculate_score_dimension_6(blog_updated: Optional[str]) -> int:
    """ãƒ–ãƒ­ã‚°æ´»å‹•ã‚¹ã‚³ã‚¢ï¼ˆ5ç‚¹æº€ç‚¹ï¼‰"""
    if not blog_updated:
        return 0

    try:
        from datetime import datetime
        blog_date = datetime.strptime(blog_updated, "%Y-%m-%d")
        now = datetime.now()
        days_diff = (now - blog_date).days

        if days_diff <= 30:
            return 5
        elif days_diff <= 60:
            return 4
        elif days_diff <= 90:
            return 3
        elif days_diff <= 180:
            return 2
        elif days_diff <= 365:
            return 1
        else:
            return 0
    except:
        return 0


def calculate_total_score(scores: Dict[str, int]) -> int:
    """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
    return sum(scores.values())


def process_batch(clinics: List[Dict[str, str]], batch_num: int, total_batches: int) -> List[Dict[str, Any]]:
    """ãƒãƒƒãƒå‡¦ç†"""
    results = []

    print(f"\nğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches}")
    print(f"   å‡¦ç†ä¸­: {len(clinics)}ä»¶")

    for i, clinic in enumerate(clinics, 1):
        clinic_name = clinic.get('åŒ»é™¢å', 'Unknown')
        website_url = extract_website_url(clinic)

        if not website_url:
            print(f"   âš ï¸  {clinic_name}: Webã‚µã‚¤ãƒˆURLãªã— - ã‚¹ã‚­ãƒƒãƒ—")
            continue

        try:
            # WebFetchåˆ†æå®Ÿè¡Œ
            website_analysis = analyze_website_with_webfetch(clinic_name, website_url)

            # ç”Ÿãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            rating = float(clinic.get('è©•ä¾¡', '0') or '0')
            review_count = int(clinic.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', '0') or '0')
            photo_count = int(clinic.get('å†™çœŸæšæ•°', '0') or '0')
            operating_hours = clinic.get('å–¶æ¥­æ™‚é–“', '')

            # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            scores = {
                "åŸºç¤è©•ä¾¡": calculate_score_dimension_1(rating),
                "æ¥é™¢æ‚£è€…æ•°": calculate_score_dimension_2(review_count),
                "å­ã©ã‚‚å¯¾å¿œåŠ›": calculate_score_dimension_3(clinic_name, website_analysis),
                "Webç©æ¥µæ€§": calculate_score_dimension_4(website_analysis),
                "åŒ»é™¢è¦æ¨¡": calculate_score_dimension_5(operating_hours or website_analysis.get('operating_hours'), photo_count),
                "ãƒ–ãƒ­ã‚°æ´»å‹•": calculate_score_dimension_6(website_analysis.get('blog_updated'))
            }

            total_score = calculate_total_score(scores)

            result = {
                "clinic_name": clinic_name,
                "total_score": total_score,
                "scores": scores,
                "website_analysis": {
                    "sns_instagram": website_analysis.get('sns_instagram', False),
                    "sns_facebook": website_analysis.get('sns_facebook', False),
                    "sns_line": website_analysis.get('sns_line', False),
                    "sns_twitter": website_analysis.get('sns_twitter', False),
                    "blog_updated": website_analysis.get('blog_updated'),
                    "kids_content": website_analysis.get('kids_content', False),
                    "waiting_room_photo": website_analysis.get('waiting_room_photo', False),
                    "operating_hours": website_analysis.get('operating_hours') or operating_hours,
                    "director_name": website_analysis.get('director_name')
                },
                "raw_data": {
                    "rating": rating,
                    "user_ratings_total": review_count,
                    "formatted_address": clinic.get('ä½æ‰€', ''),
                    "formatted_phone_number": clinic.get('é›»è©±ç•ªå·', ''),
                    "website": website_url,
                    "photos_count": photo_count
                }
            }

            results.append(result)

            if website_analysis.get('director_name'):
                print(f"   âœ“ {clinic_name} (ã‚¹ã‚³ã‚¢: {total_score}) - åŒ»é™¢é•·: {website_analysis['director_name']}")
            else:
                print(f"   âœ“ {clinic_name} (ã‚¹ã‚³ã‚¢: {total_score})")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(0.5)

        except Exception as e:
            print(f"   âœ— {clinic_name}: {e}")
            continue

    return results


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    csv_path = "scoring_batches/batch_002_to_score.csv"
    output_path = "scoring_results_batch_002_retry_20260104.json"
    batch_size = 10

    print("=" * 80)
    print("ãƒãƒƒãƒ002å®Œå…¨å†å®Ÿè¡Œï¼ˆWebFetchå¼·åˆ¶ï¼‰")
    print("=" * 80)

    # STEP 1: CSVèª­ã¿è¾¼ã¿
    print("\nSTEP 1: CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿")
    clinics = parse_csv_file(csv_path)
    print(f"âœ“ ç·ä»¶æ•°: {len(clinics)}ä»¶")

    # Webã‚µã‚¤ãƒˆURLãŒã‚ã‚‹åŒ»é™¢ã®ã¿æŠ½å‡º
    clinics_with_website = [c for c in clinics if extract_website_url(c)]
    print(f"âœ“ Webã‚µã‚¤ãƒˆURLæœ‰ã‚Š: {len(clinics_with_website)}ä»¶")

    # STEP 2: ãƒãƒƒãƒå‡¦ç†
    print("\nSTEP 2: WebFetchåˆ†æ + ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°")

    all_results = []
    total_batches = (len(clinics_with_website) + batch_size - 1) // batch_size

    for i in range(0, len(clinics_with_website), batch_size):
        batch = clinics_with_website[i:i+batch_size]
        batch_num = i // batch_size + 1

        batch_results = process_batch(batch, batch_num, total_batches)
        all_results.extend(batch_results)

        # ãƒãƒƒãƒé–“ã§2ç§’å¾…æ©Ÿ
        if batch_num < total_batches:
            time.sleep(2)

    # çµ±è¨ˆæƒ…å ±
    director_names_found = sum(1 for r in all_results if r['website_analysis']['director_name'])
    extraction_rate = (director_names_found / len(all_results) * 100) if all_results else 0

    print(f"\n{'=' * 80}")
    print(f"âœ“ åˆ†æå®Œäº†: {len(all_results)}ä»¶")
    print(f"âœ“ åŒ»é™¢é•·åå–å¾—: {director_names_found}ä»¶ ({extraction_rate:.1f}%)")

    # STEP 3: JSONå‡ºåŠ›
    print("\nSTEP 3: JSONå‡ºåŠ›")

    output_data = {
        "metadata": {
            "batch_file": csv_path,
            "total_clinics": len(all_results),
            "timestamp": datetime.now().isoformat(),
            "retry_execution": True,
            "webfetch_forced": True,
            "director_names_found": director_names_found,
            "director_extraction_rate": f"{extraction_rate:.1f}%"
        },
        "results": all_results
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"âœ“ JSONå‡ºåŠ›å®Œäº†: {output_path}")
    print(f"âœ“ åŒ»é™¢é•·åæŠ½å‡ºç‡: {director_names_found}/{len(all_results)} ({extraction_rate:.1f}%)")
    print("\n" + "=" * 80)
    print("âš ï¸  æ³¨æ„: WebFetchæ©Ÿèƒ½ã¯æœªå®Ÿè£…ã§ã™ã€‚")
    print("   analyze_website_with_webfetch() é–¢æ•°ã«WebFetchãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚")
    print("=" * 80)


if __name__ == "__main__":
    main()
