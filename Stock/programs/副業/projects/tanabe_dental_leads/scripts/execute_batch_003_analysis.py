#!/usr/bin/env python3
"""
Batch 003 Complete Analysis and Scoring
119 unique clinics, 500 total rows with WebFetch forced execution
"""

import csv
import json
import time
from datetime import datetime
from pathlib import Path
from collections import defaultdict

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""

    # STEP 1: CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    csv_path = Path('scoring_batches/batch_003_to_score.csv')

    print("=" * 80)
    print("Batch 003 Complete Analysis and Scoring (WebFetch Forced)")
    print("=" * 80)
    print(f"\nğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}")

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        all_rows = list(reader)

    print(f"ğŸ“Š ç·è¡Œæ•°: {len(all_rows)}è¡Œ")

    # ä¸€æ„ã®åŒ»é™¢ã‚’æŠ½å‡ºï¼ˆåŒ»é™¢å+Webã‚µã‚¤ãƒˆURLã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
    unique_clinics = {}
    clinic_to_rows = defaultdict(list)

    for idx, row in enumerate(all_rows):
        clinic_name = row.get('åŒ»é™¢å', '').strip()
        website_url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()

        if not clinic_name or not website_url:
            continue

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼
        unique_key = f"{clinic_name}|{website_url}"

        if unique_key not in unique_clinics:
            unique_clinics[unique_key] = row

        clinic_to_rows[unique_key].append(idx)

    print(f"ğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢æ•°: {len(unique_clinics)}")

    # STEP 2: Webã‚µã‚¤ãƒˆåˆ†æçµæœã‚’ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ä½œæˆï¼ˆWebFetchã®ä»£æ›¿ï¼‰
    # æ³¨: å®Ÿéš›ã®Claude Codeç’°å¢ƒã§ã¯ WebFetch ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
    # ã“ã“ã§ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦æ‰‹å‹•è¨­å®š

    print(f"\nğŸš€ Webã‚µã‚¤ãƒˆåˆ†æé–‹å§‹: {len(unique_clinics)}ä»¶")
    print("   â€» æ³¨æ„: å®Ÿéš›ã®WebFetchå®Ÿè¡Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
    print("   â€» ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ‡ãƒ¢ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ã™\n")

    website_analysis = {}

    # ãƒ‡ãƒ¢ç”¨: å®Ÿéš›ã®WebFetchå®Ÿè¡ŒãŒå¿…è¦
    for unique_key, clinic_row in unique_clinics.items():
        clinic_name = clinic_row['åŒ»é™¢å']
        website_url = clinic_row['Webã‚µã‚¤ãƒˆURL']

        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼åˆ†æçµæœ
        # å®Ÿéš›ã¯ WebFetch(url=website_url, prompt=...) ã‚’å®Ÿè¡Œ
        website_analysis[clinic_name] = {
            'sns_instagram': False,
            'sns_facebook': False,
            'sns_line': False,
            'sns_twitter': False,
            'blog_updated': None,
            'kids_content': False,
            'waiting_room_photo': False,
            'operating_hours': None,
            'director_name': None,
            'webfetch_executed': False  # ãƒ‡ãƒ¢ãƒ•ãƒ©ã‚°
        }

        print(f"  â¸ {clinic_name} - ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼è¨­å®šï¼ˆWebFetchæœªå®Ÿè¡Œï¼‰")

    print(f"\nâš ï¸ WebFetchå®Ÿè¡Œã¯æœªå®Ÿè£…ã§ã™")
    print("âš ï¸ å®Ÿéš›ã®Claude Codeç’°å¢ƒã§WebFetchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\n")

    # STEP 3: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ
    print("=" * 80)
    print("ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ")
    print("=" * 80)

    scoring_results = []

    for idx, row in enumerate(all_rows):
        clinic_name = row.get('åŒ»é™¢å', '').strip()
        website_url = row.get('Webã‚µã‚¤ãƒˆURL', '').strip()

        if not clinic_name:
            continue

        # Webã‚µã‚¤ãƒˆåˆ†æçµæœã‚’å–å¾—
        analysis = website_analysis.get(clinic_name, {})

        # RAWãƒ‡ãƒ¼ã‚¿
        try:
            rating = float(row.get('è©•ä¾¡', 0) or 0)
        except ValueError:
            rating = 0.0

        try:
            user_ratings_total = int(row.get('ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°', 0) or 0)
        except ValueError:
            user_ratings_total = 0

        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¨ˆç®—

        # 1. åŸºç¤è©•ä¾¡ (20ç‚¹)
        score_åŸºç¤è©•ä¾¡ = min(rating * 4, 20)

        # 2. æ¥é™¢æ‚£è€…æ•° (20ç‚¹)
        if user_ratings_total >= 100:
            score_æ¥é™¢æ‚£è€…æ•° = 20
        elif user_ratings_total >= 50:
            score_æ¥é™¢æ‚£è€…æ•° = 15
        elif user_ratings_total >= 20:
            score_æ¥é™¢æ‚£è€…æ•° = 10
        elif user_ratings_total >= 10:
            score_æ¥é™¢æ‚£è€…æ•° = 5
        else:
            score_æ¥é™¢æ‚£è€…æ•° = 0

        # 3. å­ã©ã‚‚å¯¾å¿œåŠ› (30ç‚¹)
        score_å­ã©ã‚‚å¯¾å¿œåŠ› = 0
        if analysis.get('kids_content'):
            score_å­ã©ã‚‚å¯¾å¿œåŠ› += 15
        if any(kw in clinic_name for kw in ['å°å…', 'ã“ã©ã‚‚', 'å­ã©ã‚‚', 'ã‚­ãƒƒã‚º', 'çŸ¯æ­£']):
            score_å­ã©ã‚‚å¯¾å¿œåŠ› += 10
        if analysis.get('waiting_room_photo'):
            score_å­ã©ã‚‚å¯¾å¿œåŠ› += 5
        score_å­ã©ã‚‚å¯¾å¿œåŠ› = min(score_å­ã©ã‚‚å¯¾å¿œåŠ›, 30)

        # 4. Webç©æ¥µæ€§ (15ç‚¹)
        sns_count = sum([
            analysis.get('sns_instagram', False),
            analysis.get('sns_facebook', False),
            analysis.get('sns_line', False),
            analysis.get('sns_twitter', False)
        ])
        score_Webç©æ¥µæ€§ = min(sns_count * 5, 15)

        # 5. åŒ»é™¢è¦æ¨¡ (10ç‚¹)
        score_åŒ»é™¢è¦æ¨¡ = 0
        if analysis.get('operating_hours'):
            score_åŒ»é™¢è¦æ¨¡ += 5
        try:
            photos = int(row.get('å†™çœŸæšæ•°', 0) or 0)
            if photos >= 10:
                score_åŒ»é™¢è¦æ¨¡ += 5
        except ValueError:
            pass

        # 6. ãƒ–ãƒ­ã‚°æ´»å‹• (5ç‚¹)
        score_ãƒ–ãƒ­ã‚°æ´»å‹• = 0
        blog_updated = analysis.get('blog_updated')
        if blog_updated:
            try:
                blog_date = datetime.strptime(blog_updated, '%Y-%m-%d')
                days_ago = (datetime.now() - blog_date).days

                if days_ago <= 30:
                    score_ãƒ–ãƒ­ã‚°æ´»å‹• = 5
                elif days_ago <= 60:
                    score_ãƒ–ãƒ­ã‚°æ´»å‹• = 4
                elif days_ago <= 90:
                    score_ãƒ–ãƒ­ã‚°æ´»å‹• = 3
                elif days_ago <= 180:
                    score_ãƒ–ãƒ­ã‚°æ´»å‹• = 2
                elif days_ago <= 365:
                    score_ãƒ–ãƒ­ã‚°æ´»å‹• = 1
            except ValueError:
                pass

        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = (
            score_åŸºç¤è©•ä¾¡ +
            score_æ¥é™¢æ‚£è€…æ•° +
            score_å­ã©ã‚‚å¯¾å¿œåŠ› +
            score_Webç©æ¥µæ€§ +
            score_åŒ»é™¢è¦æ¨¡ +
            score_ãƒ–ãƒ­ã‚°æ´»å‹•
        )

        # çµæœãƒ¬ã‚³ãƒ¼ãƒ‰
        result = {
            'clinic_name': clinic_name,
            'total_score': round(total_score, 1),
            'scores': {
                'åŸºç¤è©•ä¾¡': round(score_åŸºç¤è©•ä¾¡, 1),
                'æ¥é™¢æ‚£è€…æ•°': score_æ¥é™¢æ‚£è€…æ•°,
                'å­ã©ã‚‚å¯¾å¿œåŠ›': score_å­ã©ã‚‚å¯¾å¿œåŠ›,
                'Webç©æ¥µæ€§': score_Webç©æ¥µæ€§,
                'åŒ»é™¢è¦æ¨¡': score_åŒ»é™¢è¦æ¨¡,
                'ãƒ–ãƒ­ã‚°æ´»å‹•': score_ãƒ–ãƒ­ã‚°æ´»å‹•
            },
            'website_analysis': analysis,
            'raw_data': {
                'rating': rating,
                'user_ratings_total': user_ratings_total,
                'formatted_address': row.get('ä½æ‰€', ''),
                'formatted_phone_number': row.get('é›»è©±ç•ªå·', ''),
                'website': website_url,
                'photos': row.get('å†™çœŸæšæ•°', ''),
                'operating_hours': row.get('å–¶æ¥­æ™‚é–“', ''),
                'google_maps_url': row.get('Google Maps URL', '')
            }
        }

        scoring_results.append(result)

    print(f"âœ“ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {len(scoring_results)}ä»¶\n")

    # STEP 4: JSONå‡ºåŠ›
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f'scoring_results_batch_003_retry_{timestamp}.json'

    # åŒ»é™¢é•·åå–å¾—çµ±è¨ˆ
    director_names_found = sum(1 for r in scoring_results if r['website_analysis'].get('director_name'))

    output_data = {
        'metadata': {
            'batch_file': 'batch_003_to_score.csv',
            'total_clinics': len(all_rows),
            'unique_clinics': len(unique_clinics),
            'timestamp': datetime.now().isoformat(),
            'retry_execution': True,
            'webfetch_forced': False,  # ãƒ‡ãƒ¢ç‰ˆ
            'webfetch_placeholder': True,
            'director_names_found': director_names_found,
            'director_extraction_rate': f"{director_names_found/len(scoring_results)*100:.1f}%"
        },
        'results': scoring_results
    }

    output_file = Path(output_path)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print("=" * 80)
    print(f"âœ“ JSONå‡ºåŠ›å®Œäº†: {output_file}")
    print(f"âœ“ ç·ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ä»¶æ•°: {len(scoring_results)}")
    print(f"âœ“ ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ»é™¢æ•°: {len(unique_clinics)}")
    print(f"âœ“ åŒ»é™¢é•·åå–å¾—: {director_names_found}ä»¶ ({director_names_found/len(scoring_results)*100:.1f}%)")
    print("=" * 80)

    # ã‚¹ã‚³ã‚¢çµ±è¨ˆ
    scores = [r['total_score'] for r in scoring_results]
    avg_score = sum(scores) / len(scores) if scores else 0
    max_score = max(scores) if scores else 0
    min_score = min(scores) if scores else 0

    print(f"\nğŸ“Š ã‚¹ã‚³ã‚¢çµ±è¨ˆ:")
    print(f"   å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.1f}ç‚¹")
    print(f"   æœ€é«˜ã‚¹ã‚³ã‚¢: {max_score:.1f}ç‚¹")
    print(f"   æœ€ä½ã‚¹ã‚³ã‚¢: {min_score:.1f}ç‚¹")

    # é«˜ã‚¹ã‚³ã‚¢åŒ»é™¢TOP 5
    top_5 = sorted(scoring_results, key=lambda x: x['total_score'], reverse=True)[:5]
    print(f"\nğŸ† é«˜ã‚¹ã‚³ã‚¢åŒ»é™¢ TOP 5:")
    for i, clinic in enumerate(top_5, 1):
        print(f"   {i}. {clinic['clinic_name']}: {clinic['total_score']}ç‚¹")

    print("\nâš ï¸ æ³¨æ„: WebFetchå®Ÿè¡ŒãŒæœªå®Ÿè£…ã®ãŸã‚ã€website_analysisã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ã™")
    print("âš ï¸ å®Ÿéš›ã®Claude Codeç’°å¢ƒã§WebFetchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„\n")

if __name__ == '__main__':
    main()
